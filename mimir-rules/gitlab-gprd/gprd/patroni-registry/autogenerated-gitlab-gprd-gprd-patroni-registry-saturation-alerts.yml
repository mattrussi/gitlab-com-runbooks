# WARNING. DO NOT EDIT THIS FILE BY HAND. USE ./mimir-rules-jsonnet/saturation.jsonnet TO GENERATE IT
# YOUR CHANGES WILL BE OVERRIDDEN
groups:
- name: GitLab Component Saturation Statistics
  interval: 5m
  rules:
  - record: gitlab_component_saturation:ratio_quantile95_1w
    expr: quantile_over_time(0.95, gitlab_component_saturation:ratio{env="gprd",type="patroni-registry"}[1w])
  - record: gitlab_component_saturation:ratio_quantile99_1w
    expr: quantile_over_time(0.99, gitlab_component_saturation:ratio{env="gprd",type="patroni-registry"}[1w])
  - record: gitlab_component_saturation:ratio_quantile95_1h
    expr: quantile_over_time(0.95, gitlab_component_saturation:ratio{env="gprd",type="patroni-registry"}[1h])
  - record: gitlab_component_saturation:ratio_quantile99_1h
    expr: quantile_over_time(0.99, gitlab_component_saturation:ratio{env="gprd",type="patroni-registry"}[1h])
  - record: gitlab_component_saturation:ratio_avg_1h
    expr: avg_over_time(gitlab_component_saturation:ratio{env="gprd",type="patroni-registry"}[1h])
- name: GitLab Saturation Alerts
  interval: 1m
  rules:
  - alert: component_saturation_slo_out_of_bounds:cpu
    for: 5m
    annotations:
      title: The Average Service CPU Utilization resource of the {{ $labels.type }}
        service ({{ $labels.stage }} stage) has a saturation exceeding SLO and is
        close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Average Service CPU Utilization resource:

        This resource measures average CPU utilization across an all cores in a service fleet. If it is becoming saturated, it may indicate that the fleet needs horizontal or vertical scaling.
      grafana_dashboard_id: alerts-sat_cpu
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_cpu?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1465724101"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard) (
          clamp_min(
            clamp_max(
              1 - avg by (environment, tier, type, stage, shard) (
                rate(node_cpu_seconds_total{mode="idle", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
              )
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, tier, type, stage, shard) (
          clamp_min(
            clamp_max(
              1 - avg by (environment, tier, type, stage, shard) (
                rate(node_cpu_seconds_total{mode="idle", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s3
    expr: |
      gitlab_component_saturation:ratio{component="cpu",env="gprd",type="patroni-registry"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="cpu",env="gprd",type="patroni-registry"}
  - alert: component_saturation_slo_out_of_bounds:disk_inodes
    for: 15m
    annotations:
      title: The Disk inode Utilization per Device per Node resource of the {{ $labels.type
        }} service ({{ $labels.stage }} stage) has a saturation exceeding SLO and
        is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Disk inode Utilization per Device per Node resource:

        Disk inode utilization per device per node.

        If this is too high, its possible that a directory is filling up with files. Consider logging in an checking temp directories for large numbers of files
      grafana_dashboard_id: alerts-sat_disk_inodes
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_disk_inodes?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "39965907"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard, fqdn, device) (
          clamp_min(
            clamp_max(
              1 - (
                node_filesystem_files_free{fstype=~"(ext.|xfs)", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                /
                node_filesystem_files{fstype=~"(ext.|xfs)", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, tier, type, stage, shard, fqdn, device) (
          clamp_min(
            clamp_max(
              1 - (
                node_filesystem_files_free{fstype=~"(ext.|xfs)", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                /
                node_filesystem_files{fstype=~"(ext.|xfs)", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      pager: pagerduty
      rules_domain: general
      severity: s2
    expr: |
      gitlab_component_saturation:ratio{component="disk_inodes",env="gprd",type="patroni-registry"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="disk_inodes",env="gprd",type="patroni-registry"}
  - alert: ComponentResourceRunningOut_disk_inodes
    for: 15m
    annotations:
      title: The Disk inode Utilization per Device per Node resource of the {{ $labels.type
        }} service ({{ $labels.stage }} stage) is on track to hit capacity within
        6h
      description: |
        This means that this resource is growing rapidly and is predicted to exceed saturation threshold within 6h.

        Details of the Disk inode Utilization per Device per Node resource:

        Disk inode utilization per device per node.

        If this is too high, its possible that a directory is filling up with files. Consider logging in an checking temp directories for large numbers of files
      grafana_dashboard_id: alerts-sat_disk_inodes
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_disk_inodes?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "39965907"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard, fqdn, device) (
          clamp_min(
            clamp_max(
              1 - (
                node_filesystem_files_free{fstype=~"(ext.|xfs)", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                /
                node_filesystem_files{fstype=~"(ext.|xfs)", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, tier, type, stage, shard, fqdn, device) (
          clamp_min(
            clamp_max(
              1 - (
                node_filesystem_files_free{fstype=~"(ext.|xfs)", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                /
                node_filesystem_files{fstype=~"(ext.|xfs)", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      linear_prediction_saturation_alert: 6h
      pager: pagerduty
      rules_domain: general
      severity: s2
    expr: |
      predict_linear(gitlab_component_saturation:ratio{component="disk_inodes",env="gprd",type="patroni-registry"}[6h], 21600)
      > on (component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="disk_inodes",env="gprd",type="patroni-registry"}
  - alert: component_saturation_slo_out_of_bounds:disk_space
    for: 15m
    annotations:
      title: The Disk Space Utilization per Device per Node resource of the {{ $labels.type
        }} service ({{ $labels.stage }} stage) has a saturation exceeding SLO and
        is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Disk Space Utilization per Device per Node resource:

        Disk space utilization per device per node.
      grafana_dashboard_id: alerts-sat_disk_space
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_disk_space?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2661375984"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard, fqdn, device) (
          clamp_min(
            clamp_max(
              (
                1 - node_filesystem_avail_bytes{fstype=~"ext.|xfs", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} / node_filesystem_size_bytes{fstype=~"ext.|xfs", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, tier, type, stage, shard, fqdn, device) (
          clamp_min(
            clamp_max(
              (
                1 - node_filesystem_avail_bytes{fstype=~"ext.|xfs", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} / node_filesystem_size_bytes{fstype=~"ext.|xfs", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      pager: pagerduty
      rules_domain: general
      severity: s2
    expr: |
      gitlab_component_saturation:ratio{component="disk_space",env="gprd",type="patroni-registry"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="disk_space",env="gprd",type="patroni-registry"}
  - alert: ComponentResourceRunningOut_disk_space
    for: 15m
    annotations:
      title: The Disk Space Utilization per Device per Node resource of the {{ $labels.type
        }} service ({{ $labels.stage }} stage) is on track to hit capacity within
        6h
      description: |
        This means that this resource is growing rapidly and is predicted to exceed saturation threshold within 6h.

        Details of the Disk Space Utilization per Device per Node resource:

        Disk space utilization per device per node.
      grafana_dashboard_id: alerts-sat_disk_space
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_disk_space?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2661375984"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard, fqdn, device) (
          clamp_min(
            clamp_max(
              (
                1 - node_filesystem_avail_bytes{fstype=~"ext.|xfs", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} / node_filesystem_size_bytes{fstype=~"ext.|xfs", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, tier, type, stage, shard, fqdn, device) (
          clamp_min(
            clamp_max(
              (
                1 - node_filesystem_avail_bytes{fstype=~"ext.|xfs", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} / node_filesystem_size_bytes{fstype=~"ext.|xfs", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      linear_prediction_saturation_alert: 6h
      pager: pagerduty
      rules_domain: general
      severity: s2
    expr: |
      predict_linear(gitlab_component_saturation:ratio{component="disk_space",env="gprd",type="patroni-registry"}[6h], 21600)
      > on (component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="disk_space",env="gprd",type="patroni-registry"}
  - alert: component_saturation_slo_out_of_bounds:memory
    for: 5m
    annotations:
      title: The Memory Utilization per Node resource of the {{ $labels.type }} service
        ({{ $labels.stage }} stage) has a saturation exceeding SLO and is close to
        its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Memory Utilization per Node resource:

        Memory utilization per device per node.
      grafana_dashboard_id: alerts-sat_memory
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_memory?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1955556769"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard, fqdn) (
          clamp_min(
            clamp_max(
              instance:node_memory_utilization:ratio{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} or instance:node_memory_utilisation:ratio{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, tier, type, stage, shard, fqdn) (
          clamp_min(
            clamp_max(
              instance:node_memory_utilization:ratio{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} or instance:node_memory_utilisation:ratio{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
    expr: |
      gitlab_component_saturation:ratio{component="memory",env="gprd",type="patroni-registry"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="memory",env="gprd",type="patroni-registry"}
  - alert: component_saturation_slo_out_of_bounds:nf_conntrack_entries
    for: 5m
    annotations:
      title: The conntrack Entries per Node resource of the {{ $labels.type }} service
        ({{ $labels.stage }} stage) has a saturation exceeding SLO and is close to
        its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the conntrack Entries per Node resource:

        Netfilter connection tracking table utilization per node.

        When saturated, new connection attempts (incoming SYN packets) are dropped with no reply, leaving clients to slowly retry (and typically fail again) over the next several seconds.  When packets are being dropped due to this condition, kernel will log the event as: "nf_conntrack: table full, dropping packet".
      grafana_dashboard_id: alerts-sat_conntrack
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_conntrack?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "503581002"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard, fqdn, instance) (
          clamp_min(
            clamp_max(
              max_over_time(node_nf_conntrack_entries{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[1m])
              /
              node_nf_conntrack_entries_limit{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, tier, type, stage, shard, fqdn, instance) (
          clamp_min(
            clamp_max(
              max_over_time(node_nf_conntrack_entries{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[1m])
              /
              node_nf_conntrack_entries_limit{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s3
    expr: |
      gitlab_component_saturation:ratio{component="nf_conntrack_entries",env="gprd",type="patroni-registry"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="nf_conntrack_entries",env="gprd",type="patroni-registry"}
  - alert: component_saturation_slo_out_of_bounds:node_schedstat_waiting
    for: 90m
    annotations:
      title: The Node Scheduler Waiting Time resource of the {{ $labels.type }} service
        ({{ $labels.stage }} stage) has a saturation exceeding SLO and is close to
        its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Node Scheduler Waiting Time resource:

        Measures the amount of scheduler waiting time that processes are waiting to be scheduled, according to [`CPU Scheduling Metrics`](https://www.robustperception.io/cpu-scheduling-metrics-from-the-node-exporter).

        A high value indicates that a node has more processes to be run than CPU time available to handle them, and may lead to degraded responsiveness and performance from the application.

        Additionally, it may indicate that the fleet is under-provisioned.
      grafana_dashboard_id: alerts-sat_node_schedstat_waiting
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_node_schedstat_waiting?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1415313189"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard, fqdn, shard) (
          clamp_min(
            clamp_max(
              avg without (cpu) (rate(node_schedstat_waiting_seconds_total{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[1h]))
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, tier, type, stage, shard, fqdn, shard) (
          clamp_min(
            clamp_max(
              avg without (cpu) (rate(node_schedstat_waiting_seconds_total{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[1h]))
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
    expr: |
      gitlab_component_saturation:ratio{component="node_schedstat_waiting",env="gprd",type="patroni-registry"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="node_schedstat_waiting",env="gprd",type="patroni-registry"}
  - alert: component_saturation_slo_out_of_bounds:open_fds
    for: 5m
    annotations:
      title: The Open file descriptor utilization per instance resource of the {{
        $labels.type }} service ({{ $labels.stage }} stage) has a saturation exceeding
        SLO and is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Open file descriptor utilization per instance resource:

        Open file descriptor utilization per instance.

        Saturation on file descriptor limits may indicate a resource-descriptor leak in the application.

        As a temporary fix, you may want to consider restarting the affected process.
      grafana_dashboard_id: alerts-sat_open_fds
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_open_fds?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1001792825"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard, job, instance) (
          clamp_min(
            clamp_max(
              (
                process_open_fds{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                /
                process_max_fds{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              or
              (
                ruby_file_descriptors{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                /
                ruby_process_max_fds{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, tier, type, stage, shard, job, instance) (
          clamp_min(
            clamp_max(
              (
                process_open_fds{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                /
                process_max_fds{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              or
              (
                ruby_file_descriptors{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                /
                ruby_process_max_fds{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      pager: pagerduty
      rules_domain: general
      severity: s2
    expr: |
      gitlab_component_saturation:ratio{component="open_fds",env="gprd",type="patroni-registry"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="open_fds",env="gprd",type="patroni-registry"}
  - alert: component_saturation_slo_out_of_bounds:pg_active_db_connections_primary
    for: 5m
    annotations:
      title: The Active Primary DB Connection Utilization resource of the {{ $labels.type
        }} service ({{ $labels.stage }} stage) has a saturation exceeding SLO and
        is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Active Primary DB Connection Utilization resource:

        Active db connection utilization on the primary node.

        Postgres is configured to use a maximum number of connections. When this resource is saturated, connections may queue.
      grafana_dashboard_id: alerts-sat_active_db_conns_primary
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_active_db_conns_primary?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1954311497"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard, fqdn) (
          clamp_min(
            clamp_max(
              sum without (state) (
                pg_stat_activity_count{datname=~"gitlabhq_production|gitlabhq_registry", state!="idle", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} unless on(instance) (pg_replication_is_replica == 1)
              )
              / on (environment, tier, type, stage, shard, fqdn)
              pg_settings_max_connections{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, tier, type, stage, shard, fqdn) (
          clamp_min(
            clamp_max(
              sum without (state) (
                pg_stat_activity_count{datname=~"gitlabhq_production|gitlabhq_registry", state!="idle", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} unless on(instance) (pg_replication_is_replica == 1)
              )
              / on (environment, tier, type, stage, shard, fqdn)
              pg_settings_max_connections{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s3
    expr: |
      gitlab_component_saturation:ratio{component="pg_active_db_connections_primary",env="gprd",type="patroni-registry"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="pg_active_db_connections_primary",env="gprd",type="patroni-registry"}
  - alert: component_saturation_slo_out_of_bounds:pg_active_db_connections_replica
    for: 5m
    annotations:
      title: The Active Secondary DB Connection Utilization resource of the {{ $labels.type
        }} service ({{ $labels.stage }} stage) has a saturation exceeding SLO and
        is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Active Secondary DB Connection Utilization resource:

        Active db connection utilization per replica node

        Postgres is configured to use a maximum number of connections. When this resource is saturated, connections may queue.
      grafana_dashboard_id: alerts-sat_active_db_conns_replica
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_active_db_conns_replica?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3266646533"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard, fqdn) (
          clamp_min(
            clamp_max(
              sum without (state) (
                pg_stat_activity_count{datname=~"gitlabhq_production|gitlabhq_registry", state!="idle", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} and on(instance) (pg_replication_is_replica == 1)
              )
              / on (environment, tier, type, stage, shard, fqdn)
              pg_settings_max_connections{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, tier, type, stage, shard, fqdn) (
          clamp_min(
            clamp_max(
              sum without (state) (
                pg_stat_activity_count{datname=~"gitlabhq_production|gitlabhq_registry", state!="idle", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} and on(instance) (pg_replication_is_replica == 1)
              )
              / on (environment, tier, type, stage, shard, fqdn)
              pg_settings_max_connections{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s3
    expr: |
      gitlab_component_saturation:ratio{component="pg_active_db_connections_replica",env="gprd",type="patroni-registry"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="pg_active_db_connections_replica",env="gprd",type="patroni-registry"}
  - alert: component_saturation_slo_out_of_bounds:pg_btree_bloat
    for: 5m
    annotations:
      title: The Postgres btree bloat resource of the {{ $labels.type }} service ({{
        $labels.stage }} stage) has a saturation exceeding SLO and is close to its
        capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Postgres btree bloat resource:

        This estimates the total bloat in Postgres Btree indexes, as a percentage of total index size.

        IMPORTANT: bloat estimates are rough and depending on table/index structure, can be off for individual indexes, in some cases significantly (10-50%).

        The larger this measure, the more pages will unnecessarily be retrieved during index scans.
      grafana_dashboard_id: alerts-sat_pg_btree_bloat
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_pg_btree_bloat?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2387842464"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard, fqdn) (
          clamp_min(
            clamp_max(
              sum by (environment, tier, type, stage, shard, fqdn) (last_over_time(gitlab_database_bloat_btree_bloat_size{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[1h]))
              /
              sum by (environment, tier, type, stage, shard, fqdn) (last_over_time(gitlab_database_bloat_btree_real_size{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[1h]))
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, tier, type, stage, shard, fqdn) (
          clamp_min(
            clamp_max(
              sum by (environment, tier, type, stage, shard, fqdn) (last_over_time(gitlab_database_bloat_btree_bloat_size{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[1h]))
              /
              sum by (environment, tier, type, stage, shard, fqdn) (last_over_time(gitlab_database_bloat_btree_real_size{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[1h]))
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s3
    expr: |
      gitlab_component_saturation:ratio{component="pg_btree_bloat",env="gprd",type="patroni-registry"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="pg_btree_bloat",env="gprd",type="patroni-registry"}
  - alert: component_saturation_slo_out_of_bounds:pg_primary_cpu
    for: 5m
    annotations:
      title: The Average CPU Utilization on Postgres Primary Instance resource of
        the {{ $labels.type }} service ({{ $labels.stage }} stage) has a saturation
        exceeding SLO and is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Average CPU Utilization on Postgres Primary Instance resource:

        Average CPU utilization across all cores on the Postgres primary instance.
      grafana_dashboard_id: alerts-sat_pg_primary_cpu
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_pg_primary_cpu?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3989464622"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(env, environment, tier, type, stage, shard, fqdn) (
          clamp_min(
            clamp_max(
              avg without(cpu, mode) (
                1
                -
                (
                  rate(node_cpu_seconds_total{mode="idle", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
                  and on(fqdn)
                  pg_replication_is_replica{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} == 0
                )
              )
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(env, environment, tier, type, stage, shard, fqdn) (
          clamp_min(
            clamp_max(
              avg without(cpu, mode) (
                1
                -
                (
                  rate(node_cpu_seconds_total{mode="idle", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
                  and on(fqdn)
                  pg_replication_is_replica{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} == 0
                )
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      pager: pagerduty
      rules_domain: general
      severity: s2
    expr: |
      gitlab_component_saturation:ratio{component="pg_primary_cpu",env="gprd",type="patroni-registry"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="pg_primary_cpu",env="gprd",type="patroni-registry"}
  - alert: component_saturation_slo_out_of_bounds:pg_table_bloat
    for: 5m
    annotations:
      title: The Postgres Table Bloat resource of the {{ $labels.type }} service ({{
        $labels.stage }} stage) has a saturation exceeding SLO and is close to its
        capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Postgres Table Bloat resource:

        This measures the total bloat in Postgres Table pages, as a percentage of total size. This includes bloat in TOAST tables, and excludes extra space reserved due to fillfactor.
      grafana_dashboard_id: alerts-sat_pg_table_bloat
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_pg_table_bloat?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "773890326"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard, fqdn) (
          clamp_min(
            clamp_max(
              sum by (environment, tier, type, stage, shard, fqdn) (avg_over_time(gitlab_database_bloat_table_bloat_size{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[58m]))
              /
              sum by (environment, tier, type, stage, shard, fqdn) (avg_over_time(gitlab_database_bloat_table_real_size{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[58m]))
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, tier, type, stage, shard, fqdn) (
          clamp_min(
            clamp_max(
              sum by (environment, tier, type, stage, shard, fqdn) (avg_over_time(gitlab_database_bloat_table_bloat_size{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[58m]))
              /
              sum by (environment, tier, type, stage, shard, fqdn) (avg_over_time(gitlab_database_bloat_table_real_size{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[58m]))
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
    expr: |
      gitlab_component_saturation:ratio{component="pg_table_bloat",env="gprd",type="patroni-registry"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="pg_table_bloat",env="gprd",type="patroni-registry"}
  - alert: component_saturation_slo_out_of_bounds:pg_txid_vacuum_to_wraparound
    for: 5m
    annotations:
      title: The Total autovacuum time to TXID wraparound horizon resource of the
        {{ $labels.type }} service ({{ $labels.stage }} stage) has a saturation exceeding
        SLO and is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Total autovacuum time to TXID wraparound horizon resource:

        This saturation metric measures the capacity of the Postgres primary instance to perform autovacuum operations on all tables.

        It measures the total time spent in vacuum operations, over a 24 hour period divided the maximum number of autovacuum processes to give the total vacuum activity, in seconds. This value is divided by the TXID wraparound horizon for the database to produce a percentage.

        This value will approach 100% as two situation occur:

        1. The amount of time spent performing autovacuum operations goes up, due to high dead-tuple generation in the database. 1. The write transaction volume goes up, decreasing the wraparound horizon.

        If the total time spent vacuuming approached the wraparound time horizon, this would mean that the database would be at risk of being unable to complete a vacuum of all tables within the wraparound time horizon. This would put the database at risk of XID wraparound and immediate shutdown.
      grafana_dashboard_id: alerts-sat_pg_txid_vac_wraparound
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_pg_txid_vac_wraparound?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "4019870218"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard) (
          clamp_min(
            clamp_max(
              (
                sum by (environment, tier, type, stage, shard) (
                  increase(fluentd_pg_auto_vacuum_elapsed_seconds_total{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[1d])
                )
                / on(environment, type) group_left() (
                  pg_settings_autovacuum_max_workers and
                  on (instance) pg_replication_is_replica == 0
                )
              )
              /
              (
                (2^31 - 10^6)
                /
                (
                  avg by (environment, tier, type, stage, shard) (
                    deriv(pg_txid_current{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[1d]) and
                    on (instance) pg_replication_is_replica == 0
                  )
                )
              )
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, tier, type, stage, shard) (
          clamp_min(
            clamp_max(
              (
                sum by (environment, tier, type, stage, shard) (
                  increase(fluentd_pg_auto_vacuum_elapsed_seconds_total{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[1d])
                )
                / on(environment, type) group_left() (
                  pg_settings_autovacuum_max_workers and
                  on (instance) pg_replication_is_replica == 0
                )
              )
              /
              (
                (2^31 - 10^6)
                /
                (
                  avg by (environment, tier, type, stage, shard) (
                    deriv(pg_txid_current{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[1d]) and
                    on (instance) pg_replication_is_replica == 0
                  )
                )
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      pager: pagerduty
      rules_domain: general
      severity: s1
    expr: |
      gitlab_component_saturation:ratio{component="pg_txid_vacuum_to_wraparound",env="gprd",type="patroni-registry"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="pg_txid_vacuum_to_wraparound",env="gprd",type="patroni-registry"}
  - alert: component_saturation_slo_out_of_bounds:pg_vacuum_activity_v2
    for: 5m
    annotations:
      title: The Postgres Autovacuum Activity (non-sampled) resource of the {{ $labels.type
        }} service ({{ $labels.stage }} stage) has a saturation exceeding SLO and
        is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Postgres Autovacuum Activity (non-sampled) resource:

        This measures the total amount of time spent each day by autovacuum workers, as a percentage of total autovacuum capacity.

        This resource uses the `auto_vacuum_elapsed_seconds` value logged by the autovacuum worker, and aggregates this across all autovacuum jobs. In the case that there are 10 autovacuum workers, the total capacity is 10-days worth of autovacuum time per day.

        Once the system is performing 10 days worth of autovacuum per day, the capacity will be saturated.

        This resource is primarily intended to be used for long-term capacity planning.
      grafana_dashboard_id: alerts-sat_pg_vacuum_activity_v2
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_pg_vacuum_activity_v2?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3412018865"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(env, environment, tier, type, stage, shard) (
          clamp_min(
            clamp_max(
              sum by (env, environment, tier, type, stage, shard) (
                rate(fluentd_pg_auto_vacuum_elapsed_seconds_total{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[1d])
                and on (fqdn) (pg_replication_is_replica{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} == 0)
              )
              /
              avg by (env, environment, tier, type, stage, shard) (
                pg_settings_autovacuum_max_workers{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                and on (instance, job) (pg_replication_is_replica{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} == 0)
              )
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(env, environment, tier, type, stage, shard) (
          clamp_min(
            clamp_max(
              sum by (env, environment, tier, type, stage, shard) (
                rate(fluentd_pg_auto_vacuum_elapsed_seconds_total{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[1d])
                and on (fqdn) (pg_replication_is_replica{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} == 0)
              )
              /
              avg by (env, environment, tier, type, stage, shard) (
                pg_settings_autovacuum_max_workers{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                and on (instance, job) (pg_replication_is_replica{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} == 0)
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s3
    expr: |
      gitlab_component_saturation:ratio{component="pg_vacuum_activity_v2",env="gprd",type="patroni-registry"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="pg_vacuum_activity_v2",env="gprd",type="patroni-registry"}
  - alert: component_saturation_slo_out_of_bounds:pg_walsender_cpu
    for: 5m
    annotations:
      title: The Walsender CPU Saturation resource of the {{ $labels.type }} service
        ({{ $labels.stage }} stage) has a saturation exceeding SLO and is close to
        its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Walsender CPU Saturation resource:

        This saturation metric measures the total amount of time that the primary postgres instance is spending sending WAL segments to replicas. It is expressed as a percentage of all CPU available on the primary postgres instance.

        The more replicas connected, the higher this metric will be.

        Since it's expressed as a percentage of all CPU, this should always remain low, since the CPU primarily needs to be available for handling SQL statements.
      grafana_dashboard_id: alerts-sat_pg_walsender_cpu
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_pg_walsender_cpu?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1879384722"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(env, environment, tier, type, stage, shard) (
          clamp_min(
            clamp_max(
              sum by (env, environment, tier, type, stage, shard) (
                sum by(env, environment, tier, type, stage, shard, fqdn) (
                  rate(namedprocess_namegroup_cpu_seconds_total{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}", groupname=~"pg.worker.walsender|pg.worker.walwriter|wal-g"}[5m])
                  and on (fqdn) (pg_replication_is_replica{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} == 0)
                )
                /
                count by (env, environment, tier, type, stage, shard, fqdn) (
                  node_cpu_seconds_total{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}", mode="idle"} and on(fqdn) (pg_replication_is_replica{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} == 0)
                )
              )
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(env, environment, tier, type, stage, shard) (
          clamp_min(
            clamp_max(
              sum by (env, environment, tier, type, stage, shard) (
                sum by(env, environment, tier, type, stage, shard, fqdn) (
                  rate(namedprocess_namegroup_cpu_seconds_total{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}", groupname=~"pg.worker.walsender|pg.worker.walwriter|wal-g"}[5m])
                  and on (fqdn) (pg_replication_is_replica{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} == 0)
                )
                /
                count by (env, environment, tier, type, stage, shard, fqdn) (
                  node_cpu_seconds_total{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}", mode="idle"} and on(fqdn) (pg_replication_is_replica{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} == 0)
                )
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s3
    expr: |
      gitlab_component_saturation:ratio{component="pg_walsender_cpu",env="gprd",type="patroni-registry"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="pg_walsender_cpu",env="gprd",type="patroni-registry"}
  - alert: component_saturation_slo_out_of_bounds:pg_xid_wraparound
    for: 5m
    annotations:
      title: The Transaction ID Wraparound resource of the {{ $labels.type }} service
        ({{ $labels.stage }} stage) has a saturation exceeding SLO and is close to
        its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Transaction ID Wraparound resource:

        Risk of DB shutdown in the near future, approaching transaction ID wraparound.

        This is a critical situation.

        This saturation metric measures how close the database is to Transaction ID wraparound.

        When wraparound occurs, the database will automatically shutdown to prevent data loss, causing a full outage.

        Recovery would require entering single-user mode to run vacuum, taking the site down for a potentially multi-hour maintenance session.

        To avoid reaching the db shutdown threshold, consider the following short-term actions:

        1. Escalate to the SRE Datastores team, and then,

        2. Find and terminate any very old transactions. The runbook for this alert has details.  Do this first.  It is the most critical step and may be all that is necessary to let autovacuum do its job.

        3. Run a manual vacuum on tables with oldest relfrozenxid.  Manual vacuums run faster than autovacuum.

        4. Add autovacuum workers or reduce autovacuum cost delay, if autovacuum is chronically unable to keep up with the transaction rate.

        Long running transaction dashboard: https://dashboards.gitlab.net/d/alerts-long_running_transactions/alerts-long-running-transactions?orgId=1
      grafana_dashboard_id: alerts-sat_pg_xid_wraparound
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_pg_xid_wraparound?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3666116359"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard, datname) (
          clamp_min(
            clamp_max(
              (
                max without (series) (
                  label_replace(pg_database_wraparound_age_datfrozenxid{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}, "series", "datfrozenxid", "", "")
                  or
                  label_replace(pg_database_wraparound_age_datminmxid{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}, "series", "datminmxid", "", "")
                )
                and on (instance, job) (pg_replication_is_replica{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} == 0)
              )
              /
              (2^31 - 3000000)
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, tier, type, stage, shard, datname) (
          clamp_min(
            clamp_max(
              (
                max without (series) (
                  label_replace(pg_database_wraparound_age_datfrozenxid{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}, "series", "datfrozenxid", "", "")
                  or
                  label_replace(pg_database_wraparound_age_datminmxid{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}, "series", "datminmxid", "", "")
                )
                and on (instance, job) (pg_replication_is_replica{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} == 0)
              )
              /
              (2^31 - 3000000)
              ,
              1)
          ,
          0)
        )
      runbook: docs/patroni/pg_xid_wraparound_alert.md
    labels:
      alert_type: cause
      pager: pagerduty
      rules_domain: general
      severity: s1
    expr: |
      gitlab_component_saturation:ratio{component="pg_xid_wraparound",env="gprd",type="patroni-registry"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="pg_xid_wraparound",env="gprd",type="patroni-registry"}
  - alert: component_saturation_slo_out_of_bounds:pgbouncer_client_conn_replicas
    for: 5m
    annotations:
      title: The PGBouncer Client Connections per Process (Replicas) resource of the
        {{ $labels.type }} service ({{ $labels.stage }} stage) has a saturation exceeding
        SLO and is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the PGBouncer Client Connections per Process (Replicas) resource:

        Client connections per pgbouncer process for Replicas connections.

        pgbouncer is configured to use a `max_client_conn` setting. This limits the total number of client connections per pgbouncer.

        When this limit is reached, client connections may be refused, and `max_client_conn` errors may appear in the pgbouncer logs.

        This could affect users as Rails clients are left unable to connect to the database. Another potential knock-on effect is that Rails clients could fail their readiness checks for extended periods during a deployment, leading to saturation of the older nodes.
      grafana_dashboard_id: alerts-sat_pgb_client_conn_replicas
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_pgb_client_conn_replicas?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3093058894"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard, fqdn) (
          clamp_min(
            clamp_max(
              max_over_time(pgbouncer_used_clients{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
              /
              30000
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, tier, type, stage, shard, fqdn) (
          clamp_min(
            clamp_max(
              max_over_time(pgbouncer_used_clients{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
              /
              30000
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      pager: pagerduty
      rules_domain: general
      severity: s2
    expr: |
      gitlab_component_saturation:ratio{component="pgbouncer_client_conn_replicas",env="gprd",type="patroni-registry"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="pgbouncer_client_conn_replicas",env="gprd",type="patroni-registry"}
  - alert: component_saturation_slo_out_of_bounds:pgbouncer_sync_replica_pool
    for: 10m
    annotations:
      title: The Postgres Sync (Web/API/Git) replica Connection Pool Utilization per
        Node resource of the {{ $labels.type }} service ({{ $labels.stage }} stage)
        has a saturation exceeding SLO and is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Postgres Sync (Web/API/Git) replica Connection Pool Utilization per Node resource:

        pgbouncer sync connection pool Saturation per database node, for replica database connections.

        Web/api/git applications use a separate connection pool to sidekiq.

        When this resource is saturated, web/api database operations may queue, leading to rails worker saturation and 503 errors in the web.
      grafana_dashboard_id: alerts-sat_pgb_sync_pool_replica
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_pgb_sync_pool_replica?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2061891964"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard, fqdn, instance) (
          clamp_min(
            clamp_max(
              (
                avg_over_time(pgbouncer_pools_server_active_connections{user=~"gitlab|gitlab-registry", database=~"gitlabhq_registry|gitlabhq_production", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m]) +
                avg_over_time(pgbouncer_pools_server_testing_connections{user=~"gitlab|gitlab-registry", database=~"gitlabhq_registry|gitlabhq_production", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m]) +
                avg_over_time(pgbouncer_pools_server_used_connections{user=~"gitlab|gitlab-registry", database=~"gitlabhq_registry|gitlabhq_production", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m]) +
                avg_over_time(pgbouncer_pools_server_login_connections{user=~"gitlab|gitlab-registry", database=~"gitlabhq_registry|gitlabhq_production", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
              )
              / on(environment, tier, type, stage, shard, fqdn, instance) group_left()
              sum by (environment, tier, type, stage, shard, fqdn, instance) (
                avg_over_time(pgbouncer_databases_pool_size{name=~"gitlabhq_registry|gitlabhq_production", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
              )
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, tier, type, stage, shard, fqdn, instance) (
          clamp_min(
            clamp_max(
              (
                avg_over_time(pgbouncer_pools_server_active_connections{user=~"gitlab|gitlab-registry", database=~"gitlabhq_registry|gitlabhq_production", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m]) +
                avg_over_time(pgbouncer_pools_server_testing_connections{user=~"gitlab|gitlab-registry", database=~"gitlabhq_registry|gitlabhq_production", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m]) +
                avg_over_time(pgbouncer_pools_server_used_connections{user=~"gitlab|gitlab-registry", database=~"gitlabhq_registry|gitlabhq_production", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m]) +
                avg_over_time(pgbouncer_pools_server_login_connections{user=~"gitlab|gitlab-registry", database=~"gitlabhq_registry|gitlabhq_production", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
              )
              / on(environment, tier, type, stage, shard, fqdn, instance) group_left()
              sum by (environment, tier, type, stage, shard, fqdn, instance) (
                avg_over_time(pgbouncer_databases_pool_size{name=~"gitlabhq_registry|gitlabhq_production", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s3
    expr: |
      gitlab_component_saturation:ratio{component="pgbouncer_sync_replica_pool",env="gprd",type="patroni-registry"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="pgbouncer_sync_replica_pool",env="gprd",type="patroni-registry"}
  - alert: component_saturation_slo_out_of_bounds:shard_cpu
    for: 5m
    annotations:
      title: The Average CPU Utilization per Shard resource of the {{ $labels.type
        }} service ({{ $labels.stage }} stage) has a saturation exceeding SLO and
        is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Average CPU Utilization per Shard resource:

        This resource measures average CPU utilization across an all cores in a shard of a service fleet. If it is becoming saturated, it may indicate that the shard needs horizontal or vertical scaling.
      grafana_dashboard_id: alerts-sat_shard_cpu
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_shard_cpu?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1472933476"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard, shard) (
          clamp_min(
            clamp_max(
              1 - avg by (environment, tier, type, stage, shard, shard) (
                rate(node_cpu_seconds_total{mode="idle", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
              )
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, tier, type, stage, shard, shard) (
          clamp_min(
            clamp_max(
              1 - avg by (environment, tier, type, stage, shard, shard) (
                rate(node_cpu_seconds_total{mode="idle", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s3
    expr: |
      gitlab_component_saturation:ratio{component="shard_cpu",env="gprd",type="patroni-registry"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="shard_cpu",env="gprd",type="patroni-registry"}
  - alert: component_saturation_slo_out_of_bounds:single_node_cpu
    for: 10m
    annotations:
      title: The Average CPU Utilization per Node resource of the {{ $labels.type
        }} service ({{ $labels.stage }} stage) has a saturation exceeding SLO and
        is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Average CPU Utilization per Node resource:

        Average CPU utilization per Node.

        If average CPU is saturated, it may indicate that a fleet is in need to horizontal or vertical scaling. It may also indicate imbalances in load in a fleet.
      grafana_dashboard_id: alerts-sat_single_node_cpu
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_single_node_cpu?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3372411356"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard, fqdn) (
          clamp_min(
            clamp_max(
              avg without(cpu, mode) (1 - rate(node_cpu_seconds_total{mode="idle", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m]))
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, tier, type, stage, shard, fqdn) (
          clamp_min(
            clamp_max(
              avg without(cpu, mode) (1 - rate(node_cpu_seconds_total{mode="idle", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m]))
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
    expr: |
      gitlab_component_saturation:ratio{component="single_node_cpu",env="gprd",type="patroni-registry"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="single_node_cpu",env="gprd",type="patroni-registry"}
