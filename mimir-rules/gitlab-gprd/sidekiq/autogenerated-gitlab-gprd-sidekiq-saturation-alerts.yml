# WARNING. DO NOT EDIT THIS FILE BY HAND. USE ./mimir-rules-jsonnet/saturation.jsonnet TO GENERATE IT
# YOUR CHANGES WILL BE OVERRIDDEN
groups:
- name: GitLab Component Saturation Statistics
  interval: 5m
  rules:
  - record: gitlab_component_saturation:ratio_quantile95_1w
    expr: quantile_over_time(0.95, gitlab_component_saturation:ratio{env="gprd",type="sidekiq"}[1w])
  - record: gitlab_component_saturation:ratio_quantile99_1w
    expr: quantile_over_time(0.99, gitlab_component_saturation:ratio{env="gprd",type="sidekiq"}[1w])
  - record: gitlab_component_saturation:ratio_quantile95_1h
    expr: quantile_over_time(0.95, gitlab_component_saturation:ratio{env="gprd",type="sidekiq"}[1h])
  - record: gitlab_component_saturation:ratio_quantile99_1h
    expr: quantile_over_time(0.99, gitlab_component_saturation:ratio{env="gprd",type="sidekiq"}[1h])
  - record: gitlab_component_saturation:ratio_avg_1h
    expr: avg_over_time(gitlab_component_saturation:ratio{env="gprd",type="sidekiq"}[1h])
- name: GitLab Saturation Alerts
  interval: 1m
  rules:
  - alert: component_saturation_slo_out_of_bounds:excluded_sidekiq_thread_contention
    for: 5m
    annotations:
      title: The Excluded Shards Sidekiq Ruby Thread Contention resource of the {{
        $labels.type }} service ({{ $labels.stage }} stage) has a saturation exceeding
        SLO and is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Excluded Shards Sidekiq Ruby Thread Contention resource:

        Ruby (technically Ruby MRI), like some other scripting languages, uses a Global VM lock (GVL) also known as a Global Interpreter Lock (GIL) to ensure that multiple threads can execute safely. Ruby code is only allowed to execute in one thread in a process at a time. When calling out to c extensions, the thread can cede the lock to other thread while it continues to execute.

        This means that when CPU-bound workloads run in a multithreaded environment such as Puma or Sidekiq, contention with other Ruby worker threads running in the same process can occur, effectively slowing thoses threads down as they await GVL entry.

        Often the best fix for this situation is to add more workers by scaling up the fleet.
      grafana_dashboard_id: alerts-exclude_sidekiq_thread_contention
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-exclude_sidekiq_thread_contention?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_datasource_id: mimir-gitlab-gprd
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2769937795"
      grafana_variables: environment,type,stage
      promql_query: |
        quantile by(environment,fqdn,pod,shard,stage,tier,type) (
          0.99,
          clamp_min(
            clamp_max(
              rate(ruby_process_cpu_seconds_total{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}", shard="memory-bound"}[1h])
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        quantile by(environment,fqdn,pod,shard,stage,tier,type) (
          0.99,
          clamp_min(
            clamp_max(
              rate(ruby_process_cpu_seconds_total{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}", shard="memory-bound"}[1h])
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s3
    expr: |
      gitlab_component_saturation:ratio{component="excluded_sidekiq_thread_contention",env="gprd",type="sidekiq"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="excluded_sidekiq_thread_contention"}
  - alert: component_saturation_slo_out_of_bounds:kube_container_cpu_limit
    for: 15m
    annotations:
      title: The Kube Container CPU over-utilization resource of the {{ $labels.type
        }} service ({{ $labels.stage }} stage) has a saturation exceeding SLO and
        is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Kube Container CPU over-utilization resource:

        Kubernetes containers can have a limit configured on how much CPU they can consume in a burst. If we are at this limit, exceeding the allocated requested resources, we should consider revisting the container's HPA configuration.

        When a container is utilizing CPU resources up-to it's configured limit for extended periods of time, this could cause it and other running containers to be throttled.
      grafana_dashboard_id: alerts-sat_kube_container_cpu_limit
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_kube_container_cpu_limit?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_datasource_id: mimir-gitlab-gprd
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1262336683"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(container,environment,pod,shard,stage,tier,type) (
          clamp_min(
            clamp_max(
              sum by (container,environment,pod,shard,stage,tier,type) (
                rate(container_cpu_usage_seconds_total:labeled{container!="", container!="POD", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
              )
              /
              sum by(container,environment,pod,shard,stage,tier,type) (
                container_spec_cpu_quota:labeled{container!="", container!="POD", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                /
                container_spec_cpu_period:labeled{container!="", container!="POD", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(container,environment,pod,shard,stage,tier,type) (
          clamp_min(
            clamp_max(
              sum by (container,environment,pod,shard,stage,tier,type) (
                rate(container_cpu_usage_seconds_total:labeled{container!="", container!="POD", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
              )
              /
              sum by(container,environment,pod,shard,stage,tier,type) (
                container_spec_cpu_quota:labeled{container!="", container!="POD", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                /
                container_spec_cpu_period:labeled{container!="", container!="POD", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
    expr: |
      gitlab_component_saturation:ratio{component="kube_container_cpu_limit",env="gprd",type="sidekiq"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="kube_container_cpu_limit"}
  - alert: component_saturation_slo_out_of_bounds:kube_container_rss
    for: 15m
    annotations:
      title: The Kube Container Memory Utilization (RSS) resource of the {{ $labels.type
        }} service ({{ $labels.stage }} stage) has a saturation exceeding SLO and
        is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Kube Container Memory Utilization (RSS) resource:

        Records the total anonymous (unevictable) memory utilization for containers for this service, as a percentage of the memory limit as configured through Kubernetes.

        This is computed using the container's resident set size (RSS), as opposed to kube_container_memory which uses the working set size. For our purposes, RSS is the better metric as cAdvisor's working set calculation includes pages from the filesystem cache that can (and will) be evicted before the OOM killer kills the cgroup.

        A container's RSS (anonymous memory usage) is still not precisely what the OOM killer will use, but it's a better approximation of what the container's workload is actually using. RSS metrics can, however, be dramatically inflated if a process in the container uses MADV_FREE (lazy-free) memory. RSS will include the memory that is available to be reclaimed without a page fault, but not currently in use.

        The most common case of OOM kills is for anonymous memory demand to overwhelm the container's memory limit. On swapless hosts, anonymous memory cannot be evicted from the page cache, so when a container's memory usage is mostly anonymous pages, the only remaining option to relieve memory pressure may be the OOM killer.

        As container RSS approaches container memory limit, OOM kills become much more likely. Consequently, this ratio is a good leading indicator of memory saturation and OOM risk.
      grafana_dashboard_id: alerts-sat_kube_container_rss
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_kube_container_rss?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_datasource_id: mimir-gitlab-gprd
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2875690100"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment,shard,stage,tier,type) (
          clamp_min(
            clamp_max(
              container_memory_rss:labeled{container!="", container!="POD", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              /
              (container_spec_memory_limit_bytes:labeled{container!="", container!="POD", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} > 0)
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment,shard,stage,tier,type) (
          clamp_min(
            clamp_max(
              container_memory_rss:labeled{container!="", container!="POD", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              /
              (container_spec_memory_limit_bytes:labeled{container!="", container!="POD", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} > 0)
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
    expr: |
      gitlab_component_saturation:ratio{component="kube_container_rss",env="gprd",type="sidekiq"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="kube_container_rss"}
  - alert: component_saturation_slo_out_of_bounds:kube_container_throttling
    for: 10m
    annotations:
      title: The Kube container throttling resource of the {{ $labels.type }} service
        ({{ $labels.stage }} stage) has a saturation exceeding SLO and is close to
        its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Kube container throttling resource:

        Kube container throttling

        A container will be throttled if it reaches the configured cpu limit for the horizontal pod autoscaler. Or when other containers on the node are overutilizing the the CPU.

        To get around this, consider increasing the limit for this workload, taking into consideration the requested resources.
      grafana_dashboard_id: alerts-kube_container_throttling
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-kube_container_throttling?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_datasource_id: mimir-gitlab-gprd
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "54512634"
      grafana_variables: environment,type,stage
      promql_query: |
        quantile by(container,environment,pod,shard,stage,tier,type) (
          0.99,
          clamp_min(
            clamp_max(
              avg by (container,environment,pod,shard,stage,tier,type)(
                rate(container_cpu_cfs_throttled_periods_total:labeled{container!="", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
                /
                rate(container_cpu_cfs_periods_total:labeled{container!="", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
              )
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        quantile by(container,environment,pod,shard,stage,tier,type) (
          0.99,
          clamp_min(
            clamp_max(
              avg by (container,environment,pod,shard,stage,tier,type)(
                rate(container_cpu_cfs_throttled_periods_total:labeled{container!="", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
                /
                rate(container_cpu_cfs_periods_total:labeled{container!="", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s3
    expr: |
      gitlab_component_saturation:ratio{component="kube_container_throttling",env="gprd",type="sidekiq"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="kube_container_throttling"}
  - alert: component_saturation_slo_out_of_bounds:kube_horizontalpodautoscaler_desired_replicas
    for: 25m
    annotations:
      title: The Horizontal Pod Autoscaler Desired Replicas resource of the {{ $labels.type
        }} service ({{ $labels.stage }} stage) has a saturation exceeding SLO and
        is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Horizontal Pod Autoscaler Desired Replicas resource:

        The [Horizontal Pod Autoscaler](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/) automatically scales the number of Pods in a deployment based on metrics.

        The Horizontal Pod Autoscaler has a configured upper maximum. When this limit is reached, the HPA will not increase the number of pods and other resource saturation (eg, CPU, memory) may occur.
      grafana_dashboard_id: alerts-sat_kube_horizontalpodautoscaler
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_kube_horizontalpodautoscaler?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_datasource_id: mimir-gitlab-gprd
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "351198712"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment,horizontalpodautoscaler,shard,stage,tier,type) (
          clamp_min(
            clamp_max(
              kube_horizontalpodautoscaler_status_desired_replicas:labeled{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}", shard!~"database-throttled|elasticsearch|gitaly-throttled|urgent-authorized-projects", namespace!~"pubsubbeat"}
              /
              kube_horizontalpodautoscaler_spec_max_replicas:labeled{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}", shard!~"database-throttled|elasticsearch|gitaly-throttled|urgent-authorized-projects", namespace!~"pubsubbeat"}
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment,horizontalpodautoscaler,shard,stage,tier,type) (
          clamp_min(
            clamp_max(
              kube_horizontalpodautoscaler_status_desired_replicas:labeled{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}", shard!~"database-throttled|elasticsearch|gitaly-throttled|urgent-authorized-projects", namespace!~"pubsubbeat"}
              /
              kube_horizontalpodautoscaler_spec_max_replicas:labeled{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}", shard!~"database-throttled|elasticsearch|gitaly-throttled|urgent-authorized-projects", namespace!~"pubsubbeat"}
              ,
              1)
          ,
          0)
        )
      runbook: docs/kube/kubernetes.md#hpascalecapability
    labels:
      alert_type: cause
      rules_domain: general
      severity: s3
    expr: |
      gitlab_component_saturation:ratio{component="kube_horizontalpodautoscaler_desired_replicas",env="gprd",type="sidekiq"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="kube_horizontalpodautoscaler_desired_replicas"}
  - alert: component_saturation_slo_out_of_bounds:open_fds
    for: 5m
    annotations:
      title: The Open file descriptor utilization per instance resource of the {{
        $labels.type }} service ({{ $labels.stage }} stage) has a saturation exceeding
        SLO and is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Open file descriptor utilization per instance resource:

        Open file descriptor utilization per instance.

        Saturation on file descriptor limits may indicate a resource-descriptor leak in the application.

        As a temporary fix, you may want to consider restarting the affected process.
      grafana_dashboard_id: alerts-sat_open_fds
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_open_fds?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_datasource_id: mimir-gitlab-gprd
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1001792825"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment,instance,job,shard,stage,tier,type) (
          clamp_min(
            clamp_max(
              (
                process_open_fds{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                /
                process_max_fds{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              or
              (
                ruby_file_descriptors{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                /
                ruby_process_max_fds{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment,instance,job,shard,stage,tier,type) (
          clamp_min(
            clamp_max(
              (
                process_open_fds{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                /
                process_max_fds{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              or
              (
                ruby_file_descriptors{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                /
                ruby_process_max_fds{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      pager: pagerduty
      rules_domain: general
      severity: s2
    expr: |
      gitlab_component_saturation:ratio{component="open_fds",env="gprd",type="sidekiq"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="open_fds"}
  - alert: component_saturation_slo_out_of_bounds:rails_db_connection_pool
    for: 15m
    annotations:
      title: The Rails DB Connection Pool Utilization resource of the {{ $labels.type
        }} service ({{ $labels.stage }} stage) has a saturation exceeding SLO and
        is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Rails DB Connection Pool Utilization resource:

        Rails uses connection pools for its database connections. As each node may have multiple connection pools, this is by node and by database host.

        Read more about this resource in our [documentation](https://docs.gitlab.com/ee/development/database/client_side_connection_pool.html#client-side-connection-pool).

        If this resource is saturated, it may indicate that our connection pools are not correctly sized, perhaps because an unexpected application thread is using a database connection.
      grafana_dashboard_id: alerts-sat_rails_db_connection_pool
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_rails_db_connection_pool?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_datasource_id: mimir-gitlab-gprd
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "391047339"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment,host,instance,port,shard,stage,tier,type) (
          clamp_min(
            clamp_max(
              (
                avg_over_time(gitlab_database_connection_pool_busy{class="ActiveRecord::Base", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
                +
                avg_over_time(gitlab_database_connection_pool_dead{class="ActiveRecord::Base", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
              )
              /
              gitlab_database_connection_pool_size{class="ActiveRecord::Base", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment,host,instance,port,shard,stage,tier,type) (
          clamp_min(
            clamp_max(
              (
                avg_over_time(gitlab_database_connection_pool_busy{class="ActiveRecord::Base", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
                +
                avg_over_time(gitlab_database_connection_pool_dead{class="ActiveRecord::Base", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
              )
              /
              gitlab_database_connection_pool_size{class="ActiveRecord::Base", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
    expr: |
      gitlab_component_saturation:ratio{component="rails_db_connection_pool",env="gprd",type="sidekiq"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="rails_db_connection_pool"}
  - alert: component_saturation_slo_out_of_bounds:sidekiq_shard_workers
    for: 10m
    annotations:
      title: The Sidekiq Worker Utilization per shard resource of the {{ $labels.type
        }} service ({{ $labels.stage }} stage) has a saturation exceeding SLO and
        is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Sidekiq Worker Utilization per shard resource:

        Sidekiq worker utilization per shard.

        This metric represents the percentage of available threads*workers that are actively processing jobs.

        When this metric is saturated, new Sidekiq jobs will queue. Depending on whether or not the jobs are latency sensitive, this could impact user experience.
      grafana_dashboard_id: alerts-sat_sidekiq_shard_workers
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_sidekiq_shard_workers?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_datasource_id: mimir-gitlab-gprd
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "836813313"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment,shard,stage,tier,type) (
          clamp_min(
            clamp_max(
              sum by (environment,shard,stage,tier,type) (
                avg_over_time(sidekiq_running_jobs{shard!~"database-throttled|elasticsearch|gitaly-throttled|urgent-authorized-projects", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
              )
              /
              sum by (environment,shard,stage,tier,type) (
                avg_over_time(sidekiq_concurrency{shard!~"database-throttled|elasticsearch|gitaly-throttled|urgent-authorized-projects", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
              )
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment,shard,stage,tier,type) (
          clamp_min(
            clamp_max(
              sum by (environment,shard,stage,tier,type) (
                avg_over_time(sidekiq_running_jobs{shard!~"database-throttled|elasticsearch|gitaly-throttled|urgent-authorized-projects", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
              )
              /
              sum by (environment,shard,stage,tier,type) (
                avg_over_time(sidekiq_concurrency{shard!~"database-throttled|elasticsearch|gitaly-throttled|urgent-authorized-projects", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
    expr: |
      gitlab_component_saturation:ratio{component="sidekiq_shard_workers",env="gprd",type="sidekiq"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="sidekiq_shard_workers"}
  - alert: component_saturation_slo_out_of_bounds:sidekiq_thread_contention
    for: 5m
    annotations:
      title: The Sidekiq Ruby Thread Contention resource of the {{ $labels.type }}
        service ({{ $labels.stage }} stage) has a saturation exceeding SLO and is
        close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Sidekiq Ruby Thread Contention resource:

        Ruby (technically Ruby MRI), like some other scripting languages, uses a Global VM lock (GVL) also known as a Global Interpreter Lock (GIL) to ensure that multiple threads can execute safely. Ruby code is only allowed to execute in one thread in a process at a time. When calling out to c extensions, the thread can cede the lock to other thread while it continues to execute.

        This means that when CPU-bound workloads run in a multithreaded environment such as Puma or Sidekiq, contention with other Ruby worker threads running in the same process can occur, effectively slowing thoses threads down as they await GVL entry.

        Often the best fix for this situation is to add more workers by scaling up the fleet.
      grafana_dashboard_id: alerts-sat_sidekiq_thread_contention
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_sidekiq_thread_contention?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_datasource_id: mimir-gitlab-gprd
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2783215903"
      grafana_variables: environment,type,stage
      promql_query: |
        quantile by(environment,fqdn,pod,shard,stage,tier,type) (
          0.99,
          clamp_min(
            clamp_max(
              rate(ruby_process_cpu_seconds_total{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}", shard!~"memory-bound"}[1h])
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        quantile by(environment,fqdn,pod,shard,stage,tier,type) (
          0.99,
          clamp_min(
            clamp_max(
              rate(ruby_process_cpu_seconds_total{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}", shard!~"memory-bound"}[1h])
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s3
    expr: |
      gitlab_component_saturation:ratio{component="sidekiq_thread_contention",env="gprd",type="sidekiq"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="sidekiq_thread_contention"}
