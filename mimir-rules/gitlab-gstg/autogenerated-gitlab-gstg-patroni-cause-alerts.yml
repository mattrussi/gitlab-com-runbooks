# WARNING. DO NOT EDIT THIS FILE BY HAND. USE ./mimir-rules-jsonnet/patroni-cause-alerts.jsonnet TO GENERATE IT
# YOUR CHANGES WILL BE OVERRIDDEN
groups:
- name: patroni_cause_alerts
  rules:
  - alert: PostgreSQL_HotSpotTupleFetchingPrimary
    for: 10m
    annotations:
      title: Hot spot tuple fetches on the postgres primary `{{ $labels.fqdn }}` in
        the `{{ $labels.relname }}` table, `{{ $labels.relname }}`.
      description: |
        More than 50% of all tuple fetches on postgres primary `{{ $labels.fqdn }}` are for a single table.

        This may indicate that the query optimizer is using incorrect statistics to execute a query.

        This could be due to vacuum and analyze commands (issued either automatically or manually) against this table or closely related table. As a new step, check which tables were analyzed and vacuumed immediately prior to this incident.

        <https://log.gprd.gitlab.net/app/kibana#/discover?_a=(columns:!(json.hostname,json.endpoint_id,json.error_severity,json.message,json.session_start_time,json.sql_state_code,json.duration_s,json.sql),filters:!((meta:(index:'97f04200-024b-11eb-81e5-155ba78758d4',key:json.sql,params:'{{$labels.relname}}',type:phrase),query:(match:(json.sql:(query:'{{$labels.relname}}',type:phrase)))),(meta:(index:'97f04200-024b-11eb-81e5-155ba78758d4',key:json.fqdn,params:'{{$labels.fqdn}}',type:phrase),query:(match:(json.fqdn:(query:'{{$labels.fqdn}}',type:phrase))))),index:'97f04200-024b-11eb-81e5-155ba78758d4')|postgres slowlog in Kibana>

        Previous incidents of this type include <https://gitlab.com/gitlab-com/gl-infra/production/-/issues/2885> and <https://gitlab.com/gitlab-com/gl-infra/production/-/issues/3875>.
      grafana_dashboard_id: alerts-pg_user_tables_primary/alerts-pg-user-table-alerts-primary
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-pg_user_tables_primary/alerts-pg-user-table-alerts-primary?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-tier={{ $labels.tier }}&var-type={{ $labels.type
        }}&var-fqdn={{ $labels.fqdn }}&var-relname={{ $labels.relname }}
      grafana_datasource_id: mimir-gitlab-gstg
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2"
      grafana_variables: environment,tier,type,fqdn,relname
      runbook: docs/patroni/rails-sql-apdex-slow.md
    labels:
      alert_type: cause
      severity: s4
    expr: |
      (
        sum by (environment,tier,type,fqdn,relname) (
          rate(pg_stat_user_tables_idx_tup_fetch{env="gstg",shard="default",tier="db",type!~".*archive",type!~".*delayed"}[5m])
          and on(job, instance)
          pg_replication_is_replica == 0
        )
        / ignoring(relname) group_left()
          sum by (environment,tier,type,fqdn) (
            rate(pg_stat_user_tables_idx_tup_fetch{env="gstg",shard="default",tier="db",type!~".*archive",type!~".*delayed"}[5m])
            and on(job, instance)
            pg_replication_is_replica == 0
        )
      ) > 0.5
  - alert: PostgreSQL_HotSpotTupleFetchingReplicas
    for: 10m
    annotations:
      title: Hot spot tuple fetches on the postgres postgres replicas in the `{{ $labels.relname
        }}` table, `{{ $labels.relname }}`.
      description: |
        More than 50% of all tuple fetches on postgres postgres replicas are for a single table.

        This may indicate that the query optimizer is using incorrect statistics to execute a query.

        This could be due to vacuum and analyze commands (issued either automatically or manually) against this table or closely related table. As a new step, check which tables were analyzed and vacuumed immediately prior to this incident.

        <https://log.gprd.gitlab.net/app/kibana#/discover?_a=(columns:!(json.hostname,json.endpoint_id,json.error_severity,json.message,json.session_start_time,json.sql_state_code,json.duration_s,json.sql),filters:!((meta:(index:'97f04200-024b-11eb-81e5-155ba78758d4',key:json.sql,params:'{{$labels.relname}}',type:phrase),query:(match:(json.sql:(query:'{{$labels.relname}}',type:phrase))))),index:'97f04200-024b-11eb-81e5-155ba78758d4')|postgres slowlog in Kibana>

        Previous incidents of this type include <https://gitlab.com/gitlab-com/gl-infra/production/-/issues/2885> and <https://gitlab.com/gitlab-com/gl-infra/production/-/issues/3875>.
      grafana_dashboard_id: alerts-pg_user_tables_replica/alerts-pg-user-table-alerts-replicas
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-pg_user_tables_replica/alerts-pg-user-table-alerts-replicas?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-tier={{ $labels.tier }}&var-type={{ $labels.type
        }}&var-relname={{ $labels.relname }}
      grafana_datasource_id: mimir-gitlab-gstg
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2"
      grafana_variables: environment,tier,type,relname
      runbook: docs/patroni/rails-sql-apdex-slow.md
    labels:
      alert_type: cause
      severity: s4
    expr: |
      (
        sum by (environment,tier,type,relname) (
          rate(pg_stat_user_tables_idx_tup_fetch{env="gstg",shard="default",tier="db",type!~".*archive",type!~".*delayed"}[5m])
          and on(job, instance)
          pg_replication_is_replica == 1
        )
        / ignoring(relname) group_left()
          sum by (environment,tier,type) (
            rate(pg_stat_user_tables_idx_tup_fetch{env="gstg",shard="default",tier="db",type!~".*archive",type!~".*delayed"}[5m])
            and on(job, instance)
            pg_replication_is_replica == 1
        )
      ) > 0.5
  - alert: PostgreSQLAccessGroupTupleFetchesWarningTrigger
    for: 1h
    annotations:
      title: Average fetches on the postgres primary in the project_authorizations
        table exceeds 5% of total
      description: |
        More than 5% of all tuple fetches on the postgres primary are for the `project_authorizations` table.

        This work was previously addressed through the epic https://gitlab.com/groups/gitlab-org/-/epics/3343#note_652970688.

        The Authentication and authorization team should work to understand why this is happening and look to address the problem.
      grafana_dashboard_id: alerts-pg_user_tables_primary/alerts-pg-user-table-alerts-primary
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-pg_user_tables_primary/alerts-pg-user-table-alerts-primary?from=now-24h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-tier={{ $labels.tier }}&var-type={{ $labels.type
        }}&var-fqdn={{ $labels.fqdn }}&var-relname={{ $labels.relname }}
      grafana_datasource_id: mimir-gitlab-gstg
      grafana_min_zoom_hours: "24"
      grafana_panel_id: "2"
      grafana_variables: environment,tier,type,fqdn,relname
      runbook: docs/patroni/rails-sql-apdex-slow.md
    labels:
      alert_type: cause
      severity: s4
      team: authentication
    expr: |
      (
        sum by (environment,tier,type,fqdn,relname) (
          rate(pg_stat_user_tables_idx_tup_fetch{env="gstg",relname="project_authorizations",shard="default",tier="db",type!~".*archive",type!~".*delayed"}[1d])
          and on(job, instance)
          pg_replication_is_replica == 0
        )
        / ignoring(relname) group_left()
          sum by (environment,tier,type,fqdn) (
            rate(pg_stat_user_tables_idx_tup_fetch{env="gstg",shard="default",tier="db",type!~".*archive",type!~".*delayed"}[1d])
            and on(job, instance)
            pg_replication_is_replica == 0
        )
      ) > 0.05
  - alert: PatroniSubtransControlLocksDetected
    for: 5m
    annotations:
      title: Subtransactions wait events have been detected in the database in the
        last 5 minutes
      description: |
        Wait events related to subtransactions locking have been detected in the database in the last 5 minutes.

        This can eventually saturate entire database cluster if this situation continues for a longer period of time.
      grafana_datasource_id: mimir-gitlab-gstg
      runbook: docs/patroni/postgresql-subtransactions.md
    labels:
      alert_type: cause
      severity: s3
      team: subtransaction_troubleshooting
    expr: |
      sum by (environment) (
        sum_over_time(pg_stat_activity_marginalia_sampler_active_count{env="gstg",wait_event=~"[Ss]ubtrans.*"}[10m])
      ) > 10
  - alert: PatroniLongRunningTransactionDetected
    for: 1m
    annotations:
      title: Transactions detected that have been running on `{{ $labels.fqdn }}`
        for more than 10m
      description: |
        Endpoint `{{ $labels.endpoint }}` on `{{ $labels.application }}` is executing a transaction that has been running for more than 10m. This could lead to dead-tuples and performance degradation in our Patroni fleet.

        Ideally, no transaction should remain open for more than a few seconds.
      grafana_dashboard_id: alerts-long_running_transactions/alerts-long-running-transactions
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-long_running_transactions/alerts-long-running-transactions?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}
      grafana_datasource_id: mimir-gitlab-gstg
      grafana_min_zoom_hours: "6"
      grafana_variables: environment
      runbook: docs/patroni/alerts/PatroniLongRunningTransactionsDetected.md
    labels:
      alert_type: cause
      pager: pagerduty
      severity: s2
    expr: |
      topk by (environment, env, type, stage, shard) (1,
        max by (environment, env, type, stage, shard, application, endpoint, fqdn) (
          pg_stat_activity_marginalia_sampler_max_tx_age_in_seconds{command!="autovacuum",command!~"(?i:VACUUM)",command!~"(?i:CREATE)",command!~"(?i:ANALYZE)",command!~"(?i:REINDEX)",command!~"(?i:ALTER)",command!~"(?i:DROP)",env="gstg",shard="default",tier="db",type!~".*archive",type!~".*delayed"}
        )
        > 540
      )
