# WARNING. DO NOT EDIT THIS FILE BY HAND. USE ./mimir-rules-jsonnet/saturation.jsonnet TO GENERATE IT
# YOUR CHANGES WILL BE OVERRIDDEN
groups:
- name: GitLab Component Saturation Statistics
  interval: 5m
  rules:
  - record: gitlab_component_saturation:ratio_quantile95_1w
    expr: quantile_over_time(0.95, gitlab_component_saturation:ratio{env="gstg",type="kube"}[1w])
  - record: gitlab_component_saturation:ratio_quantile99_1w
    expr: quantile_over_time(0.99, gitlab_component_saturation:ratio{env="gstg",type="kube"}[1w])
  - record: gitlab_component_saturation:ratio_quantile95_1h
    expr: quantile_over_time(0.95, gitlab_component_saturation:ratio{env="gstg",type="kube"}[1h])
  - record: gitlab_component_saturation:ratio_quantile99_1h
    expr: quantile_over_time(0.99, gitlab_component_saturation:ratio{env="gstg",type="kube"}[1h])
  - record: gitlab_component_saturation:ratio_avg_1h
    expr: avg_over_time(gitlab_component_saturation:ratio{env="gstg",type="kube"}[1h])
- name: GitLab Saturation Alerts
  interval: 1m
  rules:
  - alert: component_saturation_slo_out_of_bounds:kube_container_cpu_limit
    for: 15m
    annotations:
      grafana_datasource_id: ee37ad60-dcd5-4699-98ff-949aca509226
      title: The Kube Container CPU over-utilization resource of the {{ $labels.type
        }} service ({{ $labels.stage }} stage) has a saturation exceeding SLO and
        is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Kube Container CPU over-utilization resource:

        Kubernetes containers can have a limit configured on how much CPU they can consume in a burst. If we are at this limit, exceeding the allocated requested resources, we should consider revisting the container's HPA configuration.

        When a container is utilizing CPU resources up-to it's configured limit for extended periods of time, this could cause it and other running containers to be throttled.
      grafana_dashboard_id: alerts-sat_kube_container_cpu_limit
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_kube_container_cpu_limit?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1262336683"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard, pod, container) (
          clamp_min(
            clamp_max(
              sum by (environment, tier, type, stage, shard, pod, container) (
                rate(container_cpu_usage_seconds_total:labeled{container!="", container!="POD", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
              )
              /
              sum by(environment, tier, type, stage, shard, pod, container) (
                container_spec_cpu_quota:labeled{container!="", container!="POD", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                /
                container_spec_cpu_period:labeled{container!="", container!="POD", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, tier, type, stage, shard, pod, container) (
          clamp_min(
            clamp_max(
              sum by (environment, tier, type, stage, shard, pod, container) (
                rate(container_cpu_usage_seconds_total:labeled{container!="", container!="POD", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
              )
              /
              sum by(environment, tier, type, stage, shard, pod, container) (
                container_spec_cpu_quota:labeled{container!="", container!="POD", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                /
                container_spec_cpu_period:labeled{container!="", container!="POD", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
    expr: |
      gitlab_component_saturation:ratio{component="kube_container_cpu_limit",env="gstg",type="kube"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="kube_container_cpu_limit"}
  - alert: component_saturation_slo_out_of_bounds:kube_container_memory
    for: 15m
    annotations:
      grafana_datasource_id: ee37ad60-dcd5-4699-98ff-949aca509226
      title: The Kube Container Memory Utilization resource of the {{ $labels.type
        }} service ({{ $labels.stage }} stage) has a saturation exceeding SLO and
        is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Kube Container Memory Utilization resource:

        This uses the working set size from cAdvisor for the cgroup's memory usage. That may not be a good measure as it includes filesystem cache pages that are not necessarily attributable to the application inside the cgroup, and are permitted to be evicted instead of being OOM killed.
      grafana_dashboard_id: alerts-sat_kube_container_memory
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_kube_container_memory?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "172578411"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard) (
          clamp_min(
            clamp_max(
              container_memory_working_set_bytes:labeled{container!="", container!="POD", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              /
              (container_spec_memory_limit_bytes:labeled{container!="", container!="POD", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} > 0)
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, tier, type, stage, shard) (
          clamp_min(
            clamp_max(
              container_memory_working_set_bytes:labeled{container!="", container!="POD", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              /
              (container_spec_memory_limit_bytes:labeled{container!="", container!="POD", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} > 0)
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
    expr: |
      gitlab_component_saturation:ratio{component="kube_container_memory",env="gstg",type="kube"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="kube_container_memory"}
  - alert: component_saturation_slo_out_of_bounds:kube_container_throttling
    for: 10m
    annotations:
      grafana_datasource_id: ee37ad60-dcd5-4699-98ff-949aca509226
      title: The Kube container throttling resource of the {{ $labels.type }} service
        ({{ $labels.stage }} stage) has a saturation exceeding SLO and is close to
        its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Kube container throttling resource:

        Kube container throttling

        A container will be throttled if it reaches the configured cpu limit for the horizontal pod autoscaler. Or when other containers on the node are overutilizing the the CPU.

        To get around this, consider increasing the limit for this workload, taking into consideration the requested resources.
      grafana_dashboard_id: alerts-kube_container_throttling
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-kube_container_throttling?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "54512634"
      grafana_variables: environment,type,stage
      promql_query: |
        quantile by(environment, tier, type, stage, shard, pod, container) (
          0.99,
          clamp_min(
            clamp_max(
              avg by (environment, tier, type, stage, shard, pod, container)(
                rate(container_cpu_cfs_throttled_periods_total:labeled{container!="", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
                /
                rate(container_cpu_cfs_periods_total:labeled{container!="", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
              )
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        quantile by(environment, tier, type, stage, shard, pod, container) (
          0.99,
          clamp_min(
            clamp_max(
              avg by (environment, tier, type, stage, shard, pod, container)(
                rate(container_cpu_cfs_throttled_periods_total:labeled{container!="", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
                /
                rate(container_cpu_cfs_periods_total:labeled{container!="", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s3
    expr: |
      gitlab_component_saturation:ratio{component="kube_container_throttling",env="gstg",type="kube"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="kube_container_throttling"}
  - alert: component_saturation_slo_out_of_bounds:kube_horizontalpodautoscaler_desired_replicas
    for: 25m
    annotations:
      grafana_datasource_id: ee37ad60-dcd5-4699-98ff-949aca509226
      title: The Horizontal Pod Autoscaler Desired Replicas resource of the {{ $labels.type
        }} service ({{ $labels.stage }} stage) has a saturation exceeding SLO and
        is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Horizontal Pod Autoscaler Desired Replicas resource:

        The [Horizontal Pod Autoscaler](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/) automatically scales the number of Pods in a deployment based on metrics.

        The Horizontal Pod Autoscaler has a configured upper maximum. When this limit is reached, the HPA will not increase the number of pods and other resource saturation (eg, CPU, memory) may occur.
      grafana_dashboard_id: alerts-sat_kube_horizontalpodautoscaler
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_kube_horizontalpodautoscaler?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "351198712"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard, horizontalpodautoscaler, shard) (
          clamp_min(
            clamp_max(
              kube_horizontalpodautoscaler_status_desired_replicas:labeled{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}", shard!~"database-throttled|elasticsearch|gitaly-throttled|urgent-authorized-projects|urgent-other", namespace!~"pubsubbeat"}
              /
              kube_horizontalpodautoscaler_spec_max_replicas:labeled{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}", shard!~"database-throttled|elasticsearch|gitaly-throttled|urgent-authorized-projects|urgent-other", namespace!~"pubsubbeat"}
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, tier, type, stage, shard, horizontalpodautoscaler, shard) (
          clamp_min(
            clamp_max(
              kube_horizontalpodautoscaler_status_desired_replicas:labeled{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}", shard!~"database-throttled|elasticsearch|gitaly-throttled|urgent-authorized-projects|urgent-other", namespace!~"pubsubbeat"}
              /
              kube_horizontalpodautoscaler_spec_max_replicas:labeled{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}", shard!~"database-throttled|elasticsearch|gitaly-throttled|urgent-authorized-projects|urgent-other", namespace!~"pubsubbeat"}
              ,
              1)
          ,
          0)
        )
      runbook: docs/kube/kubernetes.md#hpascalecapability
    labels:
      alert_type: cause
      rules_domain: general
      severity: s3
    expr: |
      gitlab_component_saturation:ratio{component="kube_horizontalpodautoscaler_desired_replicas",env="gstg",type="kube"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="kube_horizontalpodautoscaler_desired_replicas"}
  - alert: component_saturation_slo_out_of_bounds:kube_node_ips
    for: 5m
    annotations:
      grafana_datasource_id: ee37ad60-dcd5-4699-98ff-949aca509226
      title: The Node IP subnet saturation resource of the {{ $labels.type }} service
        ({{ $labels.stage }} stage) has a saturation exceeding SLO and is close to
        its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Node IP subnet saturation resource:

        This resource measures the number of nodes per subnet.

        If it is becoming saturated, it may indicate that clusters need to be rebuilt with a larger subnet.
      grafana_dashboard_id: alerts-sat_kube_node_ips
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_kube_node_ips?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "776071338"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(env, environment, shard, cluster) (
          clamp_min(
            clamp_max(
              sum(:kube_pod_info_node_count:{environment="{{ $labels.environment }}"}) by (env, environment, shard, cluster)
              /
              sum(
                gitlab:gcp_subnet_max_ips{environment="{{ $labels.environment }}"} * on (subnet) group_right gitlab:cluster:subnet:mapping{environment="{{ $labels.environment }}"}
              ) by (env, environment, shard, cluster)
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(env, environment, shard, cluster) (
          clamp_min(
            clamp_max(
              sum(:kube_pod_info_node_count:{environment="{{ $labels.environment }}"}) by (env, environment, shard, cluster)
              /
              sum(
                gitlab:gcp_subnet_max_ips{environment="{{ $labels.environment }}"} * on (subnet) group_right gitlab:cluster:subnet:mapping{environment="{{ $labels.environment }}"}
              ) by (env, environment, shard, cluster)
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s3
    expr: |
      gitlab_component_saturation:ratio{component="kube_node_ips",env="gstg",type="kube"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="kube_node_ips"}
  - alert: component_saturation_slo_out_of_bounds:kube_persistent_volume_claim_disk_space
    for: 5m
    annotations:
      grafana_datasource_id: ee37ad60-dcd5-4699-98ff-949aca509226
      title: The Kube Persistent Volume Claim Space Utilisation resource of the {{
        $labels.type }} service ({{ $labels.stage }} stage) has a saturation exceeding
        SLO and is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Kube Persistent Volume Claim Space Utilisation resource:

        disk space utilization on persistent volume claims.
      grafana_dashboard_id: alerts-sat_kube_pvc_disk_space
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_kube_pvc_disk_space?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "4167694322"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, shard, cluster, namespace, persistentvolumeclaim) (
          clamp_min(
            clamp_max(
              kubelet_volume_stats_used_bytes{persistentvolumeclaim!~"^zoekt.*$"}
              /
              kubelet_volume_stats_capacity_bytes{persistentvolumeclaim!~"^zoekt.*$"}
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, shard, cluster, namespace, persistentvolumeclaim) (
          clamp_min(
            clamp_max(
              kubelet_volume_stats_used_bytes{persistentvolumeclaim!~"^zoekt.*$"}
              /
              kubelet_volume_stats_capacity_bytes{persistentvolumeclaim!~"^zoekt.*$"}
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      pager: pagerduty
      rules_domain: general
      severity: s2
    expr: |
      gitlab_component_saturation:ratio{component="kube_persistent_volume_claim_disk_space",env="gstg",type="kube"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="kube_persistent_volume_claim_disk_space"}
  - alert: component_saturation_slo_out_of_bounds:kube_persistent_volume_claim_disk_space_zoekt
    for: 5m
    annotations:
      grafana_datasource_id: ee37ad60-dcd5-4699-98ff-949aca509226
      title: The Kube Persistent Volume Claim Space Utilisation resource of the {{
        $labels.type }} service ({{ $labels.stage }} stage) has a saturation exceeding
        SLO and is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Kube Persistent Volume Claim Space Utilisation resource:

        disk space utilization on persistent volume claims.
      grafana_dashboard_id: alerts-sat_zoekt_kube_pvc_disk_space
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_zoekt_kube_pvc_disk_space?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "581146623"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, shard, cluster, namespace, persistentvolumeclaim) (
          clamp_min(
            clamp_max(
              kubelet_volume_stats_used_bytes{persistentvolumeclaim=~"^zoekt.*$"}
              /
              kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=~"^zoekt.*$"}
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, shard, cluster, namespace, persistentvolumeclaim) (
          clamp_min(
            clamp_max(
              kubelet_volume_stats_used_bytes{persistentvolumeclaim=~"^zoekt.*$"}
              /
              kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=~"^zoekt.*$"}
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      pager: pagerduty
      rules_domain: general
      severity: s2
    expr: |
      gitlab_component_saturation:ratio{component="kube_persistent_volume_claim_disk_space_zoekt",env="gstg",type="kube"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="kube_persistent_volume_claim_disk_space_zoekt"}
  - alert: component_saturation_slo_out_of_bounds:kube_persistent_volume_claim_inodes
    for: 5m
    annotations:
      grafana_datasource_id: ee37ad60-dcd5-4699-98ff-949aca509226
      title: The Kube Persistent Volume Claim inode Utilisation resource of the {{
        $labels.type }} service ({{ $labels.stage }} stage) has a saturation exceeding
        SLO and is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Kube Persistent Volume Claim inode Utilisation resource:

        inode utilization on persistent volume claims.
      grafana_dashboard_id: alerts-sat_kube_pvc_inodes
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_kube_pvc_inodes?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3153876074"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, shard, persistentvolumeclaim) (
          clamp_min(
            clamp_max(
              kubelet_volume_stats_inodes_used
              /
              kubelet_volume_stats_inodes
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, shard, persistentvolumeclaim) (
          clamp_min(
            clamp_max(
              kubelet_volume_stats_inodes_used
              /
              kubelet_volume_stats_inodes
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      pager: pagerduty
      rules_domain: general
      severity: s2
    expr: |
      gitlab_component_saturation:ratio{component="kube_persistent_volume_claim_inodes",env="gstg",type="kube"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="kube_persistent_volume_claim_inodes"}
  - alert: component_saturation_slo_out_of_bounds:kube_pool_cpu
    for: 5m
    annotations:
      grafana_datasource_id: ee37ad60-dcd5-4699-98ff-949aca509226
      title: The Average Node Pool CPU Utilization resource of the {{ $labels.type
        }} service ({{ $labels.stage }} stage) has a saturation exceeding SLO and
        is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Average Node Pool CPU Utilization resource:

        This resource measures average CPU utilization across an all cores in the node pool for a service fleet.

        If it is becoming saturated, it may indicate that the fleet needs horizontal scaling.
      grafana_dashboard_id: alerts-sat_kube_pool_cpu
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_kube_pool_cpu?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1839360107"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard) (
          clamp_min(
            clamp_max(
              1 - avg by (environment, tier, type, stage, shard) (
                rate(node_cpu_seconds_total:labeled{mode="idle", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
              )
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, tier, type, stage, shard) (
          clamp_min(
            clamp_max(
              1 - avg by (environment, tier, type, stage, shard) (
                rate(node_cpu_seconds_total:labeled{mode="idle", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s3
    expr: |
      gitlab_component_saturation:ratio{component="kube_pool_cpu",env="gstg",type="kube"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="kube_pool_cpu"}
  - alert: component_saturation_slo_out_of_bounds:kube_pool_max_nodes
    for: 5m
    annotations:
      grafana_datasource_id: ee37ad60-dcd5-4699-98ff-949aca509226
      title: The Kube Pool Max Node Limit resource of the {{ $labels.type }} service
        ({{ $labels.stage }} stage) has a saturation exceeding SLO and is close to
        its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Kube Pool Max Node Limit resource:

        A GKE kubernetes node pool is close to it's maximum number of nodes.

        The maximum is defined in terraform, via the `max_node_count` field of a node pool. The limit is per-zone, so for single zone clusters the number of nodes will match the limit, for regional clusters, the limit is multiplied by the number of zones the cluster is deployed over.
      grafana_dashboard_id: alerts-sat_kube_pool_max_nodes
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_kube_pool_max_nodes?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1686893332"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(env, environment, tier, type, stage, shard, cluster, label_pool, shard) (
          clamp_min(
            clamp_max(
              count by (cluster, env, environment, label_pool, tier, type, stage, shard) (
                kube_node_labels:labeled{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              / on(cluster, env, environment, label_pool) group_left() (
                label_replace(
                  terraform_report_google_cluster_node_pool_max_node_count,
                  "label_pool", "$0", "pool_name", ".*"
                )
                * on(cluster, env, environment) group_left()
                count by (cluster, env, environment) (
                  group by (cluster, env, environment, label_topology_kubernetes_io_zone) (
                    kube_node_labels:labeled{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                  )
                )
              )
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(env, environment, tier, type, stage, shard, cluster, label_pool, shard) (
          clamp_min(
            clamp_max(
              count by (cluster, env, environment, label_pool, tier, type, stage, shard) (
                kube_node_labels:labeled{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              / on(cluster, env, environment, label_pool) group_left() (
                label_replace(
                  terraform_report_google_cluster_node_pool_max_node_count,
                  "label_pool", "$0", "pool_name", ".*"
                )
                * on(cluster, env, environment) group_left()
                count by (cluster, env, environment) (
                  group by (cluster, env, environment, label_topology_kubernetes_io_zone) (
                    kube_node_labels:labeled{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                  )
                )
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s3
    expr: |
      gitlab_component_saturation:ratio{component="kube_pool_max_nodes",env="gstg",type="kube"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="kube_pool_max_nodes"}
