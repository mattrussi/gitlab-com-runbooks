# WARNING. DO NOT EDIT THIS FILE BY HAND. USE ./mimir-rules-jsonnet/saturation.jsonnet TO GENERATE IT
# YOUR CHANGES WILL BE OVERRIDDEN
groups:
- name: GitLab Component Saturation Statistics
  interval: 5m
  rules:
  - record: gitlab_component_saturation:ratio_quantile95_1w
    expr: quantile_over_time(0.95, gitlab_component_saturation:ratio{env="gstg",type="logging"}[1w])
  - record: gitlab_component_saturation:ratio_quantile99_1w
    expr: quantile_over_time(0.99, gitlab_component_saturation:ratio{env="gstg",type="logging"}[1w])
  - record: gitlab_component_saturation:ratio_quantile95_1h
    expr: quantile_over_time(0.95, gitlab_component_saturation:ratio{env="gstg",type="logging"}[1h])
  - record: gitlab_component_saturation:ratio_quantile99_1h
    expr: quantile_over_time(0.99, gitlab_component_saturation:ratio{env="gstg",type="logging"}[1h])
  - record: gitlab_component_saturation:ratio_avg_1h
    expr: avg_over_time(gitlab_component_saturation:ratio{env="gstg",type="logging"}[1h])
- name: GitLab Saturation Alerts
  interval: 1m
  rules:
  - alert: component_saturation_slo_out_of_bounds:elastic_cpu
    for: 5m
    annotations:
      title: The Average CPU Utilization per Node resource of the {{ $labels.type
        }} service ({{ $labels.stage }} stage) has a saturation exceeding SLO and
        is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Average CPU Utilization per Node resource:

        Average CPU utilization per Node.

        This resource measures all CPU across a fleet. If it is becoming saturated, it may indicate that the fleet needs horizontal or vertical scaling. The metrics are coming from elasticsearch_exporter.
      grafana_dashboard_id: alerts-sat_elastic_cpu
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_elastic_cpu?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "804418020"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard) (
          clamp_min(
            clamp_max(
              avg by (environment, tier, type, stage, shard) (
                avg_over_time(elasticsearch_process_cpu_percent{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[1m]) / 100
              )
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, tier, type, stage, shard) (
          clamp_min(
            clamp_max(
              avg by (environment, tier, type, stage, shard) (
                avg_over_time(elasticsearch_process_cpu_percent{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[1m]) / 100
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
    expr: |
      gitlab_component_saturation:ratio{component="elastic_cpu",env="gstg",type="logging"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="elastic_cpu",env="gstg",type="logging"}
  - alert: component_saturation_slo_out_of_bounds:elastic_disk_space
    for: 5m
    annotations:
      title: The Disk Utilization Overall resource of the {{ $labels.type }} service
        ({{ $labels.stage }} stage) has a saturation exceeding SLO and is close to
        its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Disk Utilization Overall resource:

        Disk utilization per device per node.
      grafana_dashboard_id: alerts-sat_elastic_disk_space
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_elastic_disk_space?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2419103084"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard, host) (
          clamp_min(
            clamp_max(
              sum by (environment, tier, type, stage, shard, host) (
                (elasticsearch_filesystem_data_size_bytes{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} - elasticsearch_filesystem_data_free_bytes{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"})
              )
              /
              sum by (environment, tier, type, stage, shard, host) (
                elasticsearch_filesystem_data_size_bytes{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, tier, type, stage, shard, host) (
          clamp_min(
            clamp_max(
              sum by (environment, tier, type, stage, shard, host) (
                (elasticsearch_filesystem_data_size_bytes{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} - elasticsearch_filesystem_data_free_bytes{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"})
              )
              /
              sum by (environment, tier, type, stage, shard, host) (
                elasticsearch_filesystem_data_size_bytes{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      pager: pagerduty
      rules_domain: general
      severity: s2
    expr: |
      gitlab_component_saturation:ratio{component="elastic_disk_space",env="gstg",type="logging"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="elastic_disk_space",env="gstg",type="logging"}
  - alert: component_saturation_slo_out_of_bounds:elastic_jvm_heap_memory
    for: 5m
    annotations:
      title: The JVM Heap Utilization per Node resource of the {{ $labels.type }}
        service ({{ $labels.stage }} stage) has a saturation exceeding SLO and is
        close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the JVM Heap Utilization per Node resource:

        JVM heap memory utilization per node.
      grafana_dashboard_id: alerts-sat_elastic_jvm_heap_memory
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_elastic_jvm_heap_memory?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2329310959"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard, name) (
          clamp_min(
            clamp_max(
              elasticsearch_jvm_memory_used_bytes{area="heap", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              /
              elasticsearch_jvm_memory_max_bytes{area="heap", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, tier, type, stage, shard, name) (
          clamp_min(
            clamp_max(
              elasticsearch_jvm_memory_used_bytes{area="heap", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              /
              elasticsearch_jvm_memory_max_bytes{area="heap", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
    expr: |
      gitlab_component_saturation:ratio{component="elastic_jvm_heap_memory",env="gstg",type="logging"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="elastic_jvm_heap_memory",env="gstg",type="logging"}
  - alert: component_saturation_slo_out_of_bounds:elastic_single_node_cpu
    for: 5m
    annotations:
      title: The Average CPU Utilization per Node resource of the {{ $labels.type
        }} service ({{ $labels.stage }} stage) has a saturation exceeding SLO and
        is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Average CPU Utilization per Node resource:

        Average CPU per Node.

        This resource measures all CPU across a fleet. If it is becoming saturated, it may indicate that the fleet needs horizontal or vertical scaling. The metrics are coming from elasticsearch_exporter.
      grafana_dashboard_id: alerts-sat_elastic_single_node_cpu
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_elastic_single_node_cpu?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2230690360"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard, name) (
          clamp_min(
            clamp_max(
              avg_over_time(elasticsearch_process_cpu_percent{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m]) / 100
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, tier, type, stage, shard, name) (
          clamp_min(
            clamp_max(
              avg_over_time(elasticsearch_process_cpu_percent{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m]) / 100
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
    expr: |
      gitlab_component_saturation:ratio{component="elastic_single_node_cpu",env="gstg",type="logging"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="elastic_single_node_cpu",env="gstg",type="logging"}
  - alert: component_saturation_slo_out_of_bounds:elastic_single_node_disk_space
    for: 5m
    annotations:
      title: The Disk Utilization per Device per Node resource of the {{ $labels.type
        }} service ({{ $labels.stage }} stage) has a saturation exceeding SLO and
        is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Disk Utilization per Device per Node resource:

        Disk utilization per device per node.
      grafana_dashboard_id: alerts-sat_elastic_node_disk_space
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_elastic_node_disk_space?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "658552830"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard, name) (
          clamp_min(
            clamp_max(
              (
                (
                  elasticsearch_filesystem_data_size_bytes{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                  -
                  elasticsearch_filesystem_data_free_bytes{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                )
                /
                elasticsearch_filesystem_data_size_bytes{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, tier, type, stage, shard, name) (
          clamp_min(
            clamp_max(
              (
                (
                  elasticsearch_filesystem_data_size_bytes{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                  -
                  elasticsearch_filesystem_data_free_bytes{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                )
                /
                elasticsearch_filesystem_data_size_bytes{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
    expr: |
      gitlab_component_saturation:ratio{component="elastic_single_node_disk_space",env="gstg",type="logging"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="elastic_single_node_disk_space",env="gstg",type="logging"}
  - alert: component_saturation_slo_out_of_bounds:elastic_thread_pools
    for: 5m
    annotations:
      title: The Thread pool utilization resource of the {{ $labels.type }} service
        ({{ $labels.stage }} stage) has a saturation exceeding SLO and is close to
        its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Thread pool utilization resource:

        Utilization of each thread pool on each node.

        Descriptions of the threadpool types can be found at https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-threadpool.html.
      grafana_dashboard_id: alerts-sat_elastic_thread_pools
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_elastic_thread_pools?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "525964640"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard, name, exported_type) (
          clamp_min(
            clamp_max(
              (
                avg_over_time(elasticsearch_thread_pool_active_count{exported_type!="snapshot", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
                /
                (avg_over_time(elasticsearch_thread_pool_threads_count{exported_type!="snapshot", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m]) > 0)
              )
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, tier, type, stage, shard, name, exported_type) (
          clamp_min(
            clamp_max(
              (
                avg_over_time(elasticsearch_thread_pool_active_count{exported_type!="snapshot", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
                /
                (avg_over_time(elasticsearch_thread_pool_threads_count{exported_type!="snapshot", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m]) > 0)
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
    expr: |
      gitlab_component_saturation:ratio{component="elastic_thread_pools",env="gstg",type="logging"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="elastic_thread_pools",env="gstg",type="logging"}
  - alert: component_saturation_slo_out_of_bounds:kube_container_cpu_limit
    for: 15m
    annotations:
      title: The Kube Container CPU over-utilization resource of the {{ $labels.type
        }} service ({{ $labels.stage }} stage) has a saturation exceeding SLO and
        is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Kube Container CPU over-utilization resource:

        Kubernetes containers can have a limit configured on how much CPU they can consume in a burst. If we are at this limit, exceeding the allocated requested resources, we should consider revisting the container's HPA configuration.

        When a container is utilizing CPU resources up-to it's configured limit for extended periods of time, this could cause it and other running containers to be throttled.
      grafana_dashboard_id: alerts-sat_kube_container_cpu_limit
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_kube_container_cpu_limit?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1262336683"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard, pod, container) (
          clamp_min(
            clamp_max(
              sum by (environment, tier, type, stage, shard, pod, container) (
                rate(container_cpu_usage_seconds_total:labeled{container!="", container!="POD", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
              )
              /
              sum by(environment, tier, type, stage, shard, pod, container) (
                container_spec_cpu_quota:labeled{container!="", container!="POD", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                /
                container_spec_cpu_period:labeled{container!="", container!="POD", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, tier, type, stage, shard, pod, container) (
          clamp_min(
            clamp_max(
              sum by (environment, tier, type, stage, shard, pod, container) (
                rate(container_cpu_usage_seconds_total:labeled{container!="", container!="POD", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
              )
              /
              sum by(environment, tier, type, stage, shard, pod, container) (
                container_spec_cpu_quota:labeled{container!="", container!="POD", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                /
                container_spec_cpu_period:labeled{container!="", container!="POD", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
    expr: |
      gitlab_component_saturation:ratio{component="kube_container_cpu_limit",env="gstg",type="logging"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="kube_container_cpu_limit",env="gstg",type="logging"}
  - alert: component_saturation_slo_out_of_bounds:kube_container_memory
    for: 15m
    annotations:
      title: The Kube Container Memory Utilization resource of the {{ $labels.type
        }} service ({{ $labels.stage }} stage) has a saturation exceeding SLO and
        is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Kube Container Memory Utilization resource:

        This uses the working set size from cAdvisor for the cgroup's memory usage. That may not be a good measure as it includes filesystem cache pages that are not necessarily attributable to the application inside the cgroup, and are permitted to be evicted instead of being OOM killed.
      grafana_dashboard_id: alerts-sat_kube_container_memory
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_kube_container_memory?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "172578411"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard) (
          clamp_min(
            clamp_max(
              container_memory_working_set_bytes:labeled{container!="", container!="POD", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              /
              (container_spec_memory_limit_bytes:labeled{container!="", container!="POD", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} > 0)
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, tier, type, stage, shard) (
          clamp_min(
            clamp_max(
              container_memory_working_set_bytes:labeled{container!="", container!="POD", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              /
              (container_spec_memory_limit_bytes:labeled{container!="", container!="POD", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"} > 0)
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
    expr: |
      gitlab_component_saturation:ratio{component="kube_container_memory",env="gstg",type="logging"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="kube_container_memory",env="gstg",type="logging"}
  - alert: component_saturation_slo_out_of_bounds:kube_container_throttling
    for: 10m
    annotations:
      title: The Kube container throttling resource of the {{ $labels.type }} service
        ({{ $labels.stage }} stage) has a saturation exceeding SLO and is close to
        its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Kube container throttling resource:

        Kube container throttling

        A container will be throttled if it reaches the configured cpu limit for the horizontal pod autoscaler. Or when other containers on the node are overutilizing the the CPU.

        To get around this, consider increasing the limit for this workload, taking into consideration the requested resources.
      grafana_dashboard_id: alerts-kube_container_throttling
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-kube_container_throttling?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "54512634"
      grafana_variables: environment,type,stage
      promql_query: |
        quantile by(environment, tier, type, stage, shard, pod, container) (
          0.99,
          clamp_min(
            clamp_max(
              avg by (environment, tier, type, stage, shard, pod, container)(
                rate(container_cpu_cfs_throttled_periods_total:labeled{container!="", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
                /
                rate(container_cpu_cfs_periods_total:labeled{container!="", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
              )
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        quantile by(environment, tier, type, stage, shard, pod, container) (
          0.99,
          clamp_min(
            clamp_max(
              avg by (environment, tier, type, stage, shard, pod, container)(
                rate(container_cpu_cfs_throttled_periods_total:labeled{container!="", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
                /
                rate(container_cpu_cfs_periods_total:labeled{container!="", environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}[5m])
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      rules_domain: general
      severity: s3
    expr: |
      gitlab_component_saturation:ratio{component="kube_container_throttling",env="gstg",type="logging"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="kube_container_throttling",env="gstg",type="logging"}
  - alert: component_saturation_slo_out_of_bounds:kube_horizontalpodautoscaler_desired_replicas
    for: 25m
    annotations:
      title: The Horizontal Pod Autoscaler Desired Replicas resource of the {{ $labels.type
        }} service ({{ $labels.stage }} stage) has a saturation exceeding SLO and
        is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Horizontal Pod Autoscaler Desired Replicas resource:

        The [Horizontal Pod Autoscaler](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/) automatically scales the number of Pods in a deployment based on metrics.

        The Horizontal Pod Autoscaler has a configured upper maximum. When this limit is reached, the HPA will not increase the number of pods and other resource saturation (eg, CPU, memory) may occur.
      grafana_dashboard_id: alerts-sat_kube_horizontalpodautoscaler
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_kube_horizontalpodautoscaler?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "351198712"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard, horizontalpodautoscaler, shard) (
          clamp_min(
            clamp_max(
              kube_horizontalpodautoscaler_status_desired_replicas:labeled{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}", shard!~"database-throttled|elasticsearch|gitaly-throttled|urgent-authorized-projects|urgent-other", namespace!~"pubsubbeat"}
              /
              kube_horizontalpodautoscaler_spec_max_replicas:labeled{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}", shard!~"database-throttled|elasticsearch|gitaly-throttled|urgent-authorized-projects|urgent-other", namespace!~"pubsubbeat"}
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, tier, type, stage, shard, horizontalpodautoscaler, shard) (
          clamp_min(
            clamp_max(
              kube_horizontalpodautoscaler_status_desired_replicas:labeled{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}", shard!~"database-throttled|elasticsearch|gitaly-throttled|urgent-authorized-projects|urgent-other", namespace!~"pubsubbeat"}
              /
              kube_horizontalpodautoscaler_spec_max_replicas:labeled{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}", shard!~"database-throttled|elasticsearch|gitaly-throttled|urgent-authorized-projects|urgent-other", namespace!~"pubsubbeat"}
              ,
              1)
          ,
          0)
        )
      runbook: docs/kube/kubernetes.md#hpascalecapability
    labels:
      alert_type: cause
      rules_domain: general
      severity: s3
    expr: |
      gitlab_component_saturation:ratio{component="kube_horizontalpodautoscaler_desired_replicas",env="gstg",type="logging"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="kube_horizontalpodautoscaler_desired_replicas",env="gstg",type="logging"}
  - alert: component_saturation_slo_out_of_bounds:open_fds
    for: 5m
    annotations:
      title: The Open file descriptor utilization per instance resource of the {{
        $labels.type }} service ({{ $labels.stage }} stage) has a saturation exceeding
        SLO and is close to its capacity limit.
      description: |
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Open file descriptor utilization per instance resource:

        Open file descriptor utilization per instance.

        Saturation on file descriptor limits may indicate a resource-descriptor leak in the application.

        As a temporary fix, you may want to consider restarting the affected process.
      grafana_dashboard_id: alerts-sat_open_fds
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_open_fds?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-type={{ $labels.type }}&var-stage={{ $labels.stage
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1001792825"
      grafana_variables: environment,type,stage
      promql_query: |
        max by(environment, tier, type, stage, shard, job, instance) (
          clamp_min(
            clamp_max(
              (
                process_open_fds{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                /
                process_max_fds{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              or
              (
                ruby_file_descriptors{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                /
                ruby_process_max_fds{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(environment, tier, type, stage, shard, job, instance) (
          clamp_min(
            clamp_max(
              (
                process_open_fds{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                /
                process_max_fds{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              or
              (
                ruby_file_descriptors{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
                /
                ruby_process_max_fds{environment="{{ $labels.environment }}",stage="{{ $labels.stage }}",type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
    labels:
      alert_type: cause
      pager: pagerduty
      rules_domain: general
      severity: s2
    expr: |
      gitlab_component_saturation:ratio{component="open_fds",env="gstg",type="logging"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="open_fds",env="gstg",type="logging"}
