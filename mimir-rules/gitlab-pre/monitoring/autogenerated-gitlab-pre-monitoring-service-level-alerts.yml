# WARNING. DO NOT EDIT THIS FILE BY HAND. USE ./mimir-rules-jsonnet/service-component-alerts.jsonnet TO GENERATE IT
# YOUR CHANGES WILL BE OVERRIDDEN
groups:
- name: 'Service Component Alerts: monitoring'
  interval: 1m
  rules:
  - alert: MonitoringServiceGrafanaApdexSLOViolation
    for: 2m
    annotations:
      title: The grafana SLI of the monitoring service (`{{ $labels.stage }}` stage)
        has an apdex violating SLO
      description: |
        Grafana builds and displays dashboards querying Thanos, Elasticsearch and other datasources. This SLI monitors the Grafana HTTP interface.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_datasource_id: mimir-gitlab-pre
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1543481783"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (env,environment,tier,stage,le) (
            rate(grafana_http_request_duration_seconds_bucket{environment="{{ $labels.environment }}",job="grafana",shard="default",stage="{{ $labels.stage }}",type="monitoring"}[5m])
          )
        )
      runbook: docs/monitoring/README.md#alerts
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_1h{component="grafana",env="pre",monitor="global",type="monitoring"}
          < (1 - 14.4 * 0.080000)
        )
        and on (env,environment,tier,type,stage,component)
        (
          gitlab_component_apdex:ratio_5m{component="grafana",env="pre",monitor="global",type="monitoring"}
          < (1 - 14.4 * 0.080000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="grafana",env="pre",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceGrafanaApdexSLOViolation
    for: 2m
    annotations:
      title: The grafana SLI of the monitoring service (`{{ $labels.stage }}` stage)
        has an apdex violating SLO
      description: |
        Grafana builds and displays dashboards querying Thanos, Elasticsearch and other datasources. This SLI monitors the Grafana HTTP interface.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_datasource_id: mimir-gitlab-pre
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1543481783"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (env,environment,tier,stage,le) (
            rate(grafana_http_request_duration_seconds_bucket{environment="{{ $labels.environment }}",job="grafana",shard="default",stage="{{ $labels.stage }}",type="monitoring"}[5m])
          )
        )
      runbook: docs/monitoring/README.md#alerts
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_6h{component="grafana",env="pre",monitor="global",type="monitoring"}
          < (1 - 6 * 0.080000)
        )
        and on (env,environment,tier,type,stage,component)
        (
          gitlab_component_apdex:ratio_30m{component="grafana",env="pre",monitor="global",type="monitoring"}
          < (1 - 6 * 0.080000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="grafana",env="pre",monitor="global",type="monitoring"}) >= 0.16667
      )
  - alert: MonitoringServiceGrafanaErrorSLOViolation
    for: 2m
    annotations:
      title: The grafana SLI of the monitoring service (`{{ $labels.stage }}` stage)
        has an error rate violating SLO
      description: |
        Grafana builds and displays dashboards querying Thanos, Elasticsearch and other datasources. This SLI monitors the Grafana HTTP interface.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_datasource_id: mimir-gitlab-pre
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3079737102"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(grafana_http_request_duration_seconds_bucket{code=~"^5.*",environment="{{ $labels.environment }}",job="grafana",le="+Inf",shard="default",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md#alerts
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="grafana",env="pre",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
        and on (env,environment,tier,type,stage,component)
        (
          gitlab_component_errors:ratio_5m{component="grafana",env="pre",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="grafana",env="pre",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceGrafanaErrorSLOViolation
    for: 2m
    annotations:
      title: The grafana SLI of the monitoring service (`{{ $labels.stage }}` stage)
        has an error rate violating SLO
      description: |
        Grafana builds and displays dashboards querying Thanos, Elasticsearch and other datasources. This SLI monitors the Grafana HTTP interface.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_datasource_id: mimir-gitlab-pre
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3079737102"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(grafana_http_request_duration_seconds_bucket{code=~"^5.*",environment="{{ $labels.environment }}",job="grafana",le="+Inf",shard="default",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md#alerts
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="grafana",env="pre",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
        and on (env,environment,tier,type,stage,component)
        (
          gitlab_component_errors:ratio_30m{component="grafana",env="pre",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="grafana",env="pre",monitor="global",type="monitoring"}) >= 0.16667
      )
  - alert: MonitoringServiceGrafanaDatasourcesErrorSLOViolation
    for: 2m
    annotations:
      title: The grafana_datasources SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        Grafana builds and displays dashboards querying Thanos, Elasticsearch and other datasources. This SLI monitors the requests from Grafana to its datasources.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_datasource_id: mimir-gitlab-pre
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "402312104"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(grafana_datasource_request_total{code=~"^5.*",environment="{{ $labels.environment }}",job="grafana",shard="default",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md#alerts
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="grafana_datasources",env="pre",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
        and on (env,environment,tier,type,stage,component)
        (
          gitlab_component_errors:ratio_5m{component="grafana_datasources",env="pre",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="grafana_datasources",env="pre",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceGrafanaDatasourcesErrorSLOViolation
    for: 2m
    annotations:
      title: The grafana_datasources SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        Grafana builds and displays dashboards querying Thanos, Elasticsearch and other datasources. This SLI monitors the requests from Grafana to its datasources.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_datasource_id: mimir-gitlab-pre
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "402312104"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(grafana_datasource_request_total{code=~"^5.*",environment="{{ $labels.environment }}",job="grafana",shard="default",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md#alerts
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="grafana_datasources",env="pre",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
        and on (env,environment,tier,type,stage,component)
        (
          gitlab_component_errors:ratio_30m{component="grafana_datasources",env="pre",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="grafana_datasources",env="pre",monitor="global",type="monitoring"}) >= 0.16667
      )
  - alert: MonitoringServiceGrafanaGoogleLbErrorSLOViolation
    for: 2m
    annotations:
      title: The grafana_google_lb SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_datasource_id: mimir-gitlab-pre
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "665917726"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(stackdriver_https_lb_rule_loadbalancing_googleapis_com_https_request_count{environment="{{ $labels.environment }}",project_id="gitlab-ops",response_code_class="500",stage="{{ $labels.stage }}",url_map_name="k8s2-um-4zodnh0s-grafana-grafana-cfagrqyu"}[5m])
        )
      runbook: docs/monitoring/README.md#alerts
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="grafana_google_lb",env="pre",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
        and on (env,environment,tier,type,stage,component)
        (
          gitlab_component_errors:ratio_5m{component="grafana_google_lb",env="pre",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="grafana_google_lb",env="pre",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceGrafanaGoogleLbErrorSLOViolation
    for: 2m
    annotations:
      title: The grafana_google_lb SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_datasource_id: mimir-gitlab-pre
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "665917726"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(stackdriver_https_lb_rule_loadbalancing_googleapis_com_https_request_count{environment="{{ $labels.environment }}",project_id="gitlab-ops",response_code_class="500",stage="{{ $labels.stage }}",url_map_name="k8s2-um-4zodnh0s-grafana-grafana-cfagrqyu"}[5m])
        )
      runbook: docs/monitoring/README.md#alerts
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="grafana_google_lb",env="pre",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
        and on (env,environment,tier,type,stage,component)
        (
          gitlab_component_errors:ratio_30m{component="grafana_google_lb",env="pre",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="grafana_google_lb",env="pre",monitor="global",type="monitoring"}) >= 0.16667
      )
  - alert: MonitoringServiceGrafanaImageRendererApdexSLOViolation
    for: 2m
    annotations:
      title: The grafana_image_renderer SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an apdex violating SLO
      description: |
        The Grafana Image Renderer exports Grafana dashboards or panels to PNG for external use. This SLI monitors the Grafana Image Renderer HTTP interface.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_datasource_id: mimir-gitlab-pre
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3617039935"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (env,environment,tier,stage,le) (
            rate(grafana_image_renderer_service_http_request_duration_seconds_bucket{environment="{{ $labels.environment }}",job="grafana-image-renderer",shard="default",stage="{{ $labels.stage }}",type="monitoring"}[5m])
          )
        )
      runbook: docs/monitoring/README.md#alerts
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_1h{component="grafana_image_renderer",env="pre",monitor="global",type="monitoring"}
          < (1 - 14.4 * 0.080000)
        )
        and on (env,environment,tier,type,stage,component)
        (
          gitlab_component_apdex:ratio_5m{component="grafana_image_renderer",env="pre",monitor="global",type="monitoring"}
          < (1 - 14.4 * 0.080000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="grafana_image_renderer",env="pre",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceGrafanaImageRendererApdexSLOViolation
    for: 2m
    annotations:
      title: The grafana_image_renderer SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an apdex violating SLO
      description: |
        The Grafana Image Renderer exports Grafana dashboards or panels to PNG for external use. This SLI monitors the Grafana Image Renderer HTTP interface.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_datasource_id: mimir-gitlab-pre
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3617039935"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (env,environment,tier,stage,le) (
            rate(grafana_image_renderer_service_http_request_duration_seconds_bucket{environment="{{ $labels.environment }}",job="grafana-image-renderer",shard="default",stage="{{ $labels.stage }}",type="monitoring"}[5m])
          )
        )
      runbook: docs/monitoring/README.md#alerts
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_6h{component="grafana_image_renderer",env="pre",monitor="global",type="monitoring"}
          < (1 - 6 * 0.080000)
        )
        and on (env,environment,tier,type,stage,component)
        (
          gitlab_component_apdex:ratio_30m{component="grafana_image_renderer",env="pre",monitor="global",type="monitoring"}
          < (1 - 6 * 0.080000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="grafana_image_renderer",env="pre",monitor="global",type="monitoring"}) >= 0.16667
      )
  - alert: MonitoringServiceGrafanaImageRendererErrorSLOViolation
    for: 2m
    annotations:
      title: The grafana_image_renderer SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        The Grafana Image Renderer exports Grafana dashboards or panels to PNG for external use. This SLI monitors the Grafana Image Renderer HTTP interface.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_datasource_id: mimir-gitlab-pre
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1209709030"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(grafana_image_renderer_service_http_request_duration_seconds_bucket{environment="{{ $labels.environment }}",job="grafana-image-renderer",le="+Inf",shard="default",stage="{{ $labels.stage }}",status_code=~"^5.*",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md#alerts
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="grafana_image_renderer",env="pre",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
        and on (env,environment,tier,type,stage,component)
        (
          gitlab_component_errors:ratio_5m{component="grafana_image_renderer",env="pre",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="grafana_image_renderer",env="pre",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceGrafanaImageRendererErrorSLOViolation
    for: 2m
    annotations:
      title: The grafana_image_renderer SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        The Grafana Image Renderer exports Grafana dashboards or panels to PNG for external use. This SLI monitors the Grafana Image Renderer HTTP interface.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_datasource_id: mimir-gitlab-pre
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1209709030"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(grafana_image_renderer_service_http_request_duration_seconds_bucket{environment="{{ $labels.environment }}",job="grafana-image-renderer",le="+Inf",shard="default",stage="{{ $labels.stage }}",status_code=~"^5.*",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md#alerts
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="grafana_image_renderer",env="pre",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
        and on (env,environment,tier,type,stage,component)
        (
          gitlab_component_errors:ratio_30m{component="grafana_image_renderer",env="pre",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="grafana_image_renderer",env="pre",monitor="global",type="monitoring"}) >= 0.16667
      )
  - alert: MonitoringServicePrometheusApdexSLOViolation
    for: 2m
    annotations:
      title: The prometheus SLI of the monitoring service (`{{ $labels.stage }}` stage)
        has an apdex violating SLO
      description: |
        This SLI monitors Prometheus instances via the HTTP interface. 5xx responses are considered errors.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_datasource_id: mimir-gitlab-pre
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "4146537329"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (env,environment,tier,stage,le) (
            rate(prometheus_http_request_duration_seconds_bucket{environment="{{ $labels.environment }}",job!="prometheus-metamon",job=~"prometheus.*",stage="{{ $labels.stage }}",type="monitoring"}[5m])
          )
        )
      runbook: docs/monitoring/README.md#alerts
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_1h{component="prometheus",env="pre",monitor="global",type="monitoring"}
          < (1 - 14.4 * 0.001000)
        )
        and on (env,environment,tier,type,stage,component)
        (
          gitlab_component_apdex:ratio_5m{component="prometheus",env="pre",monitor="global",type="monitoring"}
          < (1 - 14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="prometheus",env="pre",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServicePrometheusApdexSLOViolation
    for: 2m
    annotations:
      title: The prometheus SLI of the monitoring service (`{{ $labels.stage }}` stage)
        has an apdex violating SLO
      description: |
        This SLI monitors Prometheus instances via the HTTP interface. 5xx responses are considered errors.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_datasource_id: mimir-gitlab-pre
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "4146537329"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (env,environment,tier,stage,le) (
            rate(prometheus_http_request_duration_seconds_bucket{environment="{{ $labels.environment }}",job!="prometheus-metamon",job=~"prometheus.*",stage="{{ $labels.stage }}",type="monitoring"}[5m])
          )
        )
      runbook: docs/monitoring/README.md#alerts
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_6h{component="prometheus",env="pre",monitor="global",type="monitoring"}
          < (1 - 6 * 0.001000)
        )
        and on (env,environment,tier,type,stage,component)
        (
          gitlab_component_apdex:ratio_30m{component="prometheus",env="pre",monitor="global",type="monitoring"}
          < (1 - 6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="prometheus",env="pre",monitor="global",type="monitoring"}) >= 0.16667
      )
  - alert: MonitoringServicePrometheusErrorSLOViolation
    for: 2m
    annotations:
      title: The prometheus SLI of the monitoring service (`{{ $labels.stage }}` stage)
        has an error rate violating SLO
      description: |
        This SLI monitors Prometheus instances via the HTTP interface. 5xx responses are considered errors.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_datasource_id: mimir-gitlab-pre
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "190969910"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(prometheus_http_requests_total{code=~"^5.*",environment="{{ $labels.environment }}",job!="prometheus-metamon",job=~"prometheus.*",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md#alerts
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="prometheus",env="pre",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
        and on (env,environment,tier,type,stage,component)
        (
          gitlab_component_errors:ratio_5m{component="prometheus",env="pre",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="prometheus",env="pre",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServicePrometheusErrorSLOViolation
    for: 2m
    annotations:
      title: The prometheus SLI of the monitoring service (`{{ $labels.stage }}` stage)
        has an error rate violating SLO
      description: |
        This SLI monitors Prometheus instances via the HTTP interface. 5xx responses are considered errors.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_datasource_id: mimir-gitlab-pre
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "190969910"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(prometheus_http_requests_total{code=~"^5.*",environment="{{ $labels.environment }}",job!="prometheus-metamon",job=~"prometheus.*",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md#alerts
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="prometheus",env="pre",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
        and on (env,environment,tier,type,stage,component)
        (
          gitlab_component_errors:ratio_30m{component="prometheus",env="pre",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="prometheus",env="pre",monitor="global",type="monitoring"}) >= 0.16667
      )
  - alert: MonitoringServicePrometheusTrafficCessation
    for: 5m
    annotations:
      title: The prometheus SLI of the monitoring service (`{{ $labels.stage }}` stage)
        has not received any traffic in the past 30m
      description: |
        This SLI monitors Prometheus instances via the HTTP interface. 5xx responses are considered errors.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_datasource_id: mimir-gitlab-pre
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "443859513"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(prometheus_http_requests_total{environment="{{ $labels.environment }}",job!="prometheus-metamon",job=~"prometheus.*",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md#alerts
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
    expr: |
      gitlab_component_ops:rate_30m{component="prometheus",env="pre",monitor="global",stage="main",type="monitoring"} == 0
      and
      gitlab_component_ops:rate_30m{component="prometheus",env="pre",monitor="global",stage="main",type="monitoring"} offset 1h >= 0.16666666666666666
  - alert: MonitoringServicePrometheusTrafficAbsent
    for: 30m
    annotations:
      title: The prometheus SLI of the monitoring service (`{{ $labels.stage }}` stage)
        has not reported any traffic in the past 30m
      description: |
        This SLI monitors Prometheus instances via the HTTP interface. 5xx responses are considered errors.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_datasource_id: mimir-gitlab-pre
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "443859513"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(prometheus_http_requests_total{environment="{{ $labels.environment }}",job!="prometheus-metamon",job=~"prometheus.*",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md#alerts
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
    expr: |
      gitlab_component_ops:rate_5m{component="prometheus",env="pre",monitor="global",stage="main",type="monitoring"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="prometheus",env="pre",monitor="global",stage="main",type="monitoring"}
  - alert: MonitoringServicePrometheusAlertSenderErrorSLOViolation
    for: 2m
    annotations:
      title: The prometheus_alert_sender SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        This SLI monitors all prometheus alert notifications that are generated by AlertManager. Alert delivery failure is considered a service-level failure.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_datasource_id: mimir-gitlab-pre
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3098809023"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(prometheus_notifications_errors_total{environment="{{ $labels.environment }}",job="prometheus",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md#alerts
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="prometheus_alert_sender",env="pre",monitor="global",type="monitoring"}
          > (14.4 * 0.005000)
        )
        and on (env,environment,tier,type,stage,component)
        (
          gitlab_component_errors:ratio_5m{component="prometheus_alert_sender",env="pre",monitor="global",type="monitoring"}
          > (14.4 * 0.005000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="prometheus_alert_sender",env="pre",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServicePrometheusAlertSenderErrorSLOViolation
    for: 2m
    annotations:
      title: The prometheus_alert_sender SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        This SLI monitors all prometheus alert notifications that are generated by AlertManager. Alert delivery failure is considered a service-level failure.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_datasource_id: mimir-gitlab-pre
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3098809023"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(prometheus_notifications_errors_total{environment="{{ $labels.environment }}",job="prometheus",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md#alerts
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="prometheus_alert_sender",env="pre",monitor="global",type="monitoring"}
          > (6 * 0.005000)
        )
        and on (env,environment,tier,type,stage,component)
        (
          gitlab_component_errors:ratio_30m{component="prometheus_alert_sender",env="pre",monitor="global",type="monitoring"}
          > (6 * 0.005000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="prometheus_alert_sender",env="pre",monitor="global",type="monitoring"}) >= 0.16667
      )
  - alert: MonitoringServicePrometheusAlertSenderTrafficCessation
    for: 5m
    annotations:
      title: The prometheus_alert_sender SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has not received any traffic in the past 30m
      description: |
        This SLI monitors all prometheus alert notifications that are generated by AlertManager. Alert delivery failure is considered a service-level failure.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_datasource_id: mimir-gitlab-pre
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2970883772"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(prometheus_notifications_sent_total{environment="{{ $labels.environment }}",job="prometheus",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md#alerts
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
    expr: |
      gitlab_component_ops:rate_30m{component="prometheus_alert_sender",env="pre",monitor="global",stage="main",type="monitoring"} == 0
      and
      gitlab_component_ops:rate_30m{component="prometheus_alert_sender",env="pre",monitor="global",stage="main",type="monitoring"} offset 1h >= 0.16666666666666666
  - alert: MonitoringServicePrometheusAlertSenderTrafficAbsent
    for: 30m
    annotations:
      title: The prometheus_alert_sender SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has not reported any traffic in the past 30m
      description: |
        This SLI monitors all prometheus alert notifications that are generated by AlertManager. Alert delivery failure is considered a service-level failure.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_datasource_id: mimir-gitlab-pre
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2970883772"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(prometheus_notifications_sent_total{environment="{{ $labels.environment }}",job="prometheus",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md#alerts
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
    expr: |
      gitlab_component_ops:rate_5m{component="prometheus_alert_sender",env="pre",monitor="global",stage="main",type="monitoring"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="prometheus_alert_sender",env="pre",monitor="global",stage="main",type="monitoring"}
  - alert: MonitoringServiceRuleEvaluationApdexSLOViolation
    for: 2m
    annotations:
      title: The rule_evaluation SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an apdex violating SLO
      description: |
        This SLI monitors Prometheus recording rule evaluations. Recording rule evalution failures are considered to be service failures.

        Prometheus rule groups are evaluating recording rules in a group in sequence at an interval. If the recording of all rules in a groups exceeds the interval for the group, we could be missing data points in the group.

        If a group fails often, we should split it up or improve query performance.

        To see which rules are often not meeting their target. Look at the SLI-details. The `rule_group` label will contain information about the slow group.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_datasource_id: mimir-gitlab-pre
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2667244350"
      grafana_variables: environment,stage
      runbook: docs/monitoring/README.md#alerts
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_1h{component="rule_evaluation",env="pre",monitor="global",type="monitoring"}
          < (1 - 14.4 * 0.001000)
        )
        and on (env,environment,tier,type,stage,component)
        (
          gitlab_component_apdex:ratio_5m{component="rule_evaluation",env="pre",monitor="global",type="monitoring"}
          < (1 - 14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="rule_evaluation",env="pre",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceRuleEvaluationApdexSLOViolation
    for: 2m
    annotations:
      title: The rule_evaluation SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an apdex violating SLO
      description: |
        This SLI monitors Prometheus recording rule evaluations. Recording rule evalution failures are considered to be service failures.

        Prometheus rule groups are evaluating recording rules in a group in sequence at an interval. If the recording of all rules in a groups exceeds the interval for the group, we could be missing data points in the group.

        If a group fails often, we should split it up or improve query performance.

        To see which rules are often not meeting their target. Look at the SLI-details. The `rule_group` label will contain information about the slow group.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_datasource_id: mimir-gitlab-pre
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2667244350"
      grafana_variables: environment,stage
      runbook: docs/monitoring/README.md#alerts
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_6h{component="rule_evaluation",env="pre",monitor="global",type="monitoring"}
          < (1 - 6 * 0.001000)
        )
        and on (env,environment,tier,type,stage,component)
        (
          gitlab_component_apdex:ratio_30m{component="rule_evaluation",env="pre",monitor="global",type="monitoring"}
          < (1 - 6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="rule_evaluation",env="pre",monitor="global",type="monitoring"}) >= 0.16667
      )
  - alert: MonitoringServiceRuleEvaluationErrorSLOViolation
    for: 2m
    annotations:
      title: The rule_evaluation SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        This SLI monitors Prometheus recording rule evaluations. Recording rule evalution failures are considered to be service failures.

        Prometheus rule groups are evaluating recording rules in a group in sequence at an interval. If the recording of all rules in a groups exceeds the interval for the group, we could be missing data points in the group.

        If a group fails often, we should split it up or improve query performance.

        To see which rules are often not meeting their target. Look at the SLI-details. The `rule_group` label will contain information about the slow group.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_datasource_id: mimir-gitlab-pre
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3727131931"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(prometheus_rule_evaluation_failures_total{environment="{{ $labels.environment }}",job!~"^thanos.*",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md#alerts
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="rule_evaluation",env="pre",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
        and on (env,environment,tier,type,stage,component)
        (
          gitlab_component_errors:ratio_5m{component="rule_evaluation",env="pre",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="rule_evaluation",env="pre",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceRuleEvaluationErrorSLOViolation
    for: 2m
    annotations:
      title: The rule_evaluation SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        This SLI monitors Prometheus recording rule evaluations. Recording rule evalution failures are considered to be service failures.

        Prometheus rule groups are evaluating recording rules in a group in sequence at an interval. If the recording of all rules in a groups exceeds the interval for the group, we could be missing data points in the group.

        If a group fails often, we should split it up or improve query performance.

        To see which rules are often not meeting their target. Look at the SLI-details. The `rule_group` label will contain information about the slow group.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_datasource_id: mimir-gitlab-pre
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3727131931"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(prometheus_rule_evaluation_failures_total{environment="{{ $labels.environment }}",job!~"^thanos.*",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md#alerts
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="rule_evaluation",env="pre",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
        and on (env,environment,tier,type,stage,component)
        (
          gitlab_component_errors:ratio_30m{component="rule_evaluation",env="pre",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="rule_evaluation",env="pre",monitor="global",type="monitoring"}) >= 0.16667
      )
  - alert: MonitoringServiceRuleEvaluationTrafficCessation
    for: 5m
    annotations:
      title: The rule_evaluation SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has not received any traffic in the past 30m
      description: |
        This SLI monitors Prometheus recording rule evaluations. Recording rule evalution failures are considered to be service failures.

        Prometheus rule groups are evaluating recording rules in a group in sequence at an interval. If the recording of all rules in a groups exceeds the interval for the group, we could be missing data points in the group.

        If a group fails often, we should split it up or improve query performance.

        To see which rules are often not meeting their target. Look at the SLI-details. The `rule_group` label will contain information about the slow group.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_datasource_id: mimir-gitlab-pre
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2621394178"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(prometheus_rule_evaluations_total{environment="{{ $labels.environment }}",job!~"^thanos.*",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md#alerts
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: ops
      slo_alert: "no"
      user_impacting: "no"
    expr: |
      gitlab_component_ops:rate_30m{component="rule_evaluation",env="pre",monitor="global",stage="main",type="monitoring"} == 0
      and
      gitlab_component_ops:rate_30m{component="rule_evaluation",env="pre",monitor="global",stage="main",type="monitoring"} offset 1h >= 0.16666666666666666
  - alert: MonitoringServiceRuleEvaluationTrafficAbsent
    for: 30m
    annotations:
      title: The rule_evaluation SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has not reported any traffic in the past 30m
      description: |
        This SLI monitors Prometheus recording rule evaluations. Recording rule evalution failures are considered to be service failures.

        Prometheus rule groups are evaluating recording rules in a group in sequence at an interval. If the recording of all rules in a groups exceeds the interval for the group, we could be missing data points in the group.

        If a group fails often, we should split it up or improve query performance.

        To see which rules are often not meeting their target. Look at the SLI-details. The `rule_group` label will contain information about the slow group.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_datasource_id: mimir-gitlab-pre
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2621394178"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(prometheus_rule_evaluations_total{environment="{{ $labels.environment }}",job!~"^thanos.*",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md#alerts
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: ops
      slo_alert: "no"
      user_impacting: "no"
    expr: |
      gitlab_component_ops:rate_5m{component="rule_evaluation",env="pre",monitor="global",stage="main",type="monitoring"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="rule_evaluation",env="pre",monitor="global",stage="main",type="monitoring"}
