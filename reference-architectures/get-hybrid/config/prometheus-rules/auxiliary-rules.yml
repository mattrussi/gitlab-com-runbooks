# WARNING. DO NOT EDIT THIS FILE BY HAND. RUN ./scripts/generate-all-reference-architecture-configs.sh TO GENERATE IT. YOUR CHANGES WILL BE OVERRIDDEN
groups:
- interval: 5m
  name: GitLab Component Saturation Statistics
  rules:
  - expr: quantile_over_time(0.95, gitlab_component_saturation:ratio{}[1w])
    record: gitlab_component_saturation:ratio_quantile95_1w
  - expr: quantile_over_time(0.99, gitlab_component_saturation:ratio{}[1w])
    record: gitlab_component_saturation:ratio_quantile99_1w
  - expr: quantile_over_time(0.95, gitlab_component_saturation:ratio{}[1h])
    record: gitlab_component_saturation:ratio_quantile95_1h
  - expr: quantile_over_time(0.99, gitlab_component_saturation:ratio{}[1h])
    record: gitlab_component_saturation:ratio_quantile99_1h
  - expr: avg_over_time(gitlab_component_saturation:ratio{}[1h])
    record: gitlab_component_saturation:ratio_avg_1h
- interval: 1m
  name: GitLab Saturation Alerts
  rules:
  - alert: component_saturation_slo_out_of_bounds:cpu
    annotations:
      description: |+
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Average Service CPU Utilization resource:

        This resource measures average CPU utilization across an all cores in a service fleet. If it is becoming saturated, it may indicate that the fleet needs horizontal or vertical scaling.

      grafana_dashboard_id: alerts-sat_cpu
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_cpu?from=now-6h/m&to=now-1m/m&var-type={{
        $labels.type }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1465724101"
      grafana_variables: type
      promql_query: |
        max by(shard,type) (
          clamp_min(
            clamp_max(
              1 - avg by (shard,type) (
                rate(node_cpu_seconds_total{mode="idle", type="{{ $labels.type }}"}[5m])
              )
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(shard,type) (
          clamp_min(
            clamp_max(
              1 - avg by (shard,type) (
                rate(node_cpu_seconds_total{mode="idle", type="{{ $labels.type }}"}[5m])
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
      title: The Average Service CPU Utilization resource of the {{ $labels.type }}
        service has a saturation exceeding SLO and is close to its capacity limit.
    expr: |
      gitlab_component_saturation:ratio{component="cpu"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="cpu"}
    for: 5m
    labels:
      alert_type: cause
      rules_domain: general
      severity: s3
  - alert: component_saturation_slo_out_of_bounds:disk_inodes
    annotations:
      description: |+
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Disk inode Utilization per Device per Node resource:

        Disk inode utilization per device per node.

        If this is too high, its possible that a directory is filling up with files. Consider logging in an checking temp directories for large numbers of files

      grafana_dashboard_id: alerts-sat_disk_inodes
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_disk_inodes?from=now-6h/m&to=now-1m/m&var-type={{
        $labels.type }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "39965907"
      grafana_variables: type
      promql_query: |
        max by(device,fqdn,type) (
          clamp_min(
            clamp_max(
              1 - (
                node_filesystem_files_free{fstype=~"(ext.|xfs)", type="{{ $labels.type }}"}
                /
                node_filesystem_files{fstype=~"(ext.|xfs)", type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(device,fqdn,type) (
          clamp_min(
            clamp_max(
              1 - (
                node_filesystem_files_free{fstype=~"(ext.|xfs)", type="{{ $labels.type }}"}
                /
                node_filesystem_files{fstype=~"(ext.|xfs)", type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
      title: The Disk inode Utilization per Device per Node resource of the {{ $labels.type
        }} service has a saturation exceeding SLO and is close to its capacity limit.
    expr: |
      gitlab_component_saturation:ratio{component="disk_inodes"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="disk_inodes"}
    for: 15m
    labels:
      alert_type: cause
      pager: pagerduty
      rules_domain: general
      severity: s2
  - alert: ComponentResourceRunningOut_disk_inodes
    annotations:
      description: |+
        This means that this resource is growing rapidly and is predicted to exceed saturation threshold within 6h.

        Details of the Disk inode Utilization per Device per Node resource:

        Disk inode utilization per device per node.

        If this is too high, its possible that a directory is filling up with files. Consider logging in an checking temp directories for large numbers of files

      grafana_dashboard_id: alerts-sat_disk_inodes
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_disk_inodes?from=now-6h/m&to=now-1m/m&var-type={{
        $labels.type }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "39965907"
      grafana_variables: type
      promql_query: |
        max by(device,fqdn,type) (
          clamp_min(
            clamp_max(
              1 - (
                node_filesystem_files_free{fstype=~"(ext.|xfs)", type="{{ $labels.type }}"}
                /
                node_filesystem_files{fstype=~"(ext.|xfs)", type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(device,fqdn,type) (
          clamp_min(
            clamp_max(
              1 - (
                node_filesystem_files_free{fstype=~"(ext.|xfs)", type="{{ $labels.type }}"}
                /
                node_filesystem_files{fstype=~"(ext.|xfs)", type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
      title: The Disk inode Utilization per Device per Node resource of the {{ $labels.type
        }} service is on track to hit capacity within 6h
    expr: |
      predict_linear(gitlab_component_saturation:ratio{component="disk_inodes"}[6h], 21600)
      > on (component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="disk_inodes"}
    for: 15m
    labels:
      alert_type: cause
      linear_prediction_saturation_alert: 6h
      pager: pagerduty
      rules_domain: general
      severity: s2
  - alert: component_saturation_slo_out_of_bounds:disk_space
    annotations:
      description: |+
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Disk Space Utilization per Device per Node resource:

        Disk space utilization per device per node.

      grafana_dashboard_id: alerts-sat_disk_space
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_disk_space?from=now-6h/m&to=now-1m/m&var-type={{
        $labels.type }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2661375984"
      grafana_variables: type
      promql_query: |
        max by(device,node,type) (
          clamp_min(
            clamp_max(
              (
                1 - node_filesystem_avail_bytes{fstype=~"ext.|xfs", type="{{ $labels.type }}"} / node_filesystem_size_bytes{fstype=~"ext.|xfs", type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(device,node,type) (
          clamp_min(
            clamp_max(
              (
                1 - node_filesystem_avail_bytes{fstype=~"ext.|xfs", type="{{ $labels.type }}"} / node_filesystem_size_bytes{fstype=~"ext.|xfs", type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
      title: The Disk Space Utilization per Device per Node resource of the {{ $labels.type
        }} service has a saturation exceeding SLO and is close to its capacity limit.
    expr: |
      gitlab_component_saturation:ratio{component="disk_space"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="disk_space"}
    for: 15m
    labels:
      alert_type: cause
      pager: pagerduty
      rules_domain: general
      severity: s2
  - alert: ComponentResourceRunningOut_disk_space
    annotations:
      description: |+
        This means that this resource is growing rapidly and is predicted to exceed saturation threshold within 6h.

        Details of the Disk Space Utilization per Device per Node resource:

        Disk space utilization per device per node.

      grafana_dashboard_id: alerts-sat_disk_space
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_disk_space?from=now-6h/m&to=now-1m/m&var-type={{
        $labels.type }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2661375984"
      grafana_variables: type
      promql_query: |
        max by(device,node,type) (
          clamp_min(
            clamp_max(
              (
                1 - node_filesystem_avail_bytes{fstype=~"ext.|xfs", type="{{ $labels.type }}"} / node_filesystem_size_bytes{fstype=~"ext.|xfs", type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(device,node,type) (
          clamp_min(
            clamp_max(
              (
                1 - node_filesystem_avail_bytes{fstype=~"ext.|xfs", type="{{ $labels.type }}"} / node_filesystem_size_bytes{fstype=~"ext.|xfs", type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
      title: The Disk Space Utilization per Device per Node resource of the {{ $labels.type
        }} service is on track to hit capacity within 6h
    expr: |
      predict_linear(gitlab_component_saturation:ratio{component="disk_space"}[6h], 21600)
      > on (component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="disk_space"}
    for: 15m
    labels:
      alert_type: cause
      linear_prediction_saturation_alert: 6h
      pager: pagerduty
      rules_domain: general
      severity: s2
  - alert: component_saturation_slo_out_of_bounds:go_goroutines
    annotations:
      description: |+
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Go goroutines Utilization per Node resource:

        Go goroutines utilization per node.

        Goroutines leaks can cause memory saturation which can cause service degradation.

        A limit of 250k goroutines is very generous, so if a service exceeds this limit, it's a sign of a leak and it should be dealt with.

      grafana_dashboard_id: alerts-sat_go_goroutines
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_go_goroutines?from=now-6h/m&to=now-1m/m&var-type={{
        $labels.type }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3712788301"
      grafana_variables: type
      promql_query: |
        max by(fqdn,instance,region,type) (
          clamp_min(
            clamp_max(
              sum by (fqdn,instance,region,type) (
                go_goroutines{type="{{ $labels.type }}"}
              )
              /
              250000
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(fqdn,instance,region,type) (
          clamp_min(
            clamp_max(
              sum by (fqdn,instance,region,type) (
                go_goroutines{type="{{ $labels.type }}"}
              )
              /
              250000
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
      title: The Go goroutines Utilization per Node resource of the {{ $labels.type
        }} service has a saturation exceeding SLO and is close to its capacity limit.
    expr: |
      gitlab_component_saturation:ratio{component="go_goroutines"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="go_goroutines"}
    for: 5m
    labels:
      alert_type: cause
      pager: pagerduty
      rules_domain: general
      severity: s2
  - alert: component_saturation_slo_out_of_bounds:go_memory
    annotations:
      description: |+
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Go Memory Utilization per Node resource:

        Go's memory allocation strategy can make it look like a Go process is saturating memory when measured using RSS, when in fact the process is not at risk of memory saturation. For this reason, we measure Go processes using the `go_memstat_alloc_bytes`

      grafana_dashboard_id: alerts-sat_go_memory
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_go_memory?from=now-6h/m&to=now-1m/m&var-type={{
        $labels.type }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3631721613"
      grafana_variables: type
      promql_query: |
        max by(fqdn,type) (
          clamp_min(
            clamp_max(
              max by (fqdn,type) (
                go_memstats_alloc_bytes{type="{{ $labels.type }}"}
              )
              /
              sum by (fqdn,type) (
                node_memory_MemTotal_bytes{type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(fqdn,type) (
          clamp_min(
            clamp_max(
              max by (fqdn,type) (
                go_memstats_alloc_bytes{type="{{ $labels.type }}"}
              )
              /
              sum by (fqdn,type) (
                node_memory_MemTotal_bytes{type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
      title: The Go Memory Utilization per Node resource of the {{ $labels.type }}
        service has a saturation exceeding SLO and is close to its capacity limit.
    expr: |
      gitlab_component_saturation:ratio{component="go_memory"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="go_memory"}
    for: 5m
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
  - alert: component_saturation_slo_out_of_bounds:kube_container_cpu_limit
    annotations:
      description: |+
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Kube Container CPU over-utilization resource:

        Kubernetes containers can have a limit configured on how much CPU they can consume in a burst. If we are at this limit, exceeding the allocated requested resources, we should consider revisting the container's HPA configuration.

        When a container is utilizing CPU resources up-to it's configured limit for extended periods of time, this could cause it and other running containers to be throttled.

      grafana_dashboard_id: alerts-sat_kube_container_cpu_limit
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_kube_container_cpu_limit?from=now-6h/m&to=now-1m/m&var-type={{
        $labels.type }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1262336683"
      grafana_variables: type
      promql_query: |
        max by(container,pod,type) (
          clamp_min(
            clamp_max(
              sum by (container,pod,type) (
                rate(container_cpu_usage_seconds_total:labeled{container!="", container!="POD", type="{{ $labels.type }}"}[5m])
              )
              /
              sum by(container,pod,type) (
                container_spec_cpu_quota:labeled{container!="", container!="POD", type="{{ $labels.type }}"}
                /
                container_spec_cpu_period:labeled{container!="", container!="POD", type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(container,pod,type) (
          clamp_min(
            clamp_max(
              sum by (container,pod,type) (
                rate(container_cpu_usage_seconds_total:labeled{container!="", container!="POD", type="{{ $labels.type }}"}[5m])
              )
              /
              sum by(container,pod,type) (
                container_spec_cpu_quota:labeled{container!="", container!="POD", type="{{ $labels.type }}"}
                /
                container_spec_cpu_period:labeled{container!="", container!="POD", type="{{ $labels.type }}"}
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
      title: The Kube Container CPU over-utilization resource of the {{ $labels.type
        }} service has a saturation exceeding SLO and is close to its capacity limit.
    expr: |
      gitlab_component_saturation:ratio{component="kube_container_cpu_limit"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="kube_container_cpu_limit"}
    for: 15m
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
  - alert: component_saturation_slo_out_of_bounds:kube_container_memory
    annotations:
      description: |+
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Kube Container Memory Utilization resource:

        This uses the working set size from cAdvisor for the cgroup's memory usage. That may not be a good measure as it includes filesystem cache pages that are not necessarily attributable to the application inside the cgroup, and are permitted to be evicted instead of being OOM killed.

      grafana_dashboard_id: alerts-sat_kube_container_memory
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_kube_container_memory?from=now-6h/m&to=now-1m/m&var-type={{
        $labels.type }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "172578411"
      grafana_variables: type
      promql_query: |
        max by(deployment,type) (
          clamp_min(
            clamp_max(
              container_memory_working_set_bytes:labeled{container!="", container!="POD", type="{{ $labels.type }}"}
              /
              (container_spec_memory_limit_bytes:labeled{container!="", container!="POD", type="{{ $labels.type }}"} > 0)
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(deployment,type) (
          clamp_min(
            clamp_max(
              container_memory_working_set_bytes:labeled{container!="", container!="POD", type="{{ $labels.type }}"}
              /
              (container_spec_memory_limit_bytes:labeled{container!="", container!="POD", type="{{ $labels.type }}"} > 0)
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
      title: The Kube Container Memory Utilization resource of the {{ $labels.type
        }} service has a saturation exceeding SLO and is close to its capacity limit.
    expr: |
      gitlab_component_saturation:ratio{component="kube_container_memory"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="kube_container_memory"}
    for: 15m
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
  - alert: component_saturation_slo_out_of_bounds:kube_container_rss
    annotations:
      description: |+
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Kube Container Memory Utilization (RSS) resource:

        Records the total anonymous (unevictable) memory utilization for containers for this service, as a percentage of the memory limit as configured through Kubernetes.

        This is computed using the container's resident set size (RSS), as opposed to kube_container_memory which uses the working set size. For our purposes, RSS is the better metric as cAdvisor's working set calculation includes pages from the filesystem cache that can (and will) be evicted before the OOM killer kills the cgroup.

        A container's RSS (anonymous memory usage) is still not precisely what the OOM killer will use, but it's a better approximation of what the container's workload is actually using. RSS metrics can, however, be dramatically inflated if a process in the container uses MADV_FREE (lazy-free) memory. RSS will include the memory that is available to be reclaimed without a page fault, but not currently in use.

        The most common case of OOM kills is for anonymous memory demand to overwhelm the container's memory limit. On swapless hosts, anonymous memory cannot be evicted from the page cache, so when a container's memory usage is mostly anonymous pages, the only remaining option to relieve memory pressure may be the OOM killer.

        As container RSS approaches container memory limit, OOM kills become much more likely. Consequently, this ratio is a good leading indicator of memory saturation and OOM risk.

      grafana_dashboard_id: alerts-sat_kube_container_rss
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_kube_container_rss?from=now-6h/m&to=now-1m/m&var-type={{
        $labels.type }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2875690100"
      grafana_variables: type
      promql_query: |
        max by(type) (
          clamp_min(
            clamp_max(
              container_memory_rss:labeled{container!="", container!="POD", type="{{ $labels.type }}"}
              /
              (container_spec_memory_limit_bytes:labeled{container!="", container!="POD", type="{{ $labels.type }}"} > 0)
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(type) (
          clamp_min(
            clamp_max(
              container_memory_rss:labeled{container!="", container!="POD", type="{{ $labels.type }}"}
              /
              (container_spec_memory_limit_bytes:labeled{container!="", container!="POD", type="{{ $labels.type }}"} > 0)
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
      title: The Kube Container Memory Utilization (RSS) resource of the {{ $labels.type
        }} service has a saturation exceeding SLO and is close to its capacity limit.
    expr: |
      gitlab_component_saturation:ratio{component="kube_container_rss"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="kube_container_rss"}
    for: 15m
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
  - alert: component_saturation_slo_out_of_bounds:kube_persistent_volume_claim_disk_space
    annotations:
      description: |+
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Kube Persistent Volume Claim Space Utilisation resource:

        disk space utilization on persistent volume claims.

      grafana_dashboard_id: alerts-sat_kube_pvc_disk_space
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_kube_pvc_disk_space?from=now-6h/m&to=now-1m/m&var-type={{
        $labels.type }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "4167694322"
      grafana_variables: type
      promql_query: |
        max by(cluster,namespace,persistentvolumeclaim) (
          clamp_min(
            clamp_max(
              kubelet_volume_stats_used_bytes
              /
              kubelet_volume_stats_capacity_bytes
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(cluster,namespace,persistentvolumeclaim) (
          clamp_min(
            clamp_max(
              kubelet_volume_stats_used_bytes
              /
              kubelet_volume_stats_capacity_bytes
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
      title: The Kube Persistent Volume Claim Space Utilisation resource of the {{
        $labels.type }} service has a saturation exceeding SLO and is close to its
        capacity limit.
    expr: |
      gitlab_component_saturation:ratio{component="kube_persistent_volume_claim_disk_space"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="kube_persistent_volume_claim_disk_space"}
    for: 5m
    labels:
      alert_type: cause
      pager: pagerduty
      rules_domain: general
      severity: s2
  - alert: component_saturation_slo_out_of_bounds:kube_persistent_volume_claim_inodes
    annotations:
      description: |+
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Kube Persistent Volume Claim inode Utilisation resource:

        inode utilization on persistent volume claims.

      grafana_dashboard_id: alerts-sat_kube_pvc_inodes
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_kube_pvc_inodes?from=now-6h/m&to=now-1m/m&var-type={{
        $labels.type }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3153876074"
      grafana_variables: type
      promql_query: |
        max by(persistentvolumeclaim) (
          clamp_min(
            clamp_max(
              kubelet_volume_stats_inodes_used
              /
              kubelet_volume_stats_inodes
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(persistentvolumeclaim) (
          clamp_min(
            clamp_max(
              kubelet_volume_stats_inodes_used
              /
              kubelet_volume_stats_inodes
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
      title: The Kube Persistent Volume Claim inode Utilisation resource of the {{
        $labels.type }} service has a saturation exceeding SLO and is close to its
        capacity limit.
    expr: |
      gitlab_component_saturation:ratio{component="kube_persistent_volume_claim_inodes"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="kube_persistent_volume_claim_inodes"}
    for: 5m
    labels:
      alert_type: cause
      pager: pagerduty
      rules_domain: general
      severity: s2
  - alert: component_saturation_slo_out_of_bounds:memory
    annotations:
      description: |+
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Memory Utilization per Node resource:

        Memory utilization per device per node.

      grafana_dashboard_id: alerts-sat_memory
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_memory?from=now-6h/m&to=now-1m/m&var-type={{
        $labels.type }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1955556769"
      grafana_variables: type
      promql_query: |
        max by(node,type) (
          clamp_min(
            clamp_max(
              instance:node_memory_utilization:ratio{type="{{ $labels.type }}"} or instance:node_memory_utilisation:ratio{type="{{ $labels.type }}"}
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(node,type) (
          clamp_min(
            clamp_max(
              instance:node_memory_utilization:ratio{type="{{ $labels.type }}"} or instance:node_memory_utilisation:ratio{type="{{ $labels.type }}"}
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
      title: The Memory Utilization per Node resource of the {{ $labels.type }} service
        has a saturation exceeding SLO and is close to its capacity limit.
    expr: |
      gitlab_component_saturation:ratio{component="memory"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="memory"}
    for: 5m
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
  - alert: component_saturation_slo_out_of_bounds:node_group_cpu
    annotations:
      description: |+
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Average Node Pool CPU Utilization resource:

        This resource measures average CPU utilization across an all cores in a node group

        If it is becoming saturated, it may indicate that the node group needs resizing.

      grafana_dashboard_id: alerts-sat_node_group_cpu
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_node_group_cpu?from=now-6h/m&to=now-1m/m&var-type={{
        $labels.type }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2284482674"
      grafana_variables: type
      promql_query: |
        max by(type) (
          clamp_min(
            clamp_max(
              1 - avg by (type) (
                rate(node_cpu_seconds_total{mode="idle", type=~".*pool"}[5m])
              )
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(type) (
          clamp_min(
            clamp_max(
              1 - avg by (type) (
                rate(node_cpu_seconds_total{mode="idle", type=~".*pool"}[5m])
              )
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
      title: The Average Node Pool CPU Utilization resource of the {{ $labels.type
        }} service has a saturation exceeding SLO and is close to its capacity limit.
    expr: |
      gitlab_component_saturation:ratio{component="node_group_cpu"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="node_group_cpu"}
    for: 5m
    labels:
      alert_type: cause
      rules_domain: general
      severity: s3
  - alert: component_saturation_slo_out_of_bounds:node_schedstat_waiting
    annotations:
      description: |+
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Node Scheduler Waiting Time resource:

        Measures the amount of scheduler waiting time that processes are waiting to be scheduled, according to [`CPU Scheduling Metrics`](https://www.robustperception.io/cpu-scheduling-metrics-from-the-node-exporter).

        A high value indicates that a node has more processes to be run than CPU time available to handle them, and may lead to degraded responsiveness and performance from the application.

        Additionally, it may indicate that the fleet is under-provisioned.

      grafana_dashboard_id: alerts-sat_node_schedstat_waiting
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_node_schedstat_waiting?from=now-6h/m&to=now-1m/m&var-type={{
        $labels.type }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1415313189"
      grafana_variables: type
      promql_query: |
        max by(fqdn,shard,type) (
          clamp_min(
            clamp_max(
              avg without (cpu) (rate(node_schedstat_waiting_seconds_total{type="{{ $labels.type }}"}[1h]))
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(fqdn,shard,type) (
          clamp_min(
            clamp_max(
              avg without (cpu) (rate(node_schedstat_waiting_seconds_total{type="{{ $labels.type }}"}[1h]))
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
      title: The Node Scheduler Waiting Time resource of the {{ $labels.type }} service
        has a saturation exceeding SLO and is close to its capacity limit.
    expr: |
      gitlab_component_saturation:ratio{component="node_schedstat_waiting"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="node_schedstat_waiting"}
    for: 90m
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
  - alert: component_saturation_slo_out_of_bounds:puma_workers
    annotations:
      description: |+
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Puma Worker Saturation resource:

        Puma thread utilization.

        Puma uses a fixed size thread pool to handle HTTP requests. This metric shows how many threads are busy handling requests. When this resource is saturated, we will see puma queuing taking place. Leading to slowdowns across the application.

        Puma saturation is usually caused by latency problems in downstream services: usually Gitaly or Postgres, but possibly also Redis. Puma saturation can also be caused by traffic spikes.

      grafana_dashboard_id: alerts-sat_puma_workers
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_puma_workers?from=now-6h/m&to=now-1m/m&var-type={{
        $labels.type }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3062464720"
      grafana_variables: type
      promql_query: |
        max by(type) (
          clamp_min(
            clamp_max(
              sum by(type) (avg_over_time(sum without (pid,worker) (puma_active_connections{type="{{ $labels.type }}"})[1m:]))
              /
              sum by(type) (sum without (pid,worker) (puma_max_threads{pid="puma_master", type="{{ $labels.type }}"}))
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(type) (
          clamp_min(
            clamp_max(
              sum by(type) (avg_over_time(sum without (pid,worker) (puma_active_connections{type="{{ $labels.type }}"})[1m:]))
              /
              sum by(type) (sum without (pid,worker) (puma_max_threads{pid="puma_master", type="{{ $labels.type }}"}))
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
      title: The Puma Worker Saturation resource of the {{ $labels.type }} service
        has a saturation exceeding SLO and is close to its capacity limit.
    expr: |
      gitlab_component_saturation:ratio{component="puma_workers"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="puma_workers"}
    for: 5m
    labels:
      alert_type: cause
      pager: pagerduty
      rules_domain: general
      severity: s2
  - alert: component_saturation_slo_out_of_bounds:single_node_cpu
    annotations:
      description: |+
        This means that this resource is running close to capacity and is at risk of exceeding its current capacity limit.

        Details of the Average CPU Utilization per Node resource:

        Average CPU utilization per Node.

        If average CPU is saturated, it may indicate that a fleet is in need to horizontal or vertical scaling. It may also indicate imbalances in load in a fleet.

      grafana_dashboard_id: alerts-sat_single_node_cpu
      grafana_dashboard_link: https://dashboards.gitlab.net/d/alerts-sat_single_node_cpu?from=now-6h/m&to=now-1m/m&var-type={{
        $labels.type }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3372411356"
      grafana_variables: type
      promql_query: |
        max by(node,type) (
          clamp_min(
            clamp_max(
              avg without(cpu, mode) (1 - rate(node_cpu_seconds_total{mode="idle", type="{{ $labels.type }}"}[5m]))
              ,
              1)
          ,
          0)
        )
      promql_template_1: |
        max by(node,type) (
          clamp_min(
            clamp_max(
              avg without(cpu, mode) (1 - rate(node_cpu_seconds_total{mode="idle", type="{{ $labels.type }}"}[5m]))
              ,
              1)
          ,
          0)
        )
      runbook: docs/{{ $labels.type }}/README.md
      title: The Average CPU Utilization per Node resource of the {{ $labels.type
        }} service has a saturation exceeding SLO and is close to its capacity limit.
    expr: |
      gitlab_component_saturation:ratio{component="single_node_cpu"} > on(component) group_left
      slo:max:hard:gitlab_component_saturation:ratio{component="single_node_cpu"}
    for: 10m
    labels:
      alert_type: cause
      rules_domain: general
      severity: s4
