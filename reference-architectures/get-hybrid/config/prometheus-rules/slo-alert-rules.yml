# WARNING. DO NOT EDIT THIS FILE BY HAND. RUN ./scripts/generate-all-reference-architecture-configs.sh TO GENERATE IT. YOUR CHANGES WILL BE OVERRIDDEN
groups:
- interval: 1m
  name: 'Service Component Alerts: gitaly'
  rules:
  - alert: GitalyServiceGoserverApdexSLOViolation
    annotations:
      description: |
        This SLI monitors all Gitaly GRPC requests in aggregate, excluding the OperationService. GRPC failures which are considered to be the "server's fault" are counted as errors. The apdex score is based on a subset of GRPC methods which are expected to be fast.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: gitaly-main/gitaly-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/gitaly-main/gitaly-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "877804374"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(grpc_server_handling_seconds_bucket{grpc_method!~"CalculateChecksum|CommitLanguages|CommitStats|CreateFork|CreateRepositoryFromURL|FetchInternalRemote|FetchIntoObjectPool|FetchRemote|FetchSourceBranch|FindRemoteRepository|FindRemoteRootRef|Fsck|GarbageCollect|OptimizeRepository|PackObjectsHookWithSidechannel|PostReceiveHook|PostUploadPackWithSidechannel|PreReceiveHook|RepackFull|RepackIncremental|ReplicateRepository|RepositorySize|SSHUploadPackWithSidechannel|UpdateHook",grpc_service!="gitaly.OperationService",grpc_type="unary",job="gitaly"}[5m])
          )
        )
      runbook: docs/gitaly/README.md#alerts
      title: The goserver SLI of the gitaly service has an apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:confidence:ratio_1h{component="goserver",confidence="98%",type="gitaly"}
          < (1 - 14.4 * 0.001000)
        )
        and on (type,component)
        (
          gitlab_component_apdex:confidence:ratio_5m{component="goserver",confidence="98%",type="gitaly"}
          < (1 - 14.4 * 0.001000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="goserver",type="gitaly"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: GitalyServiceGoserverApdexSLOViolation
    annotations:
      description: |
        This SLI monitors all Gitaly GRPC requests in aggregate, excluding the OperationService. GRPC failures which are considered to be the "server's fault" are counted as errors. The apdex score is based on a subset of GRPC methods which are expected to be fast.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: gitaly-main/gitaly-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/gitaly-main/gitaly-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "877804374"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(grpc_server_handling_seconds_bucket{grpc_method!~"CalculateChecksum|CommitLanguages|CommitStats|CreateFork|CreateRepositoryFromURL|FetchInternalRemote|FetchIntoObjectPool|FetchRemote|FetchSourceBranch|FindRemoteRepository|FindRemoteRootRef|Fsck|GarbageCollect|OptimizeRepository|PackObjectsHookWithSidechannel|PostReceiveHook|PostUploadPackWithSidechannel|PreReceiveHook|RepackFull|RepackIncremental|ReplicateRepository|RepositorySize|SSHUploadPackWithSidechannel|UpdateHook",grpc_service!="gitaly.OperationService",grpc_type="unary",job="gitaly"}[5m])
          )
        )
      runbook: docs/gitaly/README.md#alerts
      title: The goserver SLI of the gitaly service has an apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:confidence:ratio_6h{component="goserver",confidence="98%",type="gitaly"}
          < (1 - 6 * 0.001000)
        )
        and on (type,component)
        (
          gitlab_component_apdex:confidence:ratio_30m{component="goserver",confidence="98%",type="gitaly"}
          < (1 - 6 * 0.001000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="goserver",type="gitaly"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: GitalyServiceGoserverErrorSLOViolation
    annotations:
      description: |
        This SLI monitors all Gitaly GRPC requests in aggregate, excluding the OperationService. GRPC failures which are considered to be the "server's fault" are counted as errors. The apdex score is based on a subset of GRPC methods which are expected to be fast.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: gitaly-main/gitaly-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/gitaly-main/gitaly-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "594670537"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          label_replace(rate(gitaly_service_client_requests_total{grpc_code!~"AlreadyExists|Canceled|DeadlineExceeded|FailedPrecondition|InvalidArgument|NotFound|OK|PermissionDenied|ResourceExhausted|Unauthenticated",grpc_service!="gitaly.OperationService",job="gitaly"}[5m]), "_c", "0", "", "")
          or
          label_replace(rate(gitaly_service_client_requests_total{deadline_type!="limited",grpc_code="DeadlineExceeded",grpc_service!="gitaly.OperationService",job="gitaly"}[5m]), "_c", "1", "", "")
        )
      runbook: docs/gitaly/README.md#alerts
      title: The goserver SLI of the gitaly service has an error rate violating SLO
    expr: |
      (
        (
          gitlab_component_errors:confidence:ratio_1h{component="goserver",confidence="98%",type="gitaly"}
          > (14.4 * 0.000500)
        )
        and on (type,component)
        (
          gitlab_component_errors:confidence:ratio_5m{component="goserver",confidence="98%",type="gitaly"}
          > (14.4 * 0.000500)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="goserver",type="gitaly"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: GitalyServiceGoserverErrorSLOViolation
    annotations:
      description: |
        This SLI monitors all Gitaly GRPC requests in aggregate, excluding the OperationService. GRPC failures which are considered to be the "server's fault" are counted as errors. The apdex score is based on a subset of GRPC methods which are expected to be fast.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: gitaly-main/gitaly-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/gitaly-main/gitaly-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "594670537"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          label_replace(rate(gitaly_service_client_requests_total{grpc_code!~"AlreadyExists|Canceled|DeadlineExceeded|FailedPrecondition|InvalidArgument|NotFound|OK|PermissionDenied|ResourceExhausted|Unauthenticated",grpc_service!="gitaly.OperationService",job="gitaly"}[5m]), "_c", "0", "", "")
          or
          label_replace(rate(gitaly_service_client_requests_total{deadline_type!="limited",grpc_code="DeadlineExceeded",grpc_service!="gitaly.OperationService",job="gitaly"}[5m]), "_c", "1", "", "")
        )
      runbook: docs/gitaly/README.md#alerts
      title: The goserver SLI of the gitaly service has an error rate violating SLO
    expr: |
      (
        (
          gitlab_component_errors:confidence:ratio_6h{component="goserver",confidence="98%",type="gitaly"}
          > (6 * 0.000500)
        )
        and on (type,component)
        (
          gitlab_component_errors:confidence:ratio_30m{component="goserver",confidence="98%",type="gitaly"}
          > (6 * 0.000500)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="goserver",type="gitaly"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: GitalyServiceGoserverTrafficCessation
    annotations:
      description: |
        This SLI monitors all Gitaly GRPC requests in aggregate, excluding the OperationService. GRPC failures which are considered to be the "server's fault" are counted as errors. The apdex score is based on a subset of GRPC methods which are expected to be fast.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: gitaly-main/gitaly-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/gitaly-main/gitaly-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1687078843"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitaly_service_client_requests_total{grpc_service!="gitaly.OperationService",job="gitaly"}[5m])
        )
      runbook: docs/gitaly/README.md#alerts
      title: The goserver SLI of the gitaly service has not received any traffic in
        the past 30m
    expr: |
      gitlab_component_ops:rate_30m{component="goserver",type="gitaly"} == 0
      and
      gitlab_component_ops:rate_30m{component="goserver",type="gitaly"} offset 1h >= 0.16666666666666666
    for: 5m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: GitalyServiceGoserverTrafficAbsent
    annotations:
      description: |
        This SLI monitors all Gitaly GRPC requests in aggregate, excluding the OperationService. GRPC failures which are considered to be the "server's fault" are counted as errors. The apdex score is based on a subset of GRPC methods which are expected to be fast.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: gitaly-main/gitaly-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/gitaly-main/gitaly-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1687078843"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitaly_service_client_requests_total{grpc_service!="gitaly.OperationService",job="gitaly"}[5m])
        )
      runbook: docs/gitaly/README.md#alerts
      title: The goserver SLI of the gitaly service has not reported any traffic in
        the past 30m
    expr: |
      gitlab_component_ops:rate_5m{component="goserver",type="gitaly"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="goserver",type="gitaly"}
    for: 30m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
- interval: 1m
  name: 'Service Component Alerts: gitlab-shell'
  rules:
  - alert: GitlabShellServiceGrpcRequestsApdexSLOViolation
    annotations:
      description: |
        A proxy measurement of the number of GRPC SSH service requests made to Gitaly and Praefect.

        Since we are unable to measure gitlab-shell directly at present, this is the best substitute we can provide.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: gitlab-shell-main/gitlab-shell-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/gitlab-shell-main/gitlab-shell-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2041855155"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(grpc_server_handling_seconds_bucket{grpc_service="gitaly.SSHService",grpc_type="unary",job="gitaly"}[5m])
          )
        )
      runbook: docs/gitlab-shell/README.md#alerts
      title: The grpc_requests SLI of the gitlab-shell service has an apdex violating
        SLO
    expr: |
      (
        (
          gitlab_component_apdex:confidence:ratio_1h{component="grpc_requests",confidence="98%",type="gitlab-shell"}
          < (1 - 14.4 * 0.001000)
        )
        and on (type,component)
        (
          gitlab_component_apdex:confidence:ratio_5m{component="grpc_requests",confidence="98%",type="gitlab-shell"}
          < (1 - 14.4 * 0.001000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="grpc_requests",type="gitlab-shell"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: GitlabShellServiceGrpcRequestsApdexSLOViolation
    annotations:
      description: |
        A proxy measurement of the number of GRPC SSH service requests made to Gitaly and Praefect.

        Since we are unable to measure gitlab-shell directly at present, this is the best substitute we can provide.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: gitlab-shell-main/gitlab-shell-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/gitlab-shell-main/gitlab-shell-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2041855155"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(grpc_server_handling_seconds_bucket{grpc_service="gitaly.SSHService",grpc_type="unary",job="gitaly"}[5m])
          )
        )
      runbook: docs/gitlab-shell/README.md#alerts
      title: The grpc_requests SLI of the gitlab-shell service has an apdex violating
        SLO
    expr: |
      (
        (
          gitlab_component_apdex:confidence:ratio_6h{component="grpc_requests",confidence="98%",type="gitlab-shell"}
          < (1 - 6 * 0.001000)
        )
        and on (type,component)
        (
          gitlab_component_apdex:confidence:ratio_30m{component="grpc_requests",confidence="98%",type="gitlab-shell"}
          < (1 - 6 * 0.001000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="grpc_requests",type="gitlab-shell"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: GitlabShellServiceGrpcRequestsErrorSLOViolation
    annotations:
      description: |
        A proxy measurement of the number of GRPC SSH service requests made to Gitaly and Praefect.

        Since we are unable to measure gitlab-shell directly at present, this is the best substitute we can provide.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: gitlab-shell-main/gitlab-shell-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/gitlab-shell-main/gitlab-shell-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "928307072"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(grpc_server_handled_total{grpc_code!~"AlreadyExists|Canceled|DeadlineExceeded|FailedPrecondition|InvalidArgument|NotFound|OK|PermissionDenied|ResourceExhausted|Unauthenticated",grpc_service="gitaly.SSHService",job="gitaly"}[5m])
        )
      runbook: docs/gitlab-shell/README.md#alerts
      title: The grpc_requests SLI of the gitlab-shell service has an error rate violating
        SLO
    expr: |
      (
        (
          gitlab_component_errors:confidence:ratio_1h{component="grpc_requests",confidence="98%",type="gitlab-shell"}
          > (14.4 * 0.001000)
        )
        and on (type,component)
        (
          gitlab_component_errors:confidence:ratio_5m{component="grpc_requests",confidence="98%",type="gitlab-shell"}
          > (14.4 * 0.001000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="grpc_requests",type="gitlab-shell"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: GitlabShellServiceGrpcRequestsErrorSLOViolation
    annotations:
      description: |
        A proxy measurement of the number of GRPC SSH service requests made to Gitaly and Praefect.

        Since we are unable to measure gitlab-shell directly at present, this is the best substitute we can provide.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: gitlab-shell-main/gitlab-shell-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/gitlab-shell-main/gitlab-shell-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "928307072"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(grpc_server_handled_total{grpc_code!~"AlreadyExists|Canceled|DeadlineExceeded|FailedPrecondition|InvalidArgument|NotFound|OK|PermissionDenied|ResourceExhausted|Unauthenticated",grpc_service="gitaly.SSHService",job="gitaly"}[5m])
        )
      runbook: docs/gitlab-shell/README.md#alerts
      title: The grpc_requests SLI of the gitlab-shell service has an error rate violating
        SLO
    expr: |
      (
        (
          gitlab_component_errors:confidence:ratio_6h{component="grpc_requests",confidence="98%",type="gitlab-shell"}
          > (6 * 0.001000)
        )
        and on (type,component)
        (
          gitlab_component_errors:confidence:ratio_30m{component="grpc_requests",confidence="98%",type="gitlab-shell"}
          > (6 * 0.001000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="grpc_requests",type="gitlab-shell"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: GitlabShellServiceGrpcRequestsTrafficCessation
    annotations:
      description: |
        A proxy measurement of the number of GRPC SSH service requests made to Gitaly and Praefect.

        Since we are unable to measure gitlab-shell directly at present, this is the best substitute we can provide.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: gitlab-shell-main/gitlab-shell-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/gitlab-shell-main/gitlab-shell-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3837654202"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitaly_service_client_requests_total{grpc_service="gitaly.SSHService",job="gitaly"}[5m])
        )
      runbook: docs/gitlab-shell/README.md#alerts
      title: The grpc_requests SLI of the gitlab-shell service has not received any
        traffic in the past 30m
    expr: |
      gitlab_component_ops:rate_30m{component="grpc_requests",type="gitlab-shell"} == 0
      and
      gitlab_component_ops:rate_30m{component="grpc_requests",type="gitlab-shell"} offset 1h >= 0.16666666666666666
    for: 5m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: GitlabShellServiceGrpcRequestsTrafficAbsent
    annotations:
      description: |
        A proxy measurement of the number of GRPC SSH service requests made to Gitaly and Praefect.

        Since we are unable to measure gitlab-shell directly at present, this is the best substitute we can provide.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: gitlab-shell-main/gitlab-shell-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/gitlab-shell-main/gitlab-shell-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3837654202"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitaly_service_client_requests_total{grpc_service="gitaly.SSHService",job="gitaly"}[5m])
        )
      runbook: docs/gitlab-shell/README.md#alerts
      title: The grpc_requests SLI of the gitlab-shell service has not reported any
        traffic in the past 30m
    expr: |
      gitlab_component_ops:rate_5m{component="grpc_requests",type="gitlab-shell"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="grpc_requests",type="gitlab-shell"}
    for: 30m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
- interval: 1m
  name: 'Service Component Alerts: kube'
  rules:
  - alert: KubeServiceApiserverApdexSLOViolation
    annotations:
      description: |
        The Kubernetes API server validates and configures data for the api objects which include pods, services, and others. The API Server services REST operations and provides the frontend to the cluster's shared state through which all other components interact.

        This SLI measures all non-health-check endpoints. Long-polling endpoints are excluded from apdex scores.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: kube-main/kube-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/kube-main/kube-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1666123778"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(apiserver_request_duration_seconds_bucket{job="apiserver",scope!="",verb!~"^(?:CONNECT|WATCHLIST|WATCH|PROXY)$"}[5m])
          )
        )
      runbook: docs/kube/README.md#alerts
      title: The apiserver SLI of the kube service has an apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:ratio_1h{component="apiserver",type="kube"}
          < (1 - 14.4 * 0.001000)
        )
        and on (type,component)
        (
          gitlab_component_apdex:ratio_5m{component="apiserver",type="kube"}
          < (1 - 14.4 * 0.001000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="apiserver",type="kube"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "no"
      window: 1h
  - alert: KubeServiceApiserverApdexSLOViolation
    annotations:
      description: |
        The Kubernetes API server validates and configures data for the api objects which include pods, services, and others. The API Server services REST operations and provides the frontend to the cluster's shared state through which all other components interact.

        This SLI measures all non-health-check endpoints. Long-polling endpoints are excluded from apdex scores.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: kube-main/kube-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/kube-main/kube-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1666123778"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(apiserver_request_duration_seconds_bucket{job="apiserver",scope!="",verb!~"^(?:CONNECT|WATCHLIST|WATCH|PROXY)$"}[5m])
          )
        )
      runbook: docs/kube/README.md#alerts
      title: The apiserver SLI of the kube service has an apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:ratio_6h{component="apiserver",type="kube"}
          < (1 - 6 * 0.001000)
        )
        and on (type,component)
        (
          gitlab_component_apdex:ratio_30m{component="apiserver",type="kube"}
          < (1 - 6 * 0.001000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="apiserver",type="kube"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "no"
      window: 6h
  - alert: KubeServiceApiserverErrorSLOViolation
    annotations:
      description: |
        The Kubernetes API server validates and configures data for the api objects which include pods, services, and others. The API Server services REST operations and provides the frontend to the cluster's shared state through which all other components interact.

        This SLI measures all non-health-check endpoints. Long-polling endpoints are excluded from apdex scores.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: kube-main/kube-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/kube-main/kube-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "693885455"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(apiserver_request_total{code=~"5..",job="apiserver",scope!=""}[5m])
        )
      runbook: docs/kube/README.md#alerts
      title: The apiserver SLI of the kube service has an error rate violating SLO
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="apiserver",type="kube"}
          > (14.4 * 0.000500)
        )
        and on (type,component)
        (
          gitlab_component_errors:ratio_5m{component="apiserver",type="kube"}
          > (14.4 * 0.000500)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="apiserver",type="kube"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 1h
  - alert: KubeServiceApiserverErrorSLOViolation
    annotations:
      description: |
        The Kubernetes API server validates and configures data for the api objects which include pods, services, and others. The API Server services REST operations and provides the frontend to the cluster's shared state through which all other components interact.

        This SLI measures all non-health-check endpoints. Long-polling endpoints are excluded from apdex scores.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: kube-main/kube-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/kube-main/kube-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "693885455"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(apiserver_request_total{code=~"5..",job="apiserver",scope!=""}[5m])
        )
      runbook: docs/kube/README.md#alerts
      title: The apiserver SLI of the kube service has an error rate violating SLO
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="apiserver",type="kube"}
          > (6 * 0.000500)
        )
        and on (type,component)
        (
          gitlab_component_errors:ratio_30m{component="apiserver",type="kube"}
          > (6 * 0.000500)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="apiserver",type="kube"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 6h
  - alert: KubeServiceApiserverTrafficCessation
    annotations:
      description: |
        The Kubernetes API server validates and configures data for the api objects which include pods, services, and others. The API Server services REST operations and provides the frontend to the cluster's shared state through which all other components interact.

        This SLI measures all non-health-check endpoints. Long-polling endpoints are excluded from apdex scores.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: kube-main/kube-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/kube-main/kube-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3184144040"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(apiserver_request_total{job="apiserver",scope!=""}[5m])
        )
      runbook: docs/kube/README.md#alerts
      title: The apiserver SLI of the kube service has not received any traffic in
        the past 30m
    expr: |
      gitlab_component_ops:rate_30m{component="apiserver",type="kube"} == 0
      and
      gitlab_component_ops:rate_30m{component="apiserver",type="kube"} offset 1h >= 0.16666666666666666
    for: 5m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "no"
  - alert: KubeServiceApiserverTrafficAbsent
    annotations:
      description: |
        The Kubernetes API server validates and configures data for the api objects which include pods, services, and others. The API Server services REST operations and provides the frontend to the cluster's shared state through which all other components interact.

        This SLI measures all non-health-check endpoints. Long-polling endpoints are excluded from apdex scores.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: kube-main/kube-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/kube-main/kube-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3184144040"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(apiserver_request_total{job="apiserver",scope!=""}[5m])
        )
      runbook: docs/kube/README.md#alerts
      title: The apiserver SLI of the kube service has not reported any traffic in
        the past 30m
    expr: |
      gitlab_component_ops:rate_5m{component="apiserver",type="kube"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="apiserver",type="kube"}
    for: 30m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "no"
- interval: 1m
  name: 'Service Component Alerts: praefect'
  rules:
  - alert: PraefectServiceProxyApdexSLOViolation
    annotations:
      description: |
        All Gitaly operations pass through the Praefect proxy on the way to a Gitaly instance. This SLI monitors those operations in aggregate.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: praefect-main/praefect-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/praefect-main/praefect-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2100325234"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(grpc_server_handling_seconds_bucket{grpc_type="unary",job="praefect"}[5m])
          )
        )
      runbook: docs/praefect/README.md#alerts
      title: The proxy SLI of the praefect service has an apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:confidence:ratio_1h{component="proxy",confidence="98%",type="praefect"}
          < (1 - 14.4 * 0.005000)
        )
        and on (type,component)
        (
          gitlab_component_apdex:confidence:ratio_5m{component="proxy",confidence="98%",type="praefect"}
          < (1 - 14.4 * 0.005000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="proxy",type="praefect"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: PraefectServiceProxyApdexSLOViolation
    annotations:
      description: |
        All Gitaly operations pass through the Praefect proxy on the way to a Gitaly instance. This SLI monitors those operations in aggregate.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: praefect-main/praefect-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/praefect-main/praefect-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2100325234"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(grpc_server_handling_seconds_bucket{grpc_type="unary",job="praefect"}[5m])
          )
        )
      runbook: docs/praefect/README.md#alerts
      title: The proxy SLI of the praefect service has an apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:confidence:ratio_6h{component="proxy",confidence="98%",type="praefect"}
          < (1 - 6 * 0.005000)
        )
        and on (type,component)
        (
          gitlab_component_apdex:confidence:ratio_30m{component="proxy",confidence="98%",type="praefect"}
          < (1 - 6 * 0.005000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="proxy",type="praefect"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: PraefectServiceProxyErrorSLOViolation
    annotations:
      description: |
        All Gitaly operations pass through the Praefect proxy on the way to a Gitaly instance. This SLI monitors those operations in aggregate.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: praefect-main/praefect-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/praefect-main/praefect-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1178914866"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(grpc_server_handled_total{grpc_code!~"^(OK|NotFound|Unauthenticated|AlreadyExists|FailedPrecondition|Canceled)$",job="praefect"}[5m])
        )
      runbook: docs/praefect/README.md#alerts
      title: The proxy SLI of the praefect service has an error rate violating SLO
    expr: |
      (
        (
          gitlab_component_errors:confidence:ratio_1h{component="proxy",confidence="98%",type="praefect"}
          > (14.4 * 0.000500)
        )
        and on (type,component)
        (
          gitlab_component_errors:confidence:ratio_5m{component="proxy",confidence="98%",type="praefect"}
          > (14.4 * 0.000500)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="proxy",type="praefect"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: PraefectServiceProxyErrorSLOViolation
    annotations:
      description: |
        All Gitaly operations pass through the Praefect proxy on the way to a Gitaly instance. This SLI monitors those operations in aggregate.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: praefect-main/praefect-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/praefect-main/praefect-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1178914866"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(grpc_server_handled_total{grpc_code!~"^(OK|NotFound|Unauthenticated|AlreadyExists|FailedPrecondition|Canceled)$",job="praefect"}[5m])
        )
      runbook: docs/praefect/README.md#alerts
      title: The proxy SLI of the praefect service has an error rate violating SLO
    expr: |
      (
        (
          gitlab_component_errors:confidence:ratio_6h{component="proxy",confidence="98%",type="praefect"}
          > (6 * 0.000500)
        )
        and on (type,component)
        (
          gitlab_component_errors:confidence:ratio_30m{component="proxy",confidence="98%",type="praefect"}
          > (6 * 0.000500)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="proxy",type="praefect"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: PraefectServiceProxyTrafficCessation
    annotations:
      description: |
        All Gitaly operations pass through the Praefect proxy on the way to a Gitaly instance. This SLI monitors those operations in aggregate.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: praefect-main/praefect-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/praefect-main/praefect-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2797965992"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(grpc_server_handled_total{job="praefect"}[5m])
        )
      runbook: docs/praefect/README.md#alerts
      title: The proxy SLI of the praefect service has not received any traffic in
        the past 30m
    expr: |
      gitlab_component_ops:rate_30m{component="proxy",type="praefect"} == 0
      and
      gitlab_component_ops:rate_30m{component="proxy",type="praefect"} offset 1h >= 0.16666666666666666
    for: 5m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: PraefectServiceProxyTrafficAbsent
    annotations:
      description: |
        All Gitaly operations pass through the Praefect proxy on the way to a Gitaly instance. This SLI monitors those operations in aggregate.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: praefect-main/praefect-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/praefect-main/praefect-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2797965992"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(grpc_server_handled_total{job="praefect"}[5m])
        )
      runbook: docs/praefect/README.md#alerts
      title: The proxy SLI of the praefect service has not reported any traffic in
        the past 30m
    expr: |
      gitlab_component_ops:rate_5m{component="proxy",type="praefect"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="proxy",type="praefect"}
    for: 30m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: PraefectServiceReplicatorQueueApdexSLOViolation
    annotations:
      description: |
        Praefect replication operations. Latency represents the queuing delay before replication is carried out.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: praefect-main/praefect-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/praefect-main/praefect-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "374957850"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(gitaly_praefect_replication_delay_bucket{}[5m])
          )
        )
      runbook: docs/praefect/README.md#alerts
      title: The replicator_queue SLI of the praefect service has an apdex violating
        SLO
    expr: |
      (
        (
          gitlab_component_apdex:confidence:ratio_1h{component="replicator_queue",confidence="98%",type="praefect"}
          < (1 - 14.4 * 0.005000)
        )
        and on (type,component)
        (
          gitlab_component_apdex:confidence:ratio_5m{component="replicator_queue",confidence="98%",type="praefect"}
          < (1 - 14.4 * 0.005000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="replicator_queue",type="praefect"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "no"
      window: 1h
  - alert: PraefectServiceReplicatorQueueApdexSLOViolation
    annotations:
      description: |
        Praefect replication operations. Latency represents the queuing delay before replication is carried out.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: praefect-main/praefect-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/praefect-main/praefect-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "374957850"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(gitaly_praefect_replication_delay_bucket{}[5m])
          )
        )
      runbook: docs/praefect/README.md#alerts
      title: The replicator_queue SLI of the praefect service has an apdex violating
        SLO
    expr: |
      (
        (
          gitlab_component_apdex:confidence:ratio_6h{component="replicator_queue",confidence="98%",type="praefect"}
          < (1 - 6 * 0.005000)
        )
        and on (type,component)
        (
          gitlab_component_apdex:confidence:ratio_30m{component="replicator_queue",confidence="98%",type="praefect"}
          < (1 - 6 * 0.005000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="replicator_queue",type="praefect"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "no"
      window: 6h
  - alert: PraefectServiceReplicatorQueueTrafficCessation
    annotations:
      description: |
        Praefect replication operations. Latency represents the queuing delay before replication is carried out.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: praefect-main/praefect-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/praefect-main/praefect-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1815534001"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitaly_praefect_replication_delay_bucket{le="+Inf"}[5m])
        )
      runbook: docs/praefect/README.md#alerts
      title: The replicator_queue SLI of the praefect service has not received any
        traffic in the past 30m
    expr: |
      gitlab_component_ops:rate_30m{component="replicator_queue",type="praefect"} == 0
      and
      gitlab_component_ops:rate_30m{component="replicator_queue",type="praefect"} offset 1h >= 0.16666666666666666
    for: 5m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "no"
  - alert: PraefectServiceReplicatorQueueTrafficAbsent
    annotations:
      description: |
        Praefect replication operations. Latency represents the queuing delay before replication is carried out.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: praefect-main/praefect-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/praefect-main/praefect-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1815534001"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitaly_praefect_replication_delay_bucket{le="+Inf"}[5m])
        )
      runbook: docs/praefect/README.md#alerts
      title: The replicator_queue SLI of the praefect service has not reported any
        traffic in the past 30m
    expr: |
      gitlab_component_ops:rate_5m{component="replicator_queue",type="praefect"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="replicator_queue",type="praefect"}
    for: 30m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "no"
- interval: 1m
  name: 'Service Component Alerts: redis'
  rules:
  - alert: RedisServicePrimaryServerTrafficCessation
    annotations:
      description: |
        Operations on the Redis primary for Redis instance.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: redis-main/redis-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/redis-main/redis-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "175328272"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(redis_commands_processed_total{type="redis"}[5m]) and on (instance) redis_instance_info{role="master"}
        )
      runbook: docs/redis/README.md#alerts
      title: The primary_server SLI of the redis service has not received any traffic
        in the past 30m
    expr: |
      gitlab_component_ops:rate_30m{component="primary_server",type="redis"} == 0
      and
      gitlab_component_ops:rate_30m{component="primary_server",type="redis"} offset 1h >= 0.16666666666666666
    for: 5m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: RedisServicePrimaryServerTrafficAbsent
    annotations:
      description: |
        Operations on the Redis primary for Redis instance.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: redis-main/redis-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/redis-main/redis-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "175328272"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(redis_commands_processed_total{type="redis"}[5m]) and on (instance) redis_instance_info{role="master"}
        )
      runbook: docs/redis/README.md#alerts
      title: The primary_server SLI of the redis service has not reported any traffic
        in the past 30m
    expr: |
      gitlab_component_ops:rate_5m{component="primary_server",type="redis"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="primary_server",type="redis"}
    for: 30m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: RedisServiceRailsRedisClientApdexSLOViolation
    annotations:
      description: |
        Aggregation of all Redis operations issued from the Rails codebase through `Gitlab::Redis::Wrapper` subclasses.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: redis-main/redis-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/redis-main/redis-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2248986914"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(gitlab_redis_client_requests_duration_seconds_bucket{}[5m])
          )
        )
      runbook: docs/redis/README.md#alerts
      title: The rails_redis_client SLI of the redis service has an apdex violating
        SLO
    expr: |
      (
        (
          gitlab_component_apdex:confidence:ratio_1h{component="rails_redis_client",confidence="98%",type="redis"}
          < (1 - 14.4 * 0.000500)
        )
        and on (type,component)
        (
          gitlab_component_apdex:confidence:ratio_5m{component="rails_redis_client",confidence="98%",type="redis"}
          < (1 - 14.4 * 0.000500)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="rails_redis_client",type="redis"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: RedisServiceRailsRedisClientApdexSLOViolation
    annotations:
      description: |
        Aggregation of all Redis operations issued from the Rails codebase through `Gitlab::Redis::Wrapper` subclasses.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: redis-main/redis-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/redis-main/redis-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2248986914"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(gitlab_redis_client_requests_duration_seconds_bucket{}[5m])
          )
        )
      runbook: docs/redis/README.md#alerts
      title: The rails_redis_client SLI of the redis service has an apdex violating
        SLO
    expr: |
      (
        (
          gitlab_component_apdex:confidence:ratio_6h{component="rails_redis_client",confidence="98%",type="redis"}
          < (1 - 6 * 0.000500)
        )
        and on (type,component)
        (
          gitlab_component_apdex:confidence:ratio_30m{component="rails_redis_client",confidence="98%",type="redis"}
          < (1 - 6 * 0.000500)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="rails_redis_client",type="redis"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: RedisServiceRailsRedisClientErrorSLOViolation
    annotations:
      description: |
        Aggregation of all Redis operations issued from the Rails codebase through `Gitlab::Redis::Wrapper` subclasses.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: redis-main/redis-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/redis-main/redis-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "487497488"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitlab_redis_client_exceptions_total{}[5m])
        )
      runbook: docs/redis/README.md#alerts
      title: The rails_redis_client SLI of the redis service has an error rate violating
        SLO
    expr: |
      (
        (
          gitlab_component_errors:confidence:ratio_1h{component="rails_redis_client",confidence="98%",type="redis"}
          > (14.4 * 0.001000)
        )
        and on (type,component)
        (
          gitlab_component_errors:confidence:ratio_5m{component="rails_redis_client",confidence="98%",type="redis"}
          > (14.4 * 0.001000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="rails_redis_client",type="redis"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: RedisServiceRailsRedisClientErrorSLOViolation
    annotations:
      description: |
        Aggregation of all Redis operations issued from the Rails codebase through `Gitlab::Redis::Wrapper` subclasses.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: redis-main/redis-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/redis-main/redis-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "487497488"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitlab_redis_client_exceptions_total{}[5m])
        )
      runbook: docs/redis/README.md#alerts
      title: The rails_redis_client SLI of the redis service has an error rate violating
        SLO
    expr: |
      (
        (
          gitlab_component_errors:confidence:ratio_6h{component="rails_redis_client",confidence="98%",type="redis"}
          > (6 * 0.001000)
        )
        and on (type,component)
        (
          gitlab_component_errors:confidence:ratio_30m{component="rails_redis_client",confidence="98%",type="redis"}
          > (6 * 0.001000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="rails_redis_client",type="redis"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: RedisServiceRailsRedisClientTrafficCessation
    annotations:
      description: |
        Aggregation of all Redis operations issued from the Rails codebase through `Gitlab::Redis::Wrapper` subclasses.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: redis-main/redis-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/redis-main/redis-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3147312091"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitlab_redis_client_requests_total{}[5m])
        )
      runbook: docs/redis/README.md#alerts
      title: The rails_redis_client SLI of the redis service has not received any
        traffic in the past 30m
    expr: |
      gitlab_component_ops:rate_30m{component="rails_redis_client",type="redis"} == 0
      and
      gitlab_component_ops:rate_30m{component="rails_redis_client",type="redis"} offset 1h >= 0.16666666666666666
    for: 5m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: RedisServiceRailsRedisClientTrafficAbsent
    annotations:
      description: |
        Aggregation of all Redis operations issued from the Rails codebase through `Gitlab::Redis::Wrapper` subclasses.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: redis-main/redis-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/redis-main/redis-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3147312091"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitlab_redis_client_requests_total{}[5m])
        )
      runbook: docs/redis/README.md#alerts
      title: The rails_redis_client SLI of the redis service has not reported any
        traffic in the past 30m
    expr: |
      gitlab_component_ops:rate_5m{component="rails_redis_client",type="redis"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="rails_redis_client",type="redis"}
    for: 30m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: RedisServiceRedisApdexSLOViolation
    annotations:
      description: |
        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: redis-main/redis-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/redis-main/redis-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "4221579207"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(gitlab_cache_operation_duration_seconds_bucket{}[5m])
          )
        )
      runbook: docs/redis/README.md#alerts
      title: The redis SLI of the redis service has an apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:confidence:ratio_1h{component="redis",confidence="98%",type="redis"}
          < (1 - 14.4 * 0.000500)
        )
        and on (type,component)
        (
          gitlab_component_apdex:confidence:ratio_5m{component="redis",confidence="98%",type="redis"}
          < (1 - 14.4 * 0.000500)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="redis",type="redis"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: RedisServiceRedisApdexSLOViolation
    annotations:
      description: |
        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: redis-main/redis-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/redis-main/redis-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "4221579207"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(gitlab_cache_operation_duration_seconds_bucket{}[5m])
          )
        )
      runbook: docs/redis/README.md#alerts
      title: The redis SLI of the redis service has an apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:confidence:ratio_6h{component="redis",confidence="98%",type="redis"}
          < (1 - 6 * 0.000500)
        )
        and on (type,component)
        (
          gitlab_component_apdex:confidence:ratio_30m{component="redis",confidence="98%",type="redis"}
          < (1 - 6 * 0.000500)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="redis",type="redis"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: RedisServiceRedisTrafficCessation
    annotations:
      description: |
        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: redis-main/redis-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/redis-main/redis-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3565255509"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitlab_cache_operation_duration_seconds_count{}[5m])
        )
      runbook: docs/redis/README.md#alerts
      title: The redis SLI of the redis service has not received any traffic in the
        past 30m
    expr: |
      gitlab_component_ops:rate_30m{component="redis",type="redis"} == 0
      and
      gitlab_component_ops:rate_30m{component="redis",type="redis"} offset 1h >= 0.16666666666666666
    for: 5m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: RedisServiceRedisTrafficAbsent
    annotations:
      description: |
        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: redis-main/redis-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/redis-main/redis-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3565255509"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitlab_cache_operation_duration_seconds_count{}[5m])
        )
      runbook: docs/redis/README.md#alerts
      title: The redis SLI of the redis service has not reported any traffic in the
        past 30m
    expr: |
      gitlab_component_ops:rate_5m{component="redis",type="redis"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="redis",type="redis"}
    for: 30m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
- interval: 1m
  name: 'Service Component Alerts: registry'
  rules:
  - alert: RegistryServiceServerApdexSLOViolation
    annotations:
      description: |
        Aggregation of all registry HTTP requests.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2863319079"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(registry_http_request_duration_seconds_bucket{route!~"/v2/{name}/blobs/uploads/{uuid}|/v2/{name}/blobs/{digest}"}[5m])
          )
        )
      runbook: docs/registry/README.md#alerts
      title: The server SLI of the registry service has an apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:confidence:ratio_1h{component="server",confidence="98%",type="registry"}
          < (1 - 14.4 * 0.003000)
        )
        and on (type,component)
        (
          gitlab_component_apdex:confidence:ratio_5m{component="server",confidence="98%",type="registry"}
          < (1 - 14.4 * 0.003000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="server",type="registry"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: RegistryServiceServerApdexSLOViolation
    annotations:
      description: |
        Aggregation of all registry HTTP requests.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2863319079"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(registry_http_request_duration_seconds_bucket{route!~"/v2/{name}/blobs/uploads/{uuid}|/v2/{name}/blobs/{digest}"}[5m])
          )
        )
      runbook: docs/registry/README.md#alerts
      title: The server SLI of the registry service has an apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:confidence:ratio_6h{component="server",confidence="98%",type="registry"}
          < (1 - 6 * 0.003000)
        )
        and on (type,component)
        (
          gitlab_component_apdex:confidence:ratio_30m{component="server",confidence="98%",type="registry"}
          < (1 - 6 * 0.003000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="server",type="registry"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: RegistryServiceServerErrorSLOViolation
    annotations:
      description: |
        Aggregation of all registry HTTP requests.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1631973137"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(registry_http_requests_total{code=~"5.."}[5m])
        )
      runbook: docs/registry/README.md#alerts
      title: The server SLI of the registry service has an error rate violating SLO
    expr: |
      (
        (
          gitlab_component_errors:confidence:ratio_1h{component="server",confidence="98%",type="registry"}
          > (14.4 * 0.000100)
        )
        and on (type,component)
        (
          gitlab_component_errors:confidence:ratio_5m{component="server",confidence="98%",type="registry"}
          > (14.4 * 0.000100)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="server",type="registry"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: RegistryServiceServerErrorSLOViolation
    annotations:
      description: |
        Aggregation of all registry HTTP requests.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1631973137"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(registry_http_requests_total{code=~"5.."}[5m])
        )
      runbook: docs/registry/README.md#alerts
      title: The server SLI of the registry service has an error rate violating SLO
    expr: |
      (
        (
          gitlab_component_errors:confidence:ratio_6h{component="server",confidence="98%",type="registry"}
          > (6 * 0.000100)
        )
        and on (type,component)
        (
          gitlab_component_errors:confidence:ratio_30m{component="server",confidence="98%",type="registry"}
          > (6 * 0.000100)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="server",type="registry"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: RegistryServiceServerTrafficCessation
    annotations:
      description: |
        Aggregation of all registry HTTP requests.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2414183688"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(registry_http_requests_total{}[5m])
        )
      runbook: docs/registry/README.md#alerts
      title: The server SLI of the registry service has not received any traffic in
        the past 30m
    expr: |
      gitlab_component_ops:rate_30m{component="server",type="registry"} == 0
      and
      gitlab_component_ops:rate_30m{component="server",type="registry"} offset 1h >= 0.16666666666666666
    for: 5m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: RegistryServiceServerTrafficAbsent
    annotations:
      description: |
        Aggregation of all registry HTTP requests.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2414183688"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(registry_http_requests_total{}[5m])
        )
      runbook: docs/registry/README.md#alerts
      title: The server SLI of the registry service has not reported any traffic in
        the past 30m
    expr: |
      gitlab_component_ops:rate_5m{component="server",type="registry"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="server",type="registry"}
    for: 30m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: RegistryServiceServerRouteBlobDigestDeletesApdexSLOViolation
    annotations:
      description: |
        Delete requests for the blob digest endpoints on the registry.

        Used to delete blobs identified by name and digest.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "693957856"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(registry_http_request_duration_seconds_bucket{method=~"delete",route="/v2/{name}/blobs/{digest}"}[5m])
          )
        )
      runbook: docs/registry/README.md#alerts
      title: The server_route_blob_digest_deletes SLI of the registry service has
        an apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:confidence:ratio_1h{component="server_route_blob_digest_deletes",confidence="98%",type="registry"}
          < (1 - 14.4 * 0.001000)
        )
        and on (type,component)
        (
          gitlab_component_apdex:confidence:ratio_5m{component="server_route_blob_digest_deletes",confidence="98%",type="registry"}
          < (1 - 14.4 * 0.001000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="server_route_blob_digest_deletes",type="registry"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: RegistryServiceServerRouteBlobDigestDeletesApdexSLOViolation
    annotations:
      description: |
        Delete requests for the blob digest endpoints on the registry.

        Used to delete blobs identified by name and digest.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "693957856"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(registry_http_request_duration_seconds_bucket{method=~"delete",route="/v2/{name}/blobs/{digest}"}[5m])
          )
        )
      runbook: docs/registry/README.md#alerts
      title: The server_route_blob_digest_deletes SLI of the registry service has
        an apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:confidence:ratio_6h{component="server_route_blob_digest_deletes",confidence="98%",type="registry"}
          < (1 - 6 * 0.001000)
        )
        and on (type,component)
        (
          gitlab_component_apdex:confidence:ratio_30m{component="server_route_blob_digest_deletes",confidence="98%",type="registry"}
          < (1 - 6 * 0.001000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="server_route_blob_digest_deletes",type="registry"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: RegistryServiceServerRouteBlobDigestDeletesTrafficCessation
    annotations:
      description: |
        Delete requests for the blob digest endpoints on the registry.

        Used to delete blobs identified by name and digest.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "4060903410"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(registry_http_request_duration_seconds_count{method=~"delete",route="/v2/{name}/blobs/{digest}"}[5m])
        )
      runbook: docs/registry/README.md#alerts
      title: The server_route_blob_digest_deletes SLI of the registry service has
        not received any traffic in the past 30m
    expr: |
      gitlab_component_ops:rate_30m{component="server_route_blob_digest_deletes",type="registry"} == 0
      and
      gitlab_component_ops:rate_30m{component="server_route_blob_digest_deletes",type="registry"} offset 1h >= 0.16666666666666666
    for: 5m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: RegistryServiceServerRouteBlobDigestDeletesTrafficAbsent
    annotations:
      description: |
        Delete requests for the blob digest endpoints on the registry.

        Used to delete blobs identified by name and digest.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "4060903410"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(registry_http_request_duration_seconds_count{method=~"delete",route="/v2/{name}/blobs/{digest}"}[5m])
        )
      runbook: docs/registry/README.md#alerts
      title: The server_route_blob_digest_deletes SLI of the registry service has
        not reported any traffic in the past 30m
    expr: |
      gitlab_component_ops:rate_5m{component="server_route_blob_digest_deletes",type="registry"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="server_route_blob_digest_deletes",type="registry"}
    for: 30m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: RegistryServiceServerRouteBlobDigestReadsApdexSLOViolation
    annotations:
      description: |
        All read-requests (GET or HEAD) for the blob endpoints on the registry.

        GET is used to pull a layer gated by the name of repository and uniquely identified by the digest in the registry.

        HEAD is used to check the existence of a layer.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "623369547"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(registry_http_request_duration_seconds_bucket{method=~"get|head",route="/v2/{name}/blobs/{digest}"}[5m])
          )
        )
      runbook: docs/registry/README.md#alerts
      title: The server_route_blob_digest_reads SLI of the registry service has an
        apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:confidence:ratio_1h{component="server_route_blob_digest_reads",confidence="98%",type="registry"}
          < (1 - 14.4 * 0.020000)
        )
        and on (type,component)
        (
          gitlab_component_apdex:confidence:ratio_5m{component="server_route_blob_digest_reads",confidence="98%",type="registry"}
          < (1 - 14.4 * 0.020000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="server_route_blob_digest_reads",type="registry"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: RegistryServiceServerRouteBlobDigestReadsApdexSLOViolation
    annotations:
      description: |
        All read-requests (GET or HEAD) for the blob endpoints on the registry.

        GET is used to pull a layer gated by the name of repository and uniquely identified by the digest in the registry.

        HEAD is used to check the existence of a layer.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "623369547"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(registry_http_request_duration_seconds_bucket{method=~"get|head",route="/v2/{name}/blobs/{digest}"}[5m])
          )
        )
      runbook: docs/registry/README.md#alerts
      title: The server_route_blob_digest_reads SLI of the registry service has an
        apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:confidence:ratio_6h{component="server_route_blob_digest_reads",confidence="98%",type="registry"}
          < (1 - 6 * 0.020000)
        )
        and on (type,component)
        (
          gitlab_component_apdex:confidence:ratio_30m{component="server_route_blob_digest_reads",confidence="98%",type="registry"}
          < (1 - 6 * 0.020000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="server_route_blob_digest_reads",type="registry"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: RegistryServiceServerRouteBlobDigestReadsTrafficCessation
    annotations:
      description: |
        All read-requests (GET or HEAD) for the blob endpoints on the registry.

        GET is used to pull a layer gated by the name of repository and uniquely identified by the digest in the registry.

        HEAD is used to check the existence of a layer.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3739996643"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(registry_http_request_duration_seconds_count{method=~"get|head",route="/v2/{name}/blobs/{digest}"}[5m])
        )
      runbook: docs/registry/README.md#alerts
      title: The server_route_blob_digest_reads SLI of the registry service has not
        received any traffic in the past 30m
    expr: |
      gitlab_component_ops:rate_30m{component="server_route_blob_digest_reads",type="registry"} == 0
      and
      gitlab_component_ops:rate_30m{component="server_route_blob_digest_reads",type="registry"} offset 1h >= 0.16666666666666666
    for: 5m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: RegistryServiceServerRouteBlobDigestReadsTrafficAbsent
    annotations:
      description: |
        All read-requests (GET or HEAD) for the blob endpoints on the registry.

        GET is used to pull a layer gated by the name of repository and uniquely identified by the digest in the registry.

        HEAD is used to check the existence of a layer.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3739996643"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(registry_http_request_duration_seconds_count{method=~"get|head",route="/v2/{name}/blobs/{digest}"}[5m])
        )
      runbook: docs/registry/README.md#alerts
      title: The server_route_blob_digest_reads SLI of the registry service has not
        reported any traffic in the past 30m
    expr: |
      gitlab_component_ops:rate_5m{component="server_route_blob_digest_reads",type="registry"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="server_route_blob_digest_reads",type="registry"}
    for: 30m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: RegistryServiceServerRouteBlobDigestWritesApdexSLOViolation
    annotations:
      description: |
        Write requests (PUT or PATCH or POST) for the registry blob digest endpoints.

        Currently not part of the spec.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2614494702"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(registry_http_request_duration_seconds_bucket{method=~"patch|post|put",route="/v2/{name}/blobs/{digest}"}[5m])
          )
        )
      runbook: docs/registry/README.md#alerts
      title: The server_route_blob_digest_writes SLI of the registry service has an
        apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:confidence:ratio_1h{component="server_route_blob_digest_writes",confidence="98%",type="registry"}
          < (1 - 14.4 * 0.003000)
        )
        and on (type,component)
        (
          gitlab_component_apdex:confidence:ratio_5m{component="server_route_blob_digest_writes",confidence="98%",type="registry"}
          < (1 - 14.4 * 0.003000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="server_route_blob_digest_writes",type="registry"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: RegistryServiceServerRouteBlobDigestWritesApdexSLOViolation
    annotations:
      description: |
        Write requests (PUT or PATCH or POST) for the registry blob digest endpoints.

        Currently not part of the spec.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2614494702"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(registry_http_request_duration_seconds_bucket{method=~"patch|post|put",route="/v2/{name}/blobs/{digest}"}[5m])
          )
        )
      runbook: docs/registry/README.md#alerts
      title: The server_route_blob_digest_writes SLI of the registry service has an
        apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:confidence:ratio_6h{component="server_route_blob_digest_writes",confidence="98%",type="registry"}
          < (1 - 6 * 0.003000)
        )
        and on (type,component)
        (
          gitlab_component_apdex:confidence:ratio_30m{component="server_route_blob_digest_writes",confidence="98%",type="registry"}
          < (1 - 6 * 0.003000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="server_route_blob_digest_writes",type="registry"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: RegistryServiceServerRouteBlobDigestWritesTrafficCessation
    annotations:
      description: |
        Write requests (PUT or PATCH or POST) for the registry blob digest endpoints.

        Currently not part of the spec.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2206725588"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(registry_http_request_duration_seconds_count{method=~"patch|post|put",route="/v2/{name}/blobs/{digest}"}[5m])
        )
      runbook: docs/registry/README.md#alerts
      title: The server_route_blob_digest_writes SLI of the registry service has not
        received any traffic in the past 30m
    expr: |
      gitlab_component_ops:rate_30m{component="server_route_blob_digest_writes",type="registry"} == 0
      and
      gitlab_component_ops:rate_30m{component="server_route_blob_digest_writes",type="registry"} offset 1h >= 0.16666666666666666
    for: 5m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: RegistryServiceServerRouteBlobDigestWritesTrafficAbsent
    annotations:
      description: |
        Write requests (PUT or PATCH or POST) for the registry blob digest endpoints.

        Currently not part of the spec.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2206725588"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(registry_http_request_duration_seconds_count{method=~"patch|post|put",route="/v2/{name}/blobs/{digest}"}[5m])
        )
      runbook: docs/registry/README.md#alerts
      title: The server_route_blob_digest_writes SLI of the registry service has not
        reported any traffic in the past 30m
    expr: |
      gitlab_component_ops:rate_5m{component="server_route_blob_digest_writes",type="registry"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="server_route_blob_digest_writes",type="registry"}
    for: 30m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: RegistryServiceServerRouteBlobUploadUuidDeletesApdexSLOViolation
    annotations:
      description: |
        Delete requests for the registry blob upload endpoints.

        Used to cancel outstanding upload processes, releasing associated resources.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2147876809"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(registry_http_request_duration_seconds_bucket{method=~"delete",route="/v2/{name}/blobs/uploads/{uuid}"}[5m])
          )
        )
      runbook: docs/registry/README.md#alerts
      title: The server_route_blob_upload_uuid_deletes SLI of the registry service
        has an apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:confidence:ratio_1h{component="server_route_blob_upload_uuid_deletes",confidence="98%",type="registry"}
          < (1 - 14.4 * 0.003000)
        )
        and on (type,component)
        (
          gitlab_component_apdex:confidence:ratio_5m{component="server_route_blob_upload_uuid_deletes",confidence="98%",type="registry"}
          < (1 - 14.4 * 0.003000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="server_route_blob_upload_uuid_deletes",type="registry"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: RegistryServiceServerRouteBlobUploadUuidDeletesApdexSLOViolation
    annotations:
      description: |
        Delete requests for the registry blob upload endpoints.

        Used to cancel outstanding upload processes, releasing associated resources.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2147876809"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(registry_http_request_duration_seconds_bucket{method=~"delete",route="/v2/{name}/blobs/uploads/{uuid}"}[5m])
          )
        )
      runbook: docs/registry/README.md#alerts
      title: The server_route_blob_upload_uuid_deletes SLI of the registry service
        has an apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:confidence:ratio_6h{component="server_route_blob_upload_uuid_deletes",confidence="98%",type="registry"}
          < (1 - 6 * 0.003000)
        )
        and on (type,component)
        (
          gitlab_component_apdex:confidence:ratio_30m{component="server_route_blob_upload_uuid_deletes",confidence="98%",type="registry"}
          < (1 - 6 * 0.003000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="server_route_blob_upload_uuid_deletes",type="registry"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: RegistryServiceServerRouteBlobUploadUuidDeletesTrafficCessation
    annotations:
      description: |
        Delete requests for the registry blob upload endpoints.

        Used to cancel outstanding upload processes, releasing associated resources.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "714276442"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(registry_http_request_duration_seconds_count{method=~"delete",route="/v2/{name}/blobs/uploads/{uuid}"}[5m])
        )
      runbook: docs/registry/README.md#alerts
      title: The server_route_blob_upload_uuid_deletes SLI of the registry service
        has not received any traffic in the past 30m
    expr: |
      gitlab_component_ops:rate_30m{component="server_route_blob_upload_uuid_deletes",type="registry"} == 0
      and
      gitlab_component_ops:rate_30m{component="server_route_blob_upload_uuid_deletes",type="registry"} offset 1h >= 0.16666666666666666
    for: 5m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: RegistryServiceServerRouteBlobUploadUuidDeletesTrafficAbsent
    annotations:
      description: |
        Delete requests for the registry blob upload endpoints.

        Used to cancel outstanding upload processes, releasing associated resources.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "714276442"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(registry_http_request_duration_seconds_count{method=~"delete",route="/v2/{name}/blobs/uploads/{uuid}"}[5m])
        )
      runbook: docs/registry/README.md#alerts
      title: The server_route_blob_upload_uuid_deletes SLI of the registry service
        has not reported any traffic in the past 30m
    expr: |
      gitlab_component_ops:rate_5m{component="server_route_blob_upload_uuid_deletes",type="registry"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="server_route_blob_upload_uuid_deletes",type="registry"}
    for: 30m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: RegistryServiceServerRouteBlobUploadUuidReadsApdexSLOViolation
    annotations:
      description: |
        Read requests (GET) for the registry blob upload endpoints.

        GET is used to retrieve the current status of a resumable upload.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "41208249"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(registry_http_request_duration_seconds_bucket{method=~"get|head",route="/v2/{name}/blobs/uploads/{uuid}"}[5m])
          )
        )
      runbook: docs/registry/README.md#alerts
      title: The server_route_blob_upload_uuid_reads SLI of the registry service has
        an apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:confidence:ratio_1h{component="server_route_blob_upload_uuid_reads",confidence="98%",type="registry"}
          < (1 - 14.4 * 0.003000)
        )
        and on (type,component)
        (
          gitlab_component_apdex:confidence:ratio_5m{component="server_route_blob_upload_uuid_reads",confidence="98%",type="registry"}
          < (1 - 14.4 * 0.003000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="server_route_blob_upload_uuid_reads",type="registry"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: RegistryServiceServerRouteBlobUploadUuidReadsApdexSLOViolation
    annotations:
      description: |
        Read requests (GET) for the registry blob upload endpoints.

        GET is used to retrieve the current status of a resumable upload.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "41208249"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(registry_http_request_duration_seconds_bucket{method=~"get|head",route="/v2/{name}/blobs/uploads/{uuid}"}[5m])
          )
        )
      runbook: docs/registry/README.md#alerts
      title: The server_route_blob_upload_uuid_reads SLI of the registry service has
        an apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:confidence:ratio_6h{component="server_route_blob_upload_uuid_reads",confidence="98%",type="registry"}
          < (1 - 6 * 0.003000)
        )
        and on (type,component)
        (
          gitlab_component_apdex:confidence:ratio_30m{component="server_route_blob_upload_uuid_reads",confidence="98%",type="registry"}
          < (1 - 6 * 0.003000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="server_route_blob_upload_uuid_reads",type="registry"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: RegistryServiceServerRouteBlobUploadUuidWritesApdexSLOViolation
    annotations:
      description: |
        Write requests (PUT or PATCH) for the registry blob upload endpoints.

        PUT is used to complete the upload specified by uuid, optionally appending the body as the final chunk.

        PATCH is used to upload a chunk of data for the specified upload.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2199109348"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(registry_http_request_duration_seconds_bucket{method=~"patch|post|put",route="/v2/{name}/blobs/uploads/{uuid}"}[5m])
          )
        )
      runbook: docs/registry/README.md#alerts
      title: The server_route_blob_upload_uuid_writes SLI of the registry service
        has an apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:confidence:ratio_1h{component="server_route_blob_upload_uuid_writes",confidence="98%",type="registry"}
          < (1 - 14.4 * 0.030000)
        )
        and on (type,component)
        (
          gitlab_component_apdex:confidence:ratio_5m{component="server_route_blob_upload_uuid_writes",confidence="98%",type="registry"}
          < (1 - 14.4 * 0.030000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="server_route_blob_upload_uuid_writes",type="registry"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: RegistryServiceServerRouteBlobUploadUuidWritesApdexSLOViolation
    annotations:
      description: |
        Write requests (PUT or PATCH) for the registry blob upload endpoints.

        PUT is used to complete the upload specified by uuid, optionally appending the body as the final chunk.

        PATCH is used to upload a chunk of data for the specified upload.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2199109348"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(registry_http_request_duration_seconds_bucket{method=~"patch|post|put",route="/v2/{name}/blobs/uploads/{uuid}"}[5m])
          )
        )
      runbook: docs/registry/README.md#alerts
      title: The server_route_blob_upload_uuid_writes SLI of the registry service
        has an apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:confidence:ratio_6h{component="server_route_blob_upload_uuid_writes",confidence="98%",type="registry"}
          < (1 - 6 * 0.030000)
        )
        and on (type,component)
        (
          gitlab_component_apdex:confidence:ratio_30m{component="server_route_blob_upload_uuid_writes",confidence="98%",type="registry"}
          < (1 - 6 * 0.030000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="server_route_blob_upload_uuid_writes",type="registry"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: RegistryServiceServerRouteBlobUploadUuidWritesTrafficCessation
    annotations:
      description: |
        Write requests (PUT or PATCH) for the registry blob upload endpoints.

        PUT is used to complete the upload specified by uuid, optionally appending the body as the final chunk.

        PATCH is used to upload a chunk of data for the specified upload.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3946104260"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(registry_http_request_duration_seconds_count{method=~"patch|post|put",route="/v2/{name}/blobs/uploads/{uuid}"}[5m])
        )
      runbook: docs/registry/README.md#alerts
      title: The server_route_blob_upload_uuid_writes SLI of the registry service
        has not received any traffic in the past 30m
    expr: |
      gitlab_component_ops:rate_30m{component="server_route_blob_upload_uuid_writes",type="registry"} == 0
      and
      gitlab_component_ops:rate_30m{component="server_route_blob_upload_uuid_writes",type="registry"} offset 1h >= 0.16666666666666666
    for: 5m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: RegistryServiceServerRouteBlobUploadUuidWritesTrafficAbsent
    annotations:
      description: |
        Write requests (PUT or PATCH) for the registry blob upload endpoints.

        PUT is used to complete the upload specified by uuid, optionally appending the body as the final chunk.

        PATCH is used to upload a chunk of data for the specified upload.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: registry-main/registry-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/registry-main/registry-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3946104260"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(registry_http_request_duration_seconds_count{method=~"patch|post|put",route="/v2/{name}/blobs/uploads/{uuid}"}[5m])
        )
      runbook: docs/registry/README.md#alerts
      title: The server_route_blob_upload_uuid_writes SLI of the registry service
        has not reported any traffic in the past 30m
    expr: |
      gitlab_component_ops:rate_5m{component="server_route_blob_upload_uuid_writes",type="registry"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="server_route_blob_upload_uuid_writes",type="registry"}
    for: 30m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
- interval: 1m
  name: 'Service Component Alerts: sidekiq'
  rules:
  - alert: SidekiqServiceDefaultApdexSLOViolation
    annotations:
      description: |
        All other Sidekiq jobs (not in urgent_cpu_bound or urgent_other queues)

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: sidekiq-main/sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/sidekiq-main/sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "150808140"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(sidekiq_jobs_completion_seconds_bucket{queue!="urgent_cpu_bound",queue!="urgent_other"}[5m])
            or
            rate(sidekiq_jobs_queue_duration_seconds_bucket{queue!="urgent_cpu_bound",queue!="urgent_other"}[5m])
          )
        )
      runbook: docs/sidekiq/README.md#alerts
      title: The default SLI of the sidekiq service has an apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:confidence:ratio_1h{component="default",confidence="98%",type="sidekiq"}
          < (1 - 14.4 * 0.005000)
        )
        and on (type,component)
        (
          gitlab_component_apdex:confidence:ratio_5m{component="default",confidence="98%",type="sidekiq"}
          < (1 - 14.4 * 0.005000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="default",type="sidekiq"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: SidekiqServiceDefaultApdexSLOViolation
    annotations:
      description: |
        All other Sidekiq jobs (not in urgent_cpu_bound or urgent_other queues)

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: sidekiq-main/sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/sidekiq-main/sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "150808140"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(sidekiq_jobs_completion_seconds_bucket{queue!="urgent_cpu_bound",queue!="urgent_other"}[5m])
            or
            rate(sidekiq_jobs_queue_duration_seconds_bucket{queue!="urgent_cpu_bound",queue!="urgent_other"}[5m])
          )
        )
      runbook: docs/sidekiq/README.md#alerts
      title: The default SLI of the sidekiq service has an apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:confidence:ratio_6h{component="default",confidence="98%",type="sidekiq"}
          < (1 - 6 * 0.005000)
        )
        and on (type,component)
        (
          gitlab_component_apdex:confidence:ratio_30m{component="default",confidence="98%",type="sidekiq"}
          < (1 - 6 * 0.005000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="default",type="sidekiq"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: SidekiqServiceDefaultErrorSLOViolation
    annotations:
      description: |
        All other Sidekiq jobs (not in urgent_cpu_bound or urgent_other queues)

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: sidekiq-main/sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/sidekiq-main/sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "707350150"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(sidekiq_jobs_failed_total{queue!="urgent_cpu_bound",queue!="urgent_other"}[5m])
        )
      runbook: docs/sidekiq/README.md#alerts
      title: The default SLI of the sidekiq service has an error rate violating SLO
    expr: |
      (
        (
          gitlab_component_errors:confidence:ratio_1h{component="default",confidence="98%",type="sidekiq"}
          > (14.4 * 0.005000)
        )
        and on (type,component)
        (
          gitlab_component_errors:confidence:ratio_5m{component="default",confidence="98%",type="sidekiq"}
          > (14.4 * 0.005000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="default",type="sidekiq"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: SidekiqServiceDefaultErrorSLOViolation
    annotations:
      description: |
        All other Sidekiq jobs (not in urgent_cpu_bound or urgent_other queues)

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: sidekiq-main/sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/sidekiq-main/sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "707350150"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(sidekiq_jobs_failed_total{queue!="urgent_cpu_bound",queue!="urgent_other"}[5m])
        )
      runbook: docs/sidekiq/README.md#alerts
      title: The default SLI of the sidekiq service has an error rate violating SLO
    expr: |
      (
        (
          gitlab_component_errors:confidence:ratio_6h{component="default",confidence="98%",type="sidekiq"}
          > (6 * 0.005000)
        )
        and on (type,component)
        (
          gitlab_component_errors:confidence:ratio_30m{component="default",confidence="98%",type="sidekiq"}
          > (6 * 0.005000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="default",type="sidekiq"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: SidekiqServiceDefaultTrafficCessation
    annotations:
      description: |
        All other Sidekiq jobs (not in urgent_cpu_bound or urgent_other queues)

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: sidekiq-main/sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/sidekiq-main/sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3009327499"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(sidekiq_jobs_completion_seconds_bucket{le="+Inf",queue!="urgent_cpu_bound",queue!="urgent_other"}[5m])
        )
      runbook: docs/sidekiq/README.md#alerts
      title: The default SLI of the sidekiq service has not received any traffic in
        the past 30m
    expr: |
      gitlab_component_ops:rate_30m{component="default",type="sidekiq"} == 0
      and
      gitlab_component_ops:rate_30m{component="default",type="sidekiq"} offset 1h >= 0.16666666666666666
    for: 5m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: SidekiqServiceDefaultTrafficAbsent
    annotations:
      description: |
        All other Sidekiq jobs (not in urgent_cpu_bound or urgent_other queues)

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: sidekiq-main/sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/sidekiq-main/sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3009327499"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(sidekiq_jobs_completion_seconds_bucket{le="+Inf",queue!="urgent_cpu_bound",queue!="urgent_other"}[5m])
        )
      runbook: docs/sidekiq/README.md#alerts
      title: The default SLI of the sidekiq service has not reported any traffic in
        the past 30m
    expr: |
      gitlab_component_ops:rate_5m{component="default",type="sidekiq"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="default",type="sidekiq"}
    for: 30m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: SidekiqServiceEmailReceiverErrorSLOViolation
    annotations:
      description: |
        Monitors ratio between all received emails and received emails which could not be processed for some reason.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: sidekiq-main/sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/sidekiq-main/sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "212053046"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitlab_transaction_event_email_receiver_error_total{error!="Gitlab::Email::AutoGeneratedEmailError"}[5m])
        )
      runbook: docs/sidekiq/README.md#alerts
      title: The email_receiver SLI of the sidekiq service has an error rate violating
        SLO
    expr: |
      (
        (
          gitlab_component_errors:confidence:ratio_1h{component="email_receiver",confidence="98%",type="sidekiq"}
          > (14.4 * 0.300000)
        )
        and on (type,component)
        (
          gitlab_component_errors:confidence:ratio_5m{component="email_receiver",confidence="98%",type="sidekiq"}
          > (14.4 * 0.300000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="email_receiver",type="sidekiq"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: SidekiqServiceEmailReceiverErrorSLOViolation
    annotations:
      description: |
        Monitors ratio between all received emails and received emails which could not be processed for some reason.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: sidekiq-main/sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/sidekiq-main/sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "212053046"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitlab_transaction_event_email_receiver_error_total{error!="Gitlab::Email::AutoGeneratedEmailError"}[5m])
        )
      runbook: docs/sidekiq/README.md#alerts
      title: The email_receiver SLI of the sidekiq service has an error rate violating
        SLO
    expr: |
      (
        (
          gitlab_component_errors:confidence:ratio_6h{component="email_receiver",confidence="98%",type="sidekiq"}
          > (6 * 0.300000)
        )
        and on (type,component)
        (
          gitlab_component_errors:confidence:ratio_30m{component="email_receiver",confidence="98%",type="sidekiq"}
          > (6 * 0.300000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="email_receiver",type="sidekiq"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: SidekiqServiceEmailReceiverTrafficCessation
    annotations:
      description: |
        Monitors ratio between all received emails and received emails which could not be processed for some reason.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: sidekiq-main/sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/sidekiq-main/sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1545803555"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(sidekiq_jobs_completion_seconds_count{worker=~"EmailReceiverWorker|ServiceDeskEmailReceiverWorker"}[5m])
        )
      runbook: docs/sidekiq/README.md#alerts
      title: The email_receiver SLI of the sidekiq service has not received any traffic
        in the past 30m
    expr: |
      gitlab_component_ops:rate_30m{component="email_receiver",type="sidekiq"} == 0
      and
      gitlab_component_ops:rate_30m{component="email_receiver",type="sidekiq"} offset 1h >= 0.16666666666666666
    for: 5m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: SidekiqServiceEmailReceiverTrafficAbsent
    annotations:
      description: |
        Monitors ratio between all received emails and received emails which could not be processed for some reason.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: sidekiq-main/sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/sidekiq-main/sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1545803555"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(sidekiq_jobs_completion_seconds_count{worker=~"EmailReceiverWorker|ServiceDeskEmailReceiverWorker"}[5m])
        )
      runbook: docs/sidekiq/README.md#alerts
      title: The email_receiver SLI of the sidekiq service has not reported any traffic
        in the past 30m
    expr: |
      gitlab_component_ops:rate_5m{component="email_receiver",type="sidekiq"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="email_receiver",type="sidekiq"}
    for: 30m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: SidekiqServiceUrgentCpuBoundApdexSLOViolation
    annotations:
      description: |
        Sidekiq jobs in the urgent_cpu_bound queue

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: sidekiq-main/sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/sidekiq-main/sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3934814576"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(sidekiq_jobs_completion_seconds_bucket{queue="urgent_cpu_bound"}[5m])
            or
            rate(sidekiq_jobs_queue_duration_seconds_bucket{queue="urgent_cpu_bound"}[5m])
          )
        )
      runbook: docs/sidekiq/README.md#alerts
      title: The urgent_cpu_bound SLI of the sidekiq service has an apdex violating
        SLO
    expr: |
      (
        (
          gitlab_component_apdex:confidence:ratio_1h{component="urgent_cpu_bound",confidence="98%",type="sidekiq"}
          < (1 - 14.4 * 0.005000)
        )
        and on (type,component)
        (
          gitlab_component_apdex:confidence:ratio_5m{component="urgent_cpu_bound",confidence="98%",type="sidekiq"}
          < (1 - 14.4 * 0.005000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="urgent_cpu_bound",type="sidekiq"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: SidekiqServiceUrgentCpuBoundApdexSLOViolation
    annotations:
      description: |
        Sidekiq jobs in the urgent_cpu_bound queue

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: sidekiq-main/sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/sidekiq-main/sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3934814576"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(sidekiq_jobs_completion_seconds_bucket{queue="urgent_cpu_bound"}[5m])
            or
            rate(sidekiq_jobs_queue_duration_seconds_bucket{queue="urgent_cpu_bound"}[5m])
          )
        )
      runbook: docs/sidekiq/README.md#alerts
      title: The urgent_cpu_bound SLI of the sidekiq service has an apdex violating
        SLO
    expr: |
      (
        (
          gitlab_component_apdex:confidence:ratio_6h{component="urgent_cpu_bound",confidence="98%",type="sidekiq"}
          < (1 - 6 * 0.005000)
        )
        and on (type,component)
        (
          gitlab_component_apdex:confidence:ratio_30m{component="urgent_cpu_bound",confidence="98%",type="sidekiq"}
          < (1 - 6 * 0.005000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="urgent_cpu_bound",type="sidekiq"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: SidekiqServiceUrgentCpuBoundErrorSLOViolation
    annotations:
      description: |
        Sidekiq jobs in the urgent_cpu_bound queue

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: sidekiq-main/sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/sidekiq-main/sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "791323806"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(sidekiq_jobs_failed_total{queue="urgent_cpu_bound"}[5m])
        )
      runbook: docs/sidekiq/README.md#alerts
      title: The urgent_cpu_bound SLI of the sidekiq service has an error rate violating
        SLO
    expr: |
      (
        (
          gitlab_component_errors:confidence:ratio_1h{component="urgent_cpu_bound",confidence="98%",type="sidekiq"}
          > (14.4 * 0.005000)
        )
        and on (type,component)
        (
          gitlab_component_errors:confidence:ratio_5m{component="urgent_cpu_bound",confidence="98%",type="sidekiq"}
          > (14.4 * 0.005000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="urgent_cpu_bound",type="sidekiq"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: SidekiqServiceUrgentCpuBoundErrorSLOViolation
    annotations:
      description: |
        Sidekiq jobs in the urgent_cpu_bound queue

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: sidekiq-main/sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/sidekiq-main/sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "791323806"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(sidekiq_jobs_failed_total{queue="urgent_cpu_bound"}[5m])
        )
      runbook: docs/sidekiq/README.md#alerts
      title: The urgent_cpu_bound SLI of the sidekiq service has an error rate violating
        SLO
    expr: |
      (
        (
          gitlab_component_errors:confidence:ratio_6h{component="urgent_cpu_bound",confidence="98%",type="sidekiq"}
          > (6 * 0.005000)
        )
        and on (type,component)
        (
          gitlab_component_errors:confidence:ratio_30m{component="urgent_cpu_bound",confidence="98%",type="sidekiq"}
          > (6 * 0.005000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="urgent_cpu_bound",type="sidekiq"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: SidekiqServiceUrgentCpuBoundTrafficCessation
    annotations:
      description: |
        Sidekiq jobs in the urgent_cpu_bound queue

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: sidekiq-main/sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/sidekiq-main/sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "407471649"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(sidekiq_jobs_completion_seconds_bucket{le="+Inf",queue="urgent_cpu_bound"}[5m])
        )
      runbook: docs/sidekiq/README.md#alerts
      title: The urgent_cpu_bound SLI of the sidekiq service has not received any
        traffic in the past 30m
    expr: |
      gitlab_component_ops:rate_30m{component="urgent_cpu_bound",type="sidekiq"} == 0
      and
      gitlab_component_ops:rate_30m{component="urgent_cpu_bound",type="sidekiq"} offset 1h >= 0.16666666666666666
    for: 5m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: SidekiqServiceUrgentCpuBoundTrafficAbsent
    annotations:
      description: |
        Sidekiq jobs in the urgent_cpu_bound queue

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: sidekiq-main/sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/sidekiq-main/sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "407471649"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(sidekiq_jobs_completion_seconds_bucket{le="+Inf",queue="urgent_cpu_bound"}[5m])
        )
      runbook: docs/sidekiq/README.md#alerts
      title: The urgent_cpu_bound SLI of the sidekiq service has not reported any
        traffic in the past 30m
    expr: |
      gitlab_component_ops:rate_5m{component="urgent_cpu_bound",type="sidekiq"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="urgent_cpu_bound",type="sidekiq"}
    for: 30m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: SidekiqServiceUrgentOtherApdexSLOViolation
    annotations:
      description: |
        Sidekiq jobs in the urgent_other queue

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: sidekiq-main/sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/sidekiq-main/sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3234733158"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(sidekiq_jobs_completion_seconds_bucket{queue="urgent_other"}[5m])
            or
            rate(sidekiq_jobs_queue_duration_seconds_bucket{queue="urgent_other"}[5m])
          )
        )
      runbook: docs/sidekiq/README.md#alerts
      title: The urgent_other SLI of the sidekiq service has an apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:confidence:ratio_1h{component="urgent_other",confidence="98%",type="sidekiq"}
          < (1 - 14.4 * 0.005000)
        )
        and on (type,component)
        (
          gitlab_component_apdex:confidence:ratio_5m{component="urgent_other",confidence="98%",type="sidekiq"}
          < (1 - 14.4 * 0.005000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="urgent_other",type="sidekiq"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: SidekiqServiceUrgentOtherApdexSLOViolation
    annotations:
      description: |
        Sidekiq jobs in the urgent_other queue

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: sidekiq-main/sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/sidekiq-main/sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3234733158"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(sidekiq_jobs_completion_seconds_bucket{queue="urgent_other"}[5m])
            or
            rate(sidekiq_jobs_queue_duration_seconds_bucket{queue="urgent_other"}[5m])
          )
        )
      runbook: docs/sidekiq/README.md#alerts
      title: The urgent_other SLI of the sidekiq service has an apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:confidence:ratio_6h{component="urgent_other",confidence="98%",type="sidekiq"}
          < (1 - 6 * 0.005000)
        )
        and on (type,component)
        (
          gitlab_component_apdex:confidence:ratio_30m{component="urgent_other",confidence="98%",type="sidekiq"}
          < (1 - 6 * 0.005000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="urgent_other",type="sidekiq"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: SidekiqServiceUrgentOtherErrorSLOViolation
    annotations:
      description: |
        Sidekiq jobs in the urgent_other queue

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: sidekiq-main/sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/sidekiq-main/sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "4114960693"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(sidekiq_jobs_failed_total{queue="urgent_other"}[5m])
        )
      runbook: docs/sidekiq/README.md#alerts
      title: The urgent_other SLI of the sidekiq service has an error rate violating
        SLO
    expr: |
      (
        (
          gitlab_component_errors:confidence:ratio_1h{component="urgent_other",confidence="98%",type="sidekiq"}
          > (14.4 * 0.005000)
        )
        and on (type,component)
        (
          gitlab_component_errors:confidence:ratio_5m{component="urgent_other",confidence="98%",type="sidekiq"}
          > (14.4 * 0.005000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="urgent_other",type="sidekiq"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: SidekiqServiceUrgentOtherErrorSLOViolation
    annotations:
      description: |
        Sidekiq jobs in the urgent_other queue

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: sidekiq-main/sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/sidekiq-main/sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "4114960693"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(sidekiq_jobs_failed_total{queue="urgent_other"}[5m])
        )
      runbook: docs/sidekiq/README.md#alerts
      title: The urgent_other SLI of the sidekiq service has an error rate violating
        SLO
    expr: |
      (
        (
          gitlab_component_errors:confidence:ratio_6h{component="urgent_other",confidence="98%",type="sidekiq"}
          > (6 * 0.005000)
        )
        and on (type,component)
        (
          gitlab_component_errors:confidence:ratio_30m{component="urgent_other",confidence="98%",type="sidekiq"}
          > (6 * 0.005000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="urgent_other",type="sidekiq"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: SidekiqServiceUrgentOtherTrafficCessation
    annotations:
      description: |
        Sidekiq jobs in the urgent_other queue

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: sidekiq-main/sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/sidekiq-main/sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2122893503"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(sidekiq_jobs_completion_seconds_bucket{le="+Inf",queue="urgent_other"}[5m])
        )
      runbook: docs/sidekiq/README.md#alerts
      title: The urgent_other SLI of the sidekiq service has not received any traffic
        in the past 30m
    expr: |
      gitlab_component_ops:rate_30m{component="urgent_other",type="sidekiq"} == 0
      and
      gitlab_component_ops:rate_30m{component="urgent_other",type="sidekiq"} offset 1h >= 0.16666666666666666
    for: 5m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: SidekiqServiceUrgentOtherTrafficAbsent
    annotations:
      description: |
        Sidekiq jobs in the urgent_other queue

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: sidekiq-main/sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/sidekiq-main/sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2122893503"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(sidekiq_jobs_completion_seconds_bucket{le="+Inf",queue="urgent_other"}[5m])
        )
      runbook: docs/sidekiq/README.md#alerts
      title: The urgent_other SLI of the sidekiq service has not reported any traffic
        in the past 30m
    expr: |
      gitlab_component_ops:rate_5m{component="urgent_other",type="sidekiq"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="urgent_other",type="sidekiq"}
    for: 30m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
- interval: 1m
  name: 'Service Component Alerts: webservice'
  rules:
  - alert: WebserviceServiceGraphqlQueryApdexSLOViolation
    annotations:
      description: |
        A GraphQL query is executed in the context of a request. An error does not always result in a 5xx error. But could contain errors in the response. Mutliple queries could be batched inside a single request.

        This SLI counts all operations, a succeeded operation does not contain errors in it's response or return a 500 error.

        The number of GraphQL queries meeting their duration target based on the urgency of the endpoint. By default, a query should take no more than 1s. We're working on making the urgency customizable in [this epic](https://gitlab.com/groups/gitlab-org/-/epics/5841).

        We're only taking known operations into account. Known operations are queries defined in our codebase and originating from our frontend.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3837080777"
      grafana_variables: environment,stage
      runbook: docs/webservice/README.md#alerts
      title: The graphql_query SLI of the webservice service has an apdex violating
        SLO
    expr: |
      (
        (
          gitlab_component_apdex:confidence:ratio_1h{component="graphql_query",confidence="98%",type="webservice"}
          < (1 - 14.4 * 0.005000)
        )
        and on (type,component)
        (
          gitlab_component_apdex:confidence:ratio_5m{component="graphql_query",confidence="98%",type="webservice"}
          < (1 - 14.4 * 0.005000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="graphql_query",type="webservice"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: WebserviceServiceGraphqlQueryApdexSLOViolation
    annotations:
      description: |
        A GraphQL query is executed in the context of a request. An error does not always result in a 5xx error. But could contain errors in the response. Mutliple queries could be batched inside a single request.

        This SLI counts all operations, a succeeded operation does not contain errors in it's response or return a 500 error.

        The number of GraphQL queries meeting their duration target based on the urgency of the endpoint. By default, a query should take no more than 1s. We're working on making the urgency customizable in [this epic](https://gitlab.com/groups/gitlab-org/-/epics/5841).

        We're only taking known operations into account. Known operations are queries defined in our codebase and originating from our frontend.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3837080777"
      grafana_variables: environment,stage
      runbook: docs/webservice/README.md#alerts
      title: The graphql_query SLI of the webservice service has an apdex violating
        SLO
    expr: |
      (
        (
          gitlab_component_apdex:confidence:ratio_6h{component="graphql_query",confidence="98%",type="webservice"}
          < (1 - 6 * 0.005000)
        )
        and on (type,component)
        (
          gitlab_component_apdex:confidence:ratio_30m{component="graphql_query",confidence="98%",type="webservice"}
          < (1 - 6 * 0.005000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="graphql_query",type="webservice"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: WebserviceServiceGraphqlQueryErrorSLOViolation
    annotations:
      description: |
        A GraphQL query is executed in the context of a request. An error does not always result in a 5xx error. But could contain errors in the response. Mutliple queries could be batched inside a single request.

        This SLI counts all operations, a succeeded operation does not contain errors in it's response or return a 500 error.

        The number of GraphQL queries meeting their duration target based on the urgency of the endpoint. By default, a query should take no more than 1s. We're working on making the urgency customizable in [this epic](https://gitlab.com/groups/gitlab-org/-/epics/5841).

        We're only taking known operations into account. Known operations are queries defined in our codebase and originating from our frontend.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1966600567"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitlab_sli_graphql_query_error_total{endpoint_id!="graphql:unknown",job="gitlab-rails"}[5m])
        )
      runbook: docs/webservice/README.md#alerts
      title: The graphql_query SLI of the webservice service has an error rate violating
        SLO
    expr: |
      (
        (
          gitlab_component_errors:confidence:ratio_1h{component="graphql_query",confidence="98%",type="webservice"}
          > (14.4 * 0.001000)
        )
        and on (type,component)
        (
          gitlab_component_errors:confidence:ratio_5m{component="graphql_query",confidence="98%",type="webservice"}
          > (14.4 * 0.001000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="graphql_query",type="webservice"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: WebserviceServiceGraphqlQueryErrorSLOViolation
    annotations:
      description: |
        A GraphQL query is executed in the context of a request. An error does not always result in a 5xx error. But could contain errors in the response. Mutliple queries could be batched inside a single request.

        This SLI counts all operations, a succeeded operation does not contain errors in it's response or return a 500 error.

        The number of GraphQL queries meeting their duration target based on the urgency of the endpoint. By default, a query should take no more than 1s. We're working on making the urgency customizable in [this epic](https://gitlab.com/groups/gitlab-org/-/epics/5841).

        We're only taking known operations into account. Known operations are queries defined in our codebase and originating from our frontend.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1966600567"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitlab_sli_graphql_query_error_total{endpoint_id!="graphql:unknown",job="gitlab-rails"}[5m])
        )
      runbook: docs/webservice/README.md#alerts
      title: The graphql_query SLI of the webservice service has an error rate violating
        SLO
    expr: |
      (
        (
          gitlab_component_errors:confidence:ratio_6h{component="graphql_query",confidence="98%",type="webservice"}
          > (6 * 0.001000)
        )
        and on (type,component)
        (
          gitlab_component_errors:confidence:ratio_30m{component="graphql_query",confidence="98%",type="webservice"}
          > (6 * 0.001000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="graphql_query",type="webservice"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: WebserviceServiceGraphqlQueryTrafficCessation
    annotations:
      description: |
        A GraphQL query is executed in the context of a request. An error does not always result in a 5xx error. But could contain errors in the response. Mutliple queries could be batched inside a single request.

        This SLI counts all operations, a succeeded operation does not contain errors in it's response or return a 500 error.

        The number of GraphQL queries meeting their duration target based on the urgency of the endpoint. By default, a query should take no more than 1s. We're working on making the urgency customizable in [this epic](https://gitlab.com/groups/gitlab-org/-/epics/5841).

        We're only taking known operations into account. Known operations are queries defined in our codebase and originating from our frontend.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1872300215"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitlab_sli_graphql_query_total{endpoint_id!="graphql:unknown",job="gitlab-rails"}[5m])
        )
      runbook: docs/webservice/README.md#alerts
      title: The graphql_query SLI of the webservice service has not received any
        traffic in the past 30m
    expr: |
      gitlab_component_ops:rate_30m{component="graphql_query",type="webservice"} == 0
      and
      gitlab_component_ops:rate_30m{component="graphql_query",type="webservice"} offset 1h >= 0.16666666666666666
    for: 5m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: WebserviceServiceGraphqlQueryTrafficAbsent
    annotations:
      description: |
        A GraphQL query is executed in the context of a request. An error does not always result in a 5xx error. But could contain errors in the response. Mutliple queries could be batched inside a single request.

        This SLI counts all operations, a succeeded operation does not contain errors in it's response or return a 500 error.

        The number of GraphQL queries meeting their duration target based on the urgency of the endpoint. By default, a query should take no more than 1s. We're working on making the urgency customizable in [this epic](https://gitlab.com/groups/gitlab-org/-/epics/5841).

        We're only taking known operations into account. Known operations are queries defined in our codebase and originating from our frontend.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1872300215"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitlab_sli_graphql_query_total{endpoint_id!="graphql:unknown",job="gitlab-rails"}[5m])
        )
      runbook: docs/webservice/README.md#alerts
      title: The graphql_query SLI of the webservice service has not reported any
        traffic in the past 30m
    expr: |
      gitlab_component_ops:rate_5m{component="graphql_query",type="webservice"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="graphql_query",type="webservice"}
    for: 30m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: WebserviceServicePumaApdexSLOViolation
    annotations:
      description: |
        Aggregation of most web requests that pass through the puma to the GitLab rails monolith. Healthchecks are excluded.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "561966508"
      grafana_variables: environment,stage
      runbook: docs/webservice/README.md#alerts
      title: The puma SLI of the webservice service has an apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:confidence:ratio_1h{component="puma",confidence="98%",type="webservice"}
          < (1 - 14.4 * 0.002000)
        )
        and on (type,component)
        (
          gitlab_component_apdex:confidence:ratio_5m{component="puma",confidence="98%",type="webservice"}
          < (1 - 14.4 * 0.002000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="puma",type="webservice"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: WebserviceServicePumaApdexSLOViolation
    annotations:
      description: |
        Aggregation of most web requests that pass through the puma to the GitLab rails monolith. Healthchecks are excluded.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "561966508"
      grafana_variables: environment,stage
      runbook: docs/webservice/README.md#alerts
      title: The puma SLI of the webservice service has an apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:confidence:ratio_6h{component="puma",confidence="98%",type="webservice"}
          < (1 - 6 * 0.002000)
        )
        and on (type,component)
        (
          gitlab_component_apdex:confidence:ratio_30m{component="puma",confidence="98%",type="webservice"}
          < (1 - 6 * 0.002000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="puma",type="webservice"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: WebserviceServicePumaErrorSLOViolation
    annotations:
      description: |
        Aggregation of most web requests that pass through the puma to the GitLab rails monolith. Healthchecks are excluded.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2425637513"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(http_requests_total{job="gitlab-rails",status=~"5.."}[5m])
        )
      runbook: docs/webservice/README.md#alerts
      title: The puma SLI of the webservice service has an error rate violating SLO
    expr: |
      (
        (
          gitlab_component_errors:confidence:ratio_1h{component="puma",confidence="98%",type="webservice"}
          > (14.4 * 0.001000)
        )
        and on (type,component)
        (
          gitlab_component_errors:confidence:ratio_5m{component="puma",confidence="98%",type="webservice"}
          > (14.4 * 0.001000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="puma",type="webservice"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: WebserviceServicePumaErrorSLOViolation
    annotations:
      description: |
        Aggregation of most web requests that pass through the puma to the GitLab rails monolith. Healthchecks are excluded.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2425637513"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(http_requests_total{job="gitlab-rails",status=~"5.."}[5m])
        )
      runbook: docs/webservice/README.md#alerts
      title: The puma SLI of the webservice service has an error rate violating SLO
    expr: |
      (
        (
          gitlab_component_errors:confidence:ratio_6h{component="puma",confidence="98%",type="webservice"}
          > (6 * 0.001000)
        )
        and on (type,component)
        (
          gitlab_component_errors:confidence:ratio_30m{component="puma",confidence="98%",type="webservice"}
          > (6 * 0.001000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="puma",type="webservice"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: WebserviceServicePumaTrafficCessation
    annotations:
      description: |
        Aggregation of most web requests that pass through the puma to the GitLab rails monolith. Healthchecks are excluded.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "110019945"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(http_requests_total{job="gitlab-rails"}[5m])
        )
      runbook: docs/webservice/README.md#alerts
      title: The puma SLI of the webservice service has not received any traffic in
        the past 30m
    expr: |
      gitlab_component_ops:rate_30m{component="puma",type="webservice"} == 0
      and
      gitlab_component_ops:rate_30m{component="puma",type="webservice"} offset 1h >= 0.16666666666666666
    for: 5m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: WebserviceServicePumaTrafficAbsent
    annotations:
      description: |
        Aggregation of most web requests that pass through the puma to the GitLab rails monolith. Healthchecks are excluded.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "110019945"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(http_requests_total{job="gitlab-rails"}[5m])
        )
      runbook: docs/webservice/README.md#alerts
      title: The puma SLI of the webservice service has not reported any traffic in
        the past 30m
    expr: |
      gitlab_component_ops:rate_5m{component="puma",type="webservice"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="puma",type="webservice"}
    for: 30m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: WebserviceServiceWorkhorseApdexSLOViolation
    annotations:
      description: |
        Aggregation of most rails requests that pass through workhorse, monitored via the HTTP interface. Excludes API requests, git requests, health, readiness and liveness requests. Some known slow requests, such as HTTP uploads, are excluded from the apdex score.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2586818196"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(gitlab_workhorse_http_request_duration_seconds_bucket{code!~"5..",route_id!="health",route_id!="liveness",route_id!="api_graphql",route_id!="project_uploads",route_id!="action_cable",route_id!~"^git_info_refs$|^git_lfs_objects$|^git_receive_pack$|^git_upload_pack$",route_id!~"^api.*"}[5m])
          )
        )
      runbook: docs/webservice/README.md#alerts
      title: The workhorse SLI of the webservice service has an apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:confidence:ratio_1h{component="workhorse",confidence="98%",type="webservice"}
          < (1 - 14.4 * 0.002000)
        )
        and on (type,component)
        (
          gitlab_component_apdex:confidence:ratio_5m{component="workhorse",confidence="98%",type="webservice"}
          < (1 - 14.4 * 0.002000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="workhorse",type="webservice"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: WebserviceServiceWorkhorseApdexSLOViolation
    annotations:
      description: |
        Aggregation of most rails requests that pass through workhorse, monitored via the HTTP interface. Excludes API requests, git requests, health, readiness and liveness requests. Some known slow requests, such as HTTP uploads, are excluded from the apdex score.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2586818196"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(gitlab_workhorse_http_request_duration_seconds_bucket{code!~"5..",route_id!="health",route_id!="liveness",route_id!="api_graphql",route_id!="project_uploads",route_id!="action_cable",route_id!~"^git_info_refs$|^git_lfs_objects$|^git_receive_pack$|^git_upload_pack$",route_id!~"^api.*"}[5m])
          )
        )
      runbook: docs/webservice/README.md#alerts
      title: The workhorse SLI of the webservice service has an apdex violating SLO
    expr: |
      (
        (
          gitlab_component_apdex:confidence:ratio_6h{component="workhorse",confidence="98%",type="webservice"}
          < (1 - 6 * 0.002000)
        )
        and on (type,component)
        (
          gitlab_component_apdex:confidence:ratio_30m{component="workhorse",confidence="98%",type="webservice"}
          < (1 - 6 * 0.002000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="workhorse",type="webservice"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: WebserviceServiceWorkhorseErrorSLOViolation
    annotations:
      description: |
        Aggregation of most rails requests that pass through workhorse, monitored via the HTTP interface. Excludes API requests, git requests, health, readiness and liveness requests. Some known slow requests, such as HTTP uploads, are excluded from the apdex score.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2047984749"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitlab_workhorse_http_requests_total{code=~"^5.*",route_id!="health",route_id!="liveness",route_id!="api_graphql",route_id!~"^git_info_refs$|^git_lfs_objects$|^git_receive_pack$|^git_upload_pack$",route_id!~"^api.*"}[5m])
        )
      runbook: docs/webservice/README.md#alerts
      title: The workhorse SLI of the webservice service has an error rate violating
        SLO
    expr: |
      (
        (
          gitlab_component_errors:confidence:ratio_1h{component="workhorse",confidence="98%",type="webservice"}
          > (14.4 * 0.001000)
        )
        and on (type,component)
        (
          gitlab_component_errors:confidence:ratio_5m{component="workhorse",confidence="98%",type="webservice"}
          > (14.4 * 0.001000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="workhorse",type="webservice"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: WebserviceServiceWorkhorseErrorSLOViolation
    annotations:
      description: |
        Aggregation of most rails requests that pass through workhorse, monitored via the HTTP interface. Excludes API requests, git requests, health, readiness and liveness requests. Some known slow requests, such as HTTP uploads, are excluded from the apdex score.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2047984749"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitlab_workhorse_http_requests_total{code=~"^5.*",route_id!="health",route_id!="liveness",route_id!="api_graphql",route_id!~"^git_info_refs$|^git_lfs_objects$|^git_receive_pack$|^git_upload_pack$",route_id!~"^api.*"}[5m])
        )
      runbook: docs/webservice/README.md#alerts
      title: The workhorse SLI of the webservice service has an error rate violating
        SLO
    expr: |
      (
        (
          gitlab_component_errors:confidence:ratio_6h{component="workhorse",confidence="98%",type="webservice"}
          > (6 * 0.001000)
        )
        and on (type,component)
        (
          gitlab_component_errors:confidence:ratio_30m{component="workhorse",confidence="98%",type="webservice"}
          > (6 * 0.001000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="workhorse",type="webservice"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: WebserviceServiceWorkhorseTrafficCessation
    annotations:
      description: |
        Aggregation of most rails requests that pass through workhorse, monitored via the HTTP interface. Excludes API requests, git requests, health, readiness and liveness requests. Some known slow requests, such as HTTP uploads, are excluded from the apdex score.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "831392945"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitlab_workhorse_http_requests_total{route_id!="health",route_id!="liveness",route_id!="api_graphql",route_id!~"^git_info_refs$|^git_lfs_objects$|^git_receive_pack$|^git_upload_pack$",route_id!~"^api.*"}[5m])
        )
      runbook: docs/webservice/README.md#alerts
      title: The workhorse SLI of the webservice service has not received any traffic
        in the past 30m
    expr: |
      gitlab_component_ops:rate_30m{component="workhorse",type="webservice"} == 0
      and
      gitlab_component_ops:rate_30m{component="workhorse",type="webservice"} offset 1h >= 0.16666666666666666
    for: 5m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: WebserviceServiceWorkhorseTrafficAbsent
    annotations:
      description: |
        Aggregation of most rails requests that pass through workhorse, monitored via the HTTP interface. Excludes API requests, git requests, health, readiness and liveness requests. Some known slow requests, such as HTTP uploads, are excluded from the apdex score.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "831392945"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitlab_workhorse_http_requests_total{route_id!="health",route_id!="liveness",route_id!="api_graphql",route_id!~"^git_info_refs$|^git_lfs_objects$|^git_receive_pack$|^git_upload_pack$",route_id!~"^api.*"}[5m])
        )
      runbook: docs/webservice/README.md#alerts
      title: The workhorse SLI of the webservice service has not reported any traffic
        in the past 30m
    expr: |
      gitlab_component_ops:rate_5m{component="workhorse",type="webservice"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="workhorse",type="webservice"}
    for: 30m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: WebserviceServiceWorkhorseApiApdexSLOViolation
    annotations:
      description: |
        Aggregation of most API requests that pass through workhorse, monitored via the HTTP interface.

        The workhorse API apdex has a longer apdex latency than the web to allow for slow API requests.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1061500963"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(gitlab_workhorse_http_request_duration_seconds_bucket{code!~"5..",route_id!="health",route_id!="liveness",route_id!="api_graphql",route_id=~"^api.*"}[5m])
          )
        )
      runbook: docs/webservice/README.md#alerts
      title: The workhorse_api SLI of the webservice service has an apdex violating
        SLO
    expr: |
      (
        (
          gitlab_component_apdex:confidence:ratio_1h{component="workhorse_api",confidence="98%",type="webservice"}
          < (1 - 14.4 * 0.002000)
        )
        and on (type,component)
        (
          gitlab_component_apdex:confidence:ratio_5m{component="workhorse_api",confidence="98%",type="webservice"}
          < (1 - 14.4 * 0.002000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="workhorse_api",type="webservice"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: WebserviceServiceWorkhorseApiApdexSLOViolation
    annotations:
      description: |
        Aggregation of most API requests that pass through workhorse, monitored via the HTTP interface.

        The workhorse API apdex has a longer apdex latency than the web to allow for slow API requests.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1061500963"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(gitlab_workhorse_http_request_duration_seconds_bucket{code!~"5..",route_id!="health",route_id!="liveness",route_id!="api_graphql",route_id=~"^api.*"}[5m])
          )
        )
      runbook: docs/webservice/README.md#alerts
      title: The workhorse_api SLI of the webservice service has an apdex violating
        SLO
    expr: |
      (
        (
          gitlab_component_apdex:confidence:ratio_6h{component="workhorse_api",confidence="98%",type="webservice"}
          < (1 - 6 * 0.002000)
        )
        and on (type,component)
        (
          gitlab_component_apdex:confidence:ratio_30m{component="workhorse_api",confidence="98%",type="webservice"}
          < (1 - 6 * 0.002000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="workhorse_api",type="webservice"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: WebserviceServiceWorkhorseApiErrorSLOViolation
    annotations:
      description: |
        Aggregation of most API requests that pass through workhorse, monitored via the HTTP interface.

        The workhorse API apdex has a longer apdex latency than the web to allow for slow API requests.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1957610638"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitlab_workhorse_http_requests_total{code=~"^5.*",route_id!="health",route_id!="liveness",route_id!="api_graphql",route_id=~"^api.*"}[5m])
        )
      runbook: docs/webservice/README.md#alerts
      title: The workhorse_api SLI of the webservice service has an error rate violating
        SLO
    expr: |
      (
        (
          gitlab_component_errors:confidence:ratio_1h{component="workhorse_api",confidence="98%",type="webservice"}
          > (14.4 * 0.001000)
        )
        and on (type,component)
        (
          gitlab_component_errors:confidence:ratio_5m{component="workhorse_api",confidence="98%",type="webservice"}
          > (14.4 * 0.001000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="workhorse_api",type="webservice"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: WebserviceServiceWorkhorseApiErrorSLOViolation
    annotations:
      description: |
        Aggregation of most API requests that pass through workhorse, monitored via the HTTP interface.

        The workhorse API apdex has a longer apdex latency than the web to allow for slow API requests.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1957610638"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitlab_workhorse_http_requests_total{code=~"^5.*",route_id!="health",route_id!="liveness",route_id!="api_graphql",route_id=~"^api.*"}[5m])
        )
      runbook: docs/webservice/README.md#alerts
      title: The workhorse_api SLI of the webservice service has an error rate violating
        SLO
    expr: |
      (
        (
          gitlab_component_errors:confidence:ratio_6h{component="workhorse_api",confidence="98%",type="webservice"}
          > (6 * 0.001000)
        )
        and on (type,component)
        (
          gitlab_component_errors:confidence:ratio_30m{component="workhorse_api",confidence="98%",type="webservice"}
          > (6 * 0.001000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="workhorse_api",type="webservice"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: WebserviceServiceWorkhorseApiTrafficCessation
    annotations:
      description: |
        Aggregation of most API requests that pass through workhorse, monitored via the HTTP interface.

        The workhorse API apdex has a longer apdex latency than the web to allow for slow API requests.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3800674929"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitlab_workhorse_http_requests_total{route_id!="health",route_id!="liveness",route_id!="api_graphql",route_id=~"^api.*"}[5m])
        )
      runbook: docs/webservice/README.md#alerts
      title: The workhorse_api SLI of the webservice service has not received any
        traffic in the past 30m
    expr: |
      gitlab_component_ops:rate_30m{component="workhorse_api",type="webservice"} == 0
      and
      gitlab_component_ops:rate_30m{component="workhorse_api",type="webservice"} offset 1h >= 0.16666666666666666
    for: 5m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: WebserviceServiceWorkhorseApiTrafficAbsent
    annotations:
      description: |
        Aggregation of most API requests that pass through workhorse, monitored via the HTTP interface.

        The workhorse API apdex has a longer apdex latency than the web to allow for slow API requests.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3800674929"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitlab_workhorse_http_requests_total{route_id!="health",route_id!="liveness",route_id!="api_graphql",route_id=~"^api.*"}[5m])
        )
      runbook: docs/webservice/README.md#alerts
      title: The workhorse_api SLI of the webservice service has not reported any
        traffic in the past 30m
    expr: |
      gitlab_component_ops:rate_5m{component="workhorse_api",type="webservice"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="workhorse_api",type="webservice"}
    for: 30m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: WebserviceServiceWorkhorseGitApdexSLOViolation
    annotations:
      description: |
        Aggregation of git+https requests that pass through workhorse, monitored via the HTTP interface.

        For apdex score, we avoid potentially long running requests, so only use the info-refs endpoint for monitoring git+https performance.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3264851214"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(gitlab_workhorse_http_request_duration_seconds_bucket{code!~"5..",route_id="git_info_refs"}[5m])
          )
        )
      runbook: docs/webservice/README.md#alerts
      title: The workhorse_git SLI of the webservice service has an apdex violating
        SLO
    expr: |
      (
        (
          gitlab_component_apdex:confidence:ratio_1h{component="workhorse_git",confidence="98%",type="webservice"}
          < (1 - 14.4 * 0.002000)
        )
        and on (type,component)
        (
          gitlab_component_apdex:confidence:ratio_5m{component="workhorse_git",confidence="98%",type="webservice"}
          < (1 - 14.4 * 0.002000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="workhorse_git",type="webservice"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: WebserviceServiceWorkhorseGitApdexSLOViolation
    annotations:
      description: |
        Aggregation of git+https requests that pass through workhorse, monitored via the HTTP interface.

        For apdex score, we avoid potentially long running requests, so only use the info-refs endpoint for monitoring git+https performance.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3264851214"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (le) (
            rate(gitlab_workhorse_http_request_duration_seconds_bucket{code!~"5..",route_id="git_info_refs"}[5m])
          )
        )
      runbook: docs/webservice/README.md#alerts
      title: The workhorse_git SLI of the webservice service has an apdex violating
        SLO
    expr: |
      (
        (
          gitlab_component_apdex:confidence:ratio_6h{component="workhorse_git",confidence="98%",type="webservice"}
          < (1 - 6 * 0.002000)
        )
        and on (type,component)
        (
          gitlab_component_apdex:confidence:ratio_30m{component="workhorse_git",confidence="98%",type="webservice"}
          < (1 - 6 * 0.002000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="workhorse_git",type="webservice"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: WebserviceServiceWorkhorseGitErrorSLOViolation
    annotations:
      description: |
        Aggregation of git+https requests that pass through workhorse, monitored via the HTTP interface.

        For apdex score, we avoid potentially long running requests, so only use the info-refs endpoint for monitoring git+https performance.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "769712142"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitlab_workhorse_http_requests_total{code=~"^5.*",route_id=~"^git_info_refs$|^git_lfs_objects$|^git_receive_pack$|^git_upload_pack$"}[5m])
        )
      runbook: docs/webservice/README.md#alerts
      title: The workhorse_git SLI of the webservice service has an error rate violating
        SLO
    expr: |
      (
        (
          gitlab_component_errors:confidence:ratio_1h{component="workhorse_git",confidence="98%",type="webservice"}
          > (14.4 * 0.001000)
        )
        and on (type,component)
        (
          gitlab_component_errors:confidence:ratio_5m{component="workhorse_git",confidence="98%",type="webservice"}
          > (14.4 * 0.001000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_1h{component="workhorse_git",type="webservice"}) >= 1
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
  - alert: WebserviceServiceWorkhorseGitErrorSLOViolation
    annotations:
      description: |
        Aggregation of git+https requests that pass through workhorse, monitored via the HTTP interface.

        For apdex score, we avoid potentially long running requests, so only use the info-refs endpoint for monitoring git+https performance.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "769712142"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitlab_workhorse_http_requests_total{code=~"^5.*",route_id=~"^git_info_refs$|^git_lfs_objects$|^git_receive_pack$|^git_upload_pack$"}[5m])
        )
      runbook: docs/webservice/README.md#alerts
      title: The workhorse_git SLI of the webservice service has an error rate violating
        SLO
    expr: |
      (
        (
          gitlab_component_errors:confidence:ratio_6h{component="workhorse_git",confidence="98%",type="webservice"}
          > (6 * 0.001000)
        )
        and on (type,component)
        (
          gitlab_component_errors:confidence:ratio_30m{component="workhorse_git",confidence="98%",type="webservice"}
          > (6 * 0.001000)
        )
      )
      and on(type,component)
      (
        sum by(type,component) (gitlab_component_ops:rate_6h{component="workhorse_git",type="webservice"}) >= 0.16667
      )
    for: 2m
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      confidence: 98%
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
  - alert: WebserviceServiceWorkhorseGitTrafficCessation
    annotations:
      description: |
        Aggregation of git+https requests that pass through workhorse, monitored via the HTTP interface.

        For apdex score, we avoid potentially long running requests, so only use the info-refs endpoint for monitoring git+https performance.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3915518396"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitlab_workhorse_http_requests_total{route_id=~"^git_info_refs$|^git_lfs_objects$|^git_receive_pack$|^git_upload_pack$"}[5m])
        )
      runbook: docs/webservice/README.md#alerts
      title: The workhorse_git SLI of the webservice service has not received any
        traffic in the past 30m
    expr: |
      gitlab_component_ops:rate_30m{component="workhorse_git",type="webservice"} == 0
      and
      gitlab_component_ops:rate_30m{component="workhorse_git",type="webservice"} offset 1h >= 0.16666666666666666
    for: 5m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
  - alert: WebserviceServiceWorkhorseGitTrafficAbsent
    annotations:
      description: |
        Aggregation of git+https requests that pass through workhorse, monitored via the HTTP interface.

        For apdex score, we avoid potentially long running requests, so only use the info-refs endpoint for monitoring git+https performance.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: webservice-main/webservice-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/webservice-main/webservice-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3915518396"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by () (
          rate(gitlab_workhorse_http_requests_total{route_id=~"^git_info_refs$|^git_lfs_objects$|^git_receive_pack$|^git_upload_pack$"}[5m])
        )
      runbook: docs/webservice/README.md#alerts
      title: The workhorse_git SLI of the webservice service has not reported any
        traffic in the past 30m
    expr: |
      gitlab_component_ops:rate_5m{component="workhorse_git",type="webservice"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="workhorse_git",type="webservice"}
    for: 30m
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
