groups:
- name: gitlab-com-ci.rules
  rules:
  - alert: CICDWorkhorseQueuingUnderperformant
    expr: |
      histogram_quantile(
        0.90,
        sum by(environment, tier, type, stage, shard, le) (
          rate(gitlab_workhorse_queueing_waiting_time_bucket{queue_name="ci_api_job_requests",job="gitlab-workhorse"}[5m])
        )
      ) >= 30
    for: 5m
    labels:
      team: runner
      severity: s3
      alert_type: cause
    annotations:
      title: '90% of request queued on Workhorse is longer than 30s'
      description: |
        90% of requests queued on Workhorse are longer than 30s for last 5 minutes.

        This should be considered as service degradation and the reason should be investigated.
      runbook: docs/ci-runners/ci_workhorse-queuing.md

  - alert: CICDTooManyArchivingTraceFailures
    expr: |
      sum by (environment, tier, type, stage, shard) (
        rate(job_trace_archive_failed_total[5m])
      ) > 10
    for: 5m
    labels:
      team: runner
      severity: s4
      alert_type: cause
    annotations:
      title: 'Too big number of traces archiving failures: {{$value}}'
      description: |
        Traces archiving keeps failing for more than 5 minutes.
        Plese check https://dashboards.gitlab.net/d/000000159/ci?&orgId=1&panelId=153&fullscreen,
        https://new-sentry.gitlab.net/organizations/gitlab/issues/?project=3&query=ArchiveTraceWorker, and
        https://new-sentry.gitlab.net/organizations/gitlab/issues/?project=3&query=ArchiveTracesCronWorker to find out more details
      runbook: docs/ci-runners/ci_too_many_archiving_trace_failures.md

  - alert: CICDJobsStuckInDockerPull
    expr: |
      (
        sum by (environment, tier, type, stage, shard) (gitlab_runner_jobs{executor_stage="docker_pulling_image"})
        /
        sum by (environment, tier, type, stage, shard) (gitlab_runner_jobs{executor_stage="docker_run"})
      ) > 1
    for: 5m
    labels:
      team: runner
      severity: s4
      alert_type: cause
    annotations:
      title: 'More CI Jobs are in state docker_pulling_image than state docker_run'
      description: |
        More CI Jobs are in state docker_pulling_image than are in state docker_run
        for over 5 minutes. This could mean connectivity to docker hub is broken.
        Plese check https://dashboards.gitlab.net/d/000000159/ci?viewPanel=46&orgId=1&from=now-1h&to=now&fullscreen
  - alert: FleetingPendingInstancesTooHigh
    expr: |
      (
        sum by(environment, tier, type, stage, shard) (
          fleeting_provisioner_instances{state="pending"}
        ) > 0
        and
        sum by(environment, tier, type, stage, shard) (
          fleeting_provisioner_instances{state="pending"}
          -
          fleeting_provisioner_instances{state="pending"} offset 15m
        ) >= 0
      )
    for: 15m
    labels:
      team: runner
      severity: s3
      alert_type: cause
    annotations:
      title: 'Number of Fleeting pending instances is constantly growing'
      description: |
        Constantly growing number of Fleeting's pending instances usually means that something
        odd happens with our autoscaling. We've identified (and as for now didn't found the root
        cause) that occasionally Runner asks autoscaling groups to increase the number of instances,
        but these requests are never recognized by the ASGs. Runner, however, awaits these instances
        which at some moment brings it to clog in the scaling mechanism.
        Please check https://dashboards.gitlab.net/d/ci-runners-incident-autoscaling-new/ci-runners-incident-support-autoscaling-new?viewPanel=17&orgId=1&from=now-1h&to=now&fullscrean
