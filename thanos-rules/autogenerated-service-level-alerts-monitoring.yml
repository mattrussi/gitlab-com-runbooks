# WARNING. DO NOT EDIT THIS FILE BY HAND. USE ./thanos-rules-jsonnet/service-component-alerts.jsonnet TO GENERATE IT
# YOUR CHANGES WILL BE OVERRIDDEN
groups:
- name: 'Service Component Alerts: monitoring'
  interval: 1m
  partial_response_strategy: warn
  rules:
  - alert: MonitoringServiceGrafanaApdexSLOViolation
    for: 2m
    annotations:
      title: The grafana SLI of the monitoring service (`{{ $labels.stage }}` stage)
        has an apdex violating SLO
      description: |
        Grafana builds and displays dashboards querying Thanos, Elasticsearch and other datasources. This SLI monitors the Grafana HTTP interface.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1543481783"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (env,environment,tier,stage,le) (
            rate(grafana_http_request_duration_seconds_bucket{environment="{{ $labels.environment }}",job="grafana",shard="default",stage="{{ $labels.stage }}",type="monitoring"}[5m])
          )
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_1h{component="grafana",monitor="global",type="monitoring"}
          < (1 - 14.4 * 0.080000)
        )
        and
        (
          gitlab_component_apdex:ratio_5m{component="grafana",monitor="global",type="monitoring"}
          < (1 - 14.4 * 0.080000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="grafana",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceGrafanaApdexSLOViolation
    for: 2m
    annotations:
      title: The grafana SLI of the monitoring service (`{{ $labels.stage }}` stage)
        has an apdex violating SLO
      description: |
        Grafana builds and displays dashboards querying Thanos, Elasticsearch and other datasources. This SLI monitors the Grafana HTTP interface.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1543481783"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (env,environment,tier,stage,le) (
            rate(grafana_http_request_duration_seconds_bucket{environment="{{ $labels.environment }}",job="grafana",shard="default",stage="{{ $labels.stage }}",type="monitoring"}[5m])
          )
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_6h{component="grafana",monitor="global",type="monitoring"}
          < (1 - 6 * 0.080000)
        )
        and
        (
          gitlab_component_apdex:ratio_30m{component="grafana",monitor="global",type="monitoring"}
          < (1 - 6 * 0.080000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="grafana",monitor="global",type="monitoring"}) >= 0.16667
      )
  - alert: MonitoringServiceGrafanaErrorSLOViolation
    for: 2m
    annotations:
      title: The grafana SLI of the monitoring service (`{{ $labels.stage }}` stage)
        has an error rate violating SLO
      description: |
        Grafana builds and displays dashboards querying Thanos, Elasticsearch and other datasources. This SLI monitors the Grafana HTTP interface.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3079737102"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(grafana_http_request_duration_seconds_bucket{code=~"^5.*",environment="{{ $labels.environment }}",job="grafana",le="+Inf",shard="default",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="grafana",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_5m{component="grafana",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="grafana",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceGrafanaErrorSLOViolation
    for: 2m
    annotations:
      title: The grafana SLI of the monitoring service (`{{ $labels.stage }}` stage)
        has an error rate violating SLO
      description: |
        Grafana builds and displays dashboards querying Thanos, Elasticsearch and other datasources. This SLI monitors the Grafana HTTP interface.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3079737102"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(grafana_http_request_duration_seconds_bucket{code=~"^5.*",environment="{{ $labels.environment }}",job="grafana",le="+Inf",shard="default",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="grafana",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_30m{component="grafana",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="grafana",monitor="global",type="monitoring"}) >= 0.16667
      )
  - alert: MonitoringServiceGrafanaDatasourcesErrorSLOViolation
    for: 2m
    annotations:
      title: The grafana_datasources SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        Grafana builds and displays dashboards querying Thanos, Elasticsearch and other datasources. This SLI monitors the requests from Grafana to its datasources.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "402312104"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(grafana_datasource_request_total{code=~"^5.*",environment="{{ $labels.environment }}",job="grafana",shard="default",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="grafana_datasources",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_5m{component="grafana_datasources",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="grafana_datasources",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceGrafanaDatasourcesErrorSLOViolation
    for: 2m
    annotations:
      title: The grafana_datasources SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        Grafana builds and displays dashboards querying Thanos, Elasticsearch and other datasources. This SLI monitors the requests from Grafana to its datasources.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "402312104"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(grafana_datasource_request_total{code=~"^5.*",environment="{{ $labels.environment }}",job="grafana",shard="default",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="grafana_datasources",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_30m{component="grafana_datasources",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="grafana_datasources",monitor="global",type="monitoring"}) >= 0.16667
      )
  - alert: MonitoringServiceGrafanaGoogleLbErrorSLOViolation
    for: 2m
    annotations:
      title: The grafana_google_lb SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "665917726"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(stackdriver_https_lb_rule_loadbalancing_googleapis_com_https_request_count{environment="{{ $labels.environment }}",project_id="gitlab-ops",response_code_class="500",stage="{{ $labels.stage }}",url_map_name="k8s2-um-4zodnh0s-grafana-grafana-cfagrqyu"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="grafana_google_lb",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_5m{component="grafana_google_lb",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="grafana_google_lb",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceGrafanaGoogleLbErrorSLOViolation
    for: 2m
    annotations:
      title: The grafana_google_lb SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "665917726"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(stackdriver_https_lb_rule_loadbalancing_googleapis_com_https_request_count{environment="{{ $labels.environment }}",project_id="gitlab-ops",response_code_class="500",stage="{{ $labels.stage }}",url_map_name="k8s2-um-4zodnh0s-grafana-grafana-cfagrqyu"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      rules_domain: general
      severity: s3
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="grafana_google_lb",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_30m{component="grafana_google_lb",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="grafana_google_lb",monitor="global",type="monitoring"}) >= 0.16667
      )
  - alert: MonitoringServiceGrafanaImageRendererApdexSLOViolation
    for: 2m
    annotations:
      title: The grafana_image_renderer SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an apdex violating SLO
      description: |
        The Grafana Image Renderer exports Grafana dashboards or panels to PNG for external use. This SLI monitors the Grafana Image Renderer HTTP interface.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3617039935"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (env,environment,tier,stage,le) (
            rate(grafana_image_renderer_service_http_request_duration_seconds_bucket{environment="{{ $labels.environment }}",job="grafana-image-renderer",shard="default",stage="{{ $labels.stage }}",type="monitoring"}[5m])
          )
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_1h{component="grafana_image_renderer",monitor="global",type="monitoring"}
          < (1 - 14.4 * 0.080000)
        )
        and
        (
          gitlab_component_apdex:ratio_5m{component="grafana_image_renderer",monitor="global",type="monitoring"}
          < (1 - 14.4 * 0.080000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="grafana_image_renderer",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceGrafanaImageRendererApdexSLOViolation
    for: 2m
    annotations:
      title: The grafana_image_renderer SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an apdex violating SLO
      description: |
        The Grafana Image Renderer exports Grafana dashboards or panels to PNG for external use. This SLI monitors the Grafana Image Renderer HTTP interface.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3617039935"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (env,environment,tier,stage,le) (
            rate(grafana_image_renderer_service_http_request_duration_seconds_bucket{environment="{{ $labels.environment }}",job="grafana-image-renderer",shard="default",stage="{{ $labels.stage }}",type="monitoring"}[5m])
          )
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_6h{component="grafana_image_renderer",monitor="global",type="monitoring"}
          < (1 - 6 * 0.080000)
        )
        and
        (
          gitlab_component_apdex:ratio_30m{component="grafana_image_renderer",monitor="global",type="monitoring"}
          < (1 - 6 * 0.080000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="grafana_image_renderer",monitor="global",type="monitoring"}) >= 0.16667
      )
  - alert: MonitoringServiceGrafanaImageRendererErrorSLOViolation
    for: 2m
    annotations:
      title: The grafana_image_renderer SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        The Grafana Image Renderer exports Grafana dashboards or panels to PNG for external use. This SLI monitors the Grafana Image Renderer HTTP interface.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1209709030"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(grafana_image_renderer_service_http_request_duration_seconds_bucket{environment="{{ $labels.environment }}",job="grafana-image-renderer",le="+Inf",shard="default",stage="{{ $labels.stage }}",status_code=~"^5.*",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 1h
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="grafana_image_renderer",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_5m{component="grafana_image_renderer",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="grafana_image_renderer",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceGrafanaImageRendererErrorSLOViolation
    for: 2m
    annotations:
      title: The grafana_image_renderer SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        The Grafana Image Renderer exports Grafana dashboards or panels to PNG for external use. This SLI monitors the Grafana Image Renderer HTTP interface.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "1209709030"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(grafana_image_renderer_service_http_request_duration_seconds_bucket{environment="{{ $labels.environment }}",job="grafana-image-renderer",le="+Inf",shard="default",stage="{{ $labels.stage }}",status_code=~"^5.*",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "no"
      window: 6h
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="grafana_image_renderer",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_30m{component="grafana_image_renderer",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="grafana_image_renderer",monitor="global",type="monitoring"}) >= 0.16667
      )
  - alert: MonitoringServicePrometheusApdexSLOViolation
    for: 2m
    annotations:
      title: The prometheus SLI of the monitoring service (`{{ $labels.stage }}` stage)
        has an apdex violating SLO
      description: |
        This SLI monitors Prometheus instances via the HTTP interface. 5xx responses are considered errors.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "4146537329"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (env,environment,tier,stage,le) (
            rate(prometheus_http_request_duration_seconds_bucket{environment="{{ $labels.environment }}",job!="prometheus-metamon",job=~"prometheus.*",stage="{{ $labels.stage }}",type="monitoring"}[5m])
          )
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_1h{component="prometheus",monitor="global",type="monitoring"}
          < (1 - 14.4 * 0.001000)
        )
        and
        (
          gitlab_component_apdex:ratio_5m{component="prometheus",monitor="global",type="monitoring"}
          < (1 - 14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="prometheus",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServicePrometheusApdexSLOViolation
    for: 2m
    annotations:
      title: The prometheus SLI of the monitoring service (`{{ $labels.stage }}` stage)
        has an apdex violating SLO
      description: |
        This SLI monitors Prometheus instances via the HTTP interface. 5xx responses are considered errors.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "4146537329"
      grafana_variables: environment,stage
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (env,environment,tier,stage,le) (
            rate(prometheus_http_request_duration_seconds_bucket{environment="{{ $labels.environment }}",job!="prometheus-metamon",job=~"prometheus.*",stage="{{ $labels.stage }}",type="monitoring"}[5m])
          )
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
    expr: |
      (
        (
          gitlab_component_apdex:ratio_6h{component="prometheus",monitor="global",type="monitoring"}
          < (1 - 6 * 0.001000)
        )
        and
        (
          gitlab_component_apdex:ratio_30m{component="prometheus",monitor="global",type="monitoring"}
          < (1 - 6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="prometheus",monitor="global",type="monitoring"}) >= 0.16667
      )
  - alert: MonitoringServicePrometheusErrorSLOViolation
    for: 2m
    annotations:
      title: The prometheus SLI of the monitoring service (`{{ $labels.stage }}` stage)
        has an error rate violating SLO
      description: |
        This SLI monitors Prometheus instances via the HTTP interface. 5xx responses are considered errors.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "190969910"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(prometheus_http_requests_total{code=~"^5.*",environment="{{ $labels.environment }}",job!="prometheus-metamon",job=~"prometheus.*",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="prometheus",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_5m{component="prometheus",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="prometheus",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServicePrometheusErrorSLOViolation
    for: 2m
    annotations:
      title: The prometheus SLI of the monitoring service (`{{ $labels.stage }}` stage)
        has an error rate violating SLO
      description: |
        This SLI monitors Prometheus instances via the HTTP interface. 5xx responses are considered errors.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "190969910"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(prometheus_http_requests_total{code=~"^5.*",environment="{{ $labels.environment }}",job!="prometheus-metamon",job=~"prometheus.*",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="prometheus",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_30m{component="prometheus",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="prometheus",monitor="global",type="monitoring"}) >= 0.16667
      )
  - alert: MonitoringServicePrometheusTrafficCessation
    for: 5m
    annotations:
      title: The prometheus SLI of the monitoring service (`{{ $labels.stage }}` stage)
        has not received any traffic in the past 30m
      description: |
        This SLI monitors Prometheus instances via the HTTP interface. 5xx responses are considered errors.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "443859513"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(prometheus_http_requests_total{environment="{{ $labels.environment }}",job!="prometheus-metamon",job=~"prometheus.*",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
    expr: |
      gitlab_component_ops:rate_30m{component="prometheus",monitor="global",stage="main",type="monitoring"} == 0
      and
      gitlab_component_ops:rate_30m{component="prometheus",monitor="global",stage="main",type="monitoring"} offset 1h >= 0.16666666666666666
  - alert: MonitoringServicePrometheusTrafficAbsent
    for: 30m
    annotations:
      title: The prometheus SLI of the monitoring service (`{{ $labels.stage }}` stage)
        has not reported any traffic in the past 30m
      description: |
        This SLI monitors Prometheus instances via the HTTP interface. 5xx responses are considered errors.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "443859513"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(prometheus_http_requests_total{environment="{{ $labels.environment }}",job!="prometheus-metamon",job=~"prometheus.*",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
    expr: |
      gitlab_component_ops:rate_5m{component="prometheus",monitor="global",stage="main",type="monitoring"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="prometheus",monitor="global",stage="main",type="monitoring"}
  - alert: MonitoringServicePrometheusAlertSenderErrorSLOViolation
    for: 2m
    annotations:
      title: The prometheus_alert_sender SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        This SLI monitors all prometheus alert notifications that are generated by AlertManager. Alert delivery failure is considered a service-level failure.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3098809023"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(prometheus_notifications_errors_total{environment="{{ $labels.environment }}",job="prometheus",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="prometheus_alert_sender",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_5m{component="prometheus_alert_sender",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="prometheus_alert_sender",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServicePrometheusAlertSenderErrorSLOViolation
    for: 2m
    annotations:
      title: The prometheus_alert_sender SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        This SLI monitors all prometheus alert notifications that are generated by AlertManager. Alert delivery failure is considered a service-level failure.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3098809023"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(prometheus_notifications_errors_total{environment="{{ $labels.environment }}",job="prometheus",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="prometheus_alert_sender",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_30m{component="prometheus_alert_sender",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="prometheus_alert_sender",monitor="global",type="monitoring"}) >= 0.16667
      )
  - alert: MonitoringServicePrometheusAlertSenderTrafficCessation
    for: 5m
    annotations:
      title: The prometheus_alert_sender SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has not received any traffic in the past 30m
      description: |
        This SLI monitors all prometheus alert notifications that are generated by AlertManager. Alert delivery failure is considered a service-level failure.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2970883772"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(prometheus_notifications_sent_total{environment="{{ $labels.environment }}",job="prometheus",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
    expr: |
      gitlab_component_ops:rate_30m{component="prometheus_alert_sender",monitor="global",stage="main",type="monitoring"} == 0
      and
      gitlab_component_ops:rate_30m{component="prometheus_alert_sender",monitor="global",stage="main",type="monitoring"} offset 1h >= 0.16666666666666666
  - alert: MonitoringServicePrometheusAlertSenderTrafficAbsent
    for: 30m
    annotations:
      title: The prometheus_alert_sender SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has not reported any traffic in the past 30m
      description: |
        This SLI monitors all prometheus alert notifications that are generated by AlertManager. Alert delivery failure is considered a service-level failure.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2970883772"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(prometheus_notifications_sent_total{environment="{{ $labels.environment }}",job="prometheus",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
    expr: |
      gitlab_component_ops:rate_5m{component="prometheus_alert_sender",monitor="global",stage="main",type="monitoring"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="prometheus_alert_sender",monitor="global",stage="main",type="monitoring"}
  - alert: MonitoringServiceRuleEvaluationErrorSLOViolation
    for: 2m
    annotations:
      title: The rule_evaluation SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        This SLI monitors Prometheus recording rule evaluations. Recording rule evalution failures are considered to be service failures.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3727131931"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(prometheus_rule_evaluation_failures_total{environment="{{ $labels.environment }}",job!~"^thanos.*",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
    expr: |
      (
        (
          gitlab_component_errors:ratio_1h{component="rule_evaluation",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_5m{component="rule_evaluation",monitor="global",type="monitoring"}
          > (14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_1h{component="rule_evaluation",monitor="global",type="monitoring"}) >= 1
      )
  - alert: MonitoringServiceRuleEvaluationErrorSLOViolation
    for: 2m
    annotations:
      title: The rule_evaluation SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has an error rate violating SLO
      description: |
        This SLI monitors Prometheus recording rule evaluations. Recording rule evalution failures are considered to be service failures.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3727131931"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(prometheus_rule_evaluation_failures_total{environment="{{ $labels.environment }}",job!~"^thanos.*",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
    expr: |
      (
        (
          gitlab_component_errors:ratio_6h{component="rule_evaluation",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
        and
        (
          gitlab_component_errors:ratio_30m{component="rule_evaluation",monitor="global",type="monitoring"}
          > (6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,component)
      (
        sum by(env,environment,tier,type,stage,component) (gitlab_component_ops:rate_6h{component="rule_evaluation",monitor="global",type="monitoring"}) >= 0.16667
      )
  - alert: MonitoringServiceRuleEvaluationTrafficCessation
    for: 5m
    annotations:
      title: The rule_evaluation SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has not received any traffic in the past 30m
      description: |
        This SLI monitors Prometheus recording rule evaluations. Recording rule evalution failures are considered to be service failures.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2621394178"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(prometheus_rule_evaluations_total{environment="{{ $labels.environment }}",job!~"^thanos.*",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
    expr: |
      gitlab_component_ops:rate_30m{component="rule_evaluation",monitor="global",stage="main",type="monitoring"} == 0
      and
      gitlab_component_ops:rate_30m{component="rule_evaluation",monitor="global",stage="main",type="monitoring"} offset 1h >= 0.16666666666666666
  - alert: MonitoringServiceRuleEvaluationTrafficAbsent
    for: 30m
    annotations:
      title: The rule_evaluation SLI of the monitoring service (`{{ $labels.stage
        }}` stage) has not reported any traffic in the past 30m
      description: |
        This SLI monitors Prometheus recording rule evaluations. Recording rule evalution failures are considered to be service failures.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.
      grafana_dashboard_id: monitoring-main/monitoring-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/monitoring-main/monitoring-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2621394178"
      grafana_variables: environment,stage
      promql_template_1: |
        sum by (env,environment,tier,stage) (
          rate(prometheus_rule_evaluations_total{environment="{{ $labels.environment }}",job!~"^thanos.*",stage="{{ $labels.stage }}",type="monitoring"}[5m])
        )
      runbook: docs/monitoring/README.md
    labels:
      aggregation: component
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
    expr: |
      gitlab_component_ops:rate_5m{component="rule_evaluation",monitor="global",stage="main",type="monitoring"} offset 1h
      unless
      gitlab_component_ops:rate_5m{component="rule_evaluation",monitor="global",stage="main",type="monitoring"}
