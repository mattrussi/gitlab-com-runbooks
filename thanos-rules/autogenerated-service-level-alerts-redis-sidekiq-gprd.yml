# WARNING. DO NOT EDIT THIS FILE BY HAND. USE ./thanos-rules-jsonnet/service-component-alerts.jsonnet TO GENERATE IT
# YOUR CHANGES WILL BE OVERRIDDEN
groups:
- name: 'Service Component Alerts: redis-sidekiq'
  interval: 1m
  partial_response_strategy: warn
  rules:
  - alert: RedisSidekiqServicePrimaryServerTrafficCessationSingleShard
    for: 5m
    annotations:
      title: The primary_server SLI of the redis-sidekiq service on shard `{{ $labels.shard
        }}` has not received any traffic in the past 30m
      description: |
        Operations on the Redis primary for Redis Sidekiq instance.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.

        Since the `{{ $labels.type }}` service is not fully redundant, SLI violations on a single shard may represent a user-impacting service degradation.
      grafana_dashboard_id: redis-sidekiq-main/redis-sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/redis-sidekiq-main/redis-sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}&var-shard={{ $labels.shard
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "175328272"
      grafana_variables: environment,stage,shard
      promql_template_1: |
        sum by (env,environment,tier,stage,shard) (
          rate(redis_commands_processed_total{environment="{{ $labels.environment }}",shard="{{ $labels.shard }}",stage="{{ $labels.stage }}",type="redis-sidekiq"}[5m]) and on (instance) redis_instance_info{role="master"}
        )
      runbook: docs/redis-sidekiq/README.md
    labels:
      aggregation: component_shard
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
    expr: |
      gitlab_component_shard_ops:rate_30m{component="primary_server",env="gprd",monitor="global",type="redis-sidekiq"} == 0
      and
      gitlab_component_shard_ops:rate_30m{component="primary_server",env="gprd",monitor="global",type="redis-sidekiq"} offset 1h >= 0.16666666666666666
  - alert: RedisSidekiqServicePrimaryServerTrafficAbsentSingleShard
    for: 30m
    annotations:
      title: The primary_server SLI of the redis-sidekiq service on shard `{{ $labels.shard
        }}` has not reported any traffic in the past 30m
      description: |
        Operations on the Redis primary for Redis Sidekiq instance.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.

        Since the `{{ $labels.type }}` service is not fully redundant, SLI violations on a single shard may represent a user-impacting service degradation.
      grafana_dashboard_id: redis-sidekiq-main/redis-sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/redis-sidekiq-main/redis-sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}&var-shard={{ $labels.shard
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "175328272"
      grafana_variables: environment,stage,shard
      promql_template_1: |
        sum by (env,environment,tier,stage,shard) (
          rate(redis_commands_processed_total{environment="{{ $labels.environment }}",shard="{{ $labels.shard }}",stage="{{ $labels.stage }}",type="redis-sidekiq"}[5m]) and on (instance) redis_instance_info{role="master"}
        )
      runbook: docs/redis-sidekiq/README.md
    labels:
      aggregation: component_shard
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
    expr: |
      gitlab_component_shard_ops:rate_5m{component="primary_server",env="gprd",monitor="global",type="redis-sidekiq"} offset 1h
      unless
      gitlab_component_shard_ops:rate_5m{component="primary_server",env="gprd",monitor="global",type="redis-sidekiq"}
  - alert: RedisSidekiqServiceRailsRedisClientApdexSLOViolationSingleShard
    for: 10m
    annotations:
      title: The rails_redis_client SLI of the redis-sidekiq service on shard `{{
        $labels.shard }}` has an apdex violating SLO
      description: |
        Aggregation of all Redis operations issued to the Redis Sidekiq service from the Rails codebase.

        If this SLI is experiencing a degradation, it may be caused by saturation in the Redis Sidekiq instance caused by high traffic volumes from Sidekiq clients (Rails or other sidekiq jobs), or very large messages being delivered via Sidekiq.

        Reviewing Sidekiq job logs may help the investigation.

        Since the `{{ $labels.type }}` service is not fully redundant, SLI violations on a single shard may represent a user-impacting service degradation.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: redis-sidekiq-main/redis-sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/redis-sidekiq-main/redis-sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}&var-shard={{ $labels.shard
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2248986914"
      grafana_variables: environment,stage,shard
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (env,environment,tier,stage,shard,le) (
            rate(gitlab_redis_client_requests_duration_seconds_bucket{environment="{{ $labels.environment }}",shard="{{ $labels.shard }}",stage="{{ $labels.stage }}",storage="queues",type!="ops-gitlab-net"}[5m])
          )
        )
      runbook: docs/redis-sidekiq/README.md
    labels:
      aggregation: component_shard
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
    expr: |
      (
        (
          gitlab_component_shard_apdex:ratio_1h{component="rails_redis_client",env="gprd",monitor="global",type="redis-sidekiq"}
          < (1 - 14.4 * 0.000100)
        )
        and
        (
          gitlab_component_shard_apdex:ratio_5m{component="rails_redis_client",env="gprd",monitor="global",type="redis-sidekiq"}
          < (1 - 14.4 * 0.000100)
        )
      )
      and on(env,environment,tier,type,stage,shard,component)
      (
        sum by(env,environment,tier,type,stage,shard,component) (gitlab_component_shard_ops:rate_1h{component="rails_redis_client",env="gprd",monitor="global",type="redis-sidekiq"}) >= 1
      )
  - alert: RedisSidekiqServiceRailsRedisClientApdexSLOViolationSingleShard
    for: 10m
    annotations:
      title: The rails_redis_client SLI of the redis-sidekiq service on shard `{{
        $labels.shard }}` has an apdex violating SLO
      description: |
        Aggregation of all Redis operations issued to the Redis Sidekiq service from the Rails codebase.

        If this SLI is experiencing a degradation, it may be caused by saturation in the Redis Sidekiq instance caused by high traffic volumes from Sidekiq clients (Rails or other sidekiq jobs), or very large messages being delivered via Sidekiq.

        Reviewing Sidekiq job logs may help the investigation.

        Since the `{{ $labels.type }}` service is not fully redundant, SLI violations on a single shard may represent a user-impacting service degradation.

        Currently the apdex value is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: redis-sidekiq-main/redis-sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/redis-sidekiq-main/redis-sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}&var-shard={{ $labels.shard
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "2248986914"
      grafana_variables: environment,stage,shard
      promql_template_1: |
        histogram_quantile(
          0.950000,
          sum by (env,environment,tier,stage,shard,le) (
            rate(gitlab_redis_client_requests_duration_seconds_bucket{environment="{{ $labels.environment }}",shard="{{ $labels.shard }}",stage="{{ $labels.stage }}",storage="queues",type!="ops-gitlab-net"}[5m])
          )
        )
      runbook: docs/redis-sidekiq/README.md
    labels:
      aggregation: component_shard
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: apdex
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
    expr: |
      (
        (
          gitlab_component_shard_apdex:ratio_6h{component="rails_redis_client",env="gprd",monitor="global",type="redis-sidekiq"}
          < (1 - 6 * 0.000100)
        )
        and
        (
          gitlab_component_shard_apdex:ratio_30m{component="rails_redis_client",env="gprd",monitor="global",type="redis-sidekiq"}
          < (1 - 6 * 0.000100)
        )
      )
      and on(env,environment,tier,type,stage,shard,component)
      (
        sum by(env,environment,tier,type,stage,shard,component) (gitlab_component_shard_ops:rate_6h{component="rails_redis_client",env="gprd",monitor="global",type="redis-sidekiq"}) >= 0.16667
      )
  - alert: RedisSidekiqServiceRailsRedisClientErrorSLOViolationSingleShard
    for: 10m
    annotations:
      title: The rails_redis_client SLI of the redis-sidekiq service on shard `{{
        $labels.shard }}` has an error rate violating SLO
      description: |
        Aggregation of all Redis operations issued to the Redis Sidekiq service from the Rails codebase.

        If this SLI is experiencing a degradation, it may be caused by saturation in the Redis Sidekiq instance caused by high traffic volumes from Sidekiq clients (Rails or other sidekiq jobs), or very large messages being delivered via Sidekiq.

        Reviewing Sidekiq job logs may help the investigation.

        Since the `{{ $labels.type }}` service is not fully redundant, SLI violations on a single shard may represent a user-impacting service degradation.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: redis-sidekiq-main/redis-sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/redis-sidekiq-main/redis-sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}&var-shard={{ $labels.shard
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "487497488"
      grafana_variables: environment,stage,shard
      promql_template_1: |
        sum by (env,environment,tier,stage,shard) (
          rate(gitlab_redis_client_exceptions_total{environment="{{ $labels.environment }}",shard="{{ $labels.shard }}",stage="{{ $labels.stage }}",storage="queues",type!="ops-gitlab-net"}[5m])
        )
      runbook: docs/redis-sidekiq/README.md
    labels:
      aggregation: component_shard
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 1h
    expr: |
      (
        (
          gitlab_component_shard_errors:ratio_1h{component="rails_redis_client",env="gprd",monitor="global",type="redis-sidekiq"}
          > (14.4 * 0.001000)
        )
        and
        (
          gitlab_component_shard_errors:ratio_5m{component="rails_redis_client",env="gprd",monitor="global",type="redis-sidekiq"}
          > (14.4 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,shard,component)
      (
        sum by(env,environment,tier,type,stage,shard,component) (gitlab_component_shard_ops:rate_1h{component="rails_redis_client",env="gprd",monitor="global",type="redis-sidekiq"}) >= 1
      )
  - alert: RedisSidekiqServiceRailsRedisClientErrorSLOViolationSingleShard
    for: 10m
    annotations:
      title: The rails_redis_client SLI of the redis-sidekiq service on shard `{{
        $labels.shard }}` has an error rate violating SLO
      description: |
        Aggregation of all Redis operations issued to the Redis Sidekiq service from the Rails codebase.

        If this SLI is experiencing a degradation, it may be caused by saturation in the Redis Sidekiq instance caused by high traffic volumes from Sidekiq clients (Rails or other sidekiq jobs), or very large messages being delivered via Sidekiq.

        Reviewing Sidekiq job logs may help the investigation.

        Since the `{{ $labels.type }}` service is not fully redundant, SLI violations on a single shard may represent a user-impacting service degradation.

        Currently the error-rate is {{ $value | humanizePercentage }}.
      grafana_dashboard_id: redis-sidekiq-main/redis-sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/redis-sidekiq-main/redis-sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}&var-shard={{ $labels.shard
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "487497488"
      grafana_variables: environment,stage,shard
      promql_template_1: |
        sum by (env,environment,tier,stage,shard) (
          rate(gitlab_redis_client_exceptions_total{environment="{{ $labels.environment }}",shard="{{ $labels.shard }}",stage="{{ $labels.stage }}",storage="queues",type!="ops-gitlab-net"}[5m])
        )
      runbook: docs/redis-sidekiq/README.md
    labels:
      aggregation: component_shard
      alert_class: slo_violation
      alert_type: symptom
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: error
      slo_alert: "yes"
      user_impacting: "yes"
      window: 6h
    expr: |
      (
        (
          gitlab_component_shard_errors:ratio_6h{component="rails_redis_client",env="gprd",monitor="global",type="redis-sidekiq"}
          > (6 * 0.001000)
        )
        and
        (
          gitlab_component_shard_errors:ratio_30m{component="rails_redis_client",env="gprd",monitor="global",type="redis-sidekiq"}
          > (6 * 0.001000)
        )
      )
      and on(env,environment,tier,type,stage,shard,component)
      (
        sum by(env,environment,tier,type,stage,shard,component) (gitlab_component_shard_ops:rate_6h{component="rails_redis_client",env="gprd",monitor="global",type="redis-sidekiq"}) >= 0.16667
      )
  - alert: RedisSidekiqServiceRailsRedisClientTrafficCessationSingleShard
    for: 5m
    annotations:
      title: The rails_redis_client SLI of the redis-sidekiq service on shard `{{
        $labels.shard }}` has not received any traffic in the past 30m
      description: |
        Aggregation of all Redis operations issued to the Redis Sidekiq service from the Rails codebase.

        If this SLI is experiencing a degradation, it may be caused by saturation in the Redis Sidekiq instance caused by high traffic volumes from Sidekiq clients (Rails or other sidekiq jobs), or very large messages being delivered via Sidekiq.

        Reviewing Sidekiq job logs may help the investigation.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.

        Since the `{{ $labels.type }}` service is not fully redundant, SLI violations on a single shard may represent a user-impacting service degradation.
      grafana_dashboard_id: redis-sidekiq-main/redis-sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/redis-sidekiq-main/redis-sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}&var-shard={{ $labels.shard
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3147312091"
      grafana_variables: environment,stage,shard
      promql_template_1: |
        sum by (env,environment,tier,stage,shard) (
          rate(gitlab_redis_client_requests_total{environment="{{ $labels.environment }}",shard="{{ $labels.shard }}",stage="{{ $labels.stage }}",storage="queues",type!="ops-gitlab-net"}[5m])
        )
      runbook: docs/redis-sidekiq/README.md
    labels:
      aggregation: component_shard
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
    expr: |
      gitlab_component_shard_ops:rate_30m{component="rails_redis_client",env="gprd",monitor="global",type="redis-sidekiq"} == 0
      and
      gitlab_component_shard_ops:rate_30m{component="rails_redis_client",env="gprd",monitor="global",type="redis-sidekiq"} offset 1h >= 0.16666666666666666
  - alert: RedisSidekiqServiceRailsRedisClientTrafficAbsentSingleShard
    for: 30m
    annotations:
      title: The rails_redis_client SLI of the redis-sidekiq service on shard `{{
        $labels.shard }}` has not reported any traffic in the past 30m
      description: |
        Aggregation of all Redis operations issued to the Redis Sidekiq service from the Rails codebase.

        If this SLI is experiencing a degradation, it may be caused by saturation in the Redis Sidekiq instance caused by high traffic volumes from Sidekiq clients (Rails or other sidekiq jobs), or very large messages being delivered via Sidekiq.

        Reviewing Sidekiq job logs may help the investigation.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.

        Since the `{{ $labels.type }}` service is not fully redundant, SLI violations on a single shard may represent a user-impacting service degradation.
      grafana_dashboard_id: redis-sidekiq-main/redis-sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/redis-sidekiq-main/redis-sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}&var-shard={{ $labels.shard
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3147312091"
      grafana_variables: environment,stage,shard
      promql_template_1: |
        sum by (env,environment,tier,stage,shard) (
          rate(gitlab_redis_client_requests_total{environment="{{ $labels.environment }}",shard="{{ $labels.shard }}",stage="{{ $labels.stage }}",storage="queues",type!="ops-gitlab-net"}[5m])
        )
      runbook: docs/redis-sidekiq/README.md
    labels:
      aggregation: component_shard
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
    expr: |
      gitlab_component_shard_ops:rate_5m{component="rails_redis_client",env="gprd",monitor="global",type="redis-sidekiq"} offset 1h
      unless
      gitlab_component_shard_ops:rate_5m{component="rails_redis_client",env="gprd",monitor="global",type="redis-sidekiq"}
  - alert: RedisSidekiqServiceSecondaryServersTrafficCessationSingleShard
    for: 5m
    annotations:
      title: The secondary_servers SLI of the redis-sidekiq service on shard `{{ $labels.shard
        }}` has not received any traffic in the past 30m
      description: |
        Operations on the Redis secondaries for the Redis Sidekiq instance.

        This alert signifies that the SLI is reporting a cessation of traffic; the signal is present, but is zero.

        Since the `{{ $labels.type }}` service is not fully redundant, SLI violations on a single shard may represent a user-impacting service degradation.
      grafana_dashboard_id: redis-sidekiq-main/redis-sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/redis-sidekiq-main/redis-sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}&var-shard={{ $labels.shard
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3020974736"
      grafana_variables: environment,stage,shard
      promql_template_1: |
        sum by (env,environment,tier,stage,shard) (
          rate(redis_commands_processed_total{environment="{{ $labels.environment }}",shard="{{ $labels.shard }}",stage="{{ $labels.stage }}",type="redis-sidekiq"}[5m]) and on (instance) redis_instance_info{role="slave"}
        )
      runbook: docs/redis-sidekiq/README.md
    labels:
      aggregation: component_shard
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
    expr: |
      gitlab_component_shard_ops:rate_30m{component="secondary_servers",env="gprd",monitor="global",type="redis-sidekiq"} == 0
      and
      gitlab_component_shard_ops:rate_30m{component="secondary_servers",env="gprd",monitor="global",type="redis-sidekiq"} offset 1h >= 0.16666666666666666
  - alert: RedisSidekiqServiceSecondaryServersTrafficAbsentSingleShard
    for: 30m
    annotations:
      title: The secondary_servers SLI of the redis-sidekiq service on shard `{{ $labels.shard
        }}` has not reported any traffic in the past 30m
      description: |
        Operations on the Redis secondaries for the Redis Sidekiq instance.

        This alert signifies that the SLI was previously reporting traffic, but is no longer - the signal is absent.

        This could be caused by a change to the metrics used in the SLI, or by the service not receiving traffic.

        Since the `{{ $labels.type }}` service is not fully redundant, SLI violations on a single shard may represent a user-impacting service degradation.
      grafana_dashboard_id: redis-sidekiq-main/redis-sidekiq-overview
      grafana_dashboard_link: https://dashboards.gitlab.net/d/redis-sidekiq-main/redis-sidekiq-overview?from=now-6h/m&to=now-1m/m&var-environment={{
        $labels.environment }}&var-stage={{ $labels.stage }}&var-shard={{ $labels.shard
        }}
      grafana_min_zoom_hours: "6"
      grafana_panel_id: "3020974736"
      grafana_variables: environment,stage,shard
      promql_template_1: |
        sum by (env,environment,tier,stage,shard) (
          rate(redis_commands_processed_total{environment="{{ $labels.environment }}",shard="{{ $labels.shard }}",stage="{{ $labels.stage }}",type="redis-sidekiq"}[5m]) and on (instance) redis_instance_info{role="slave"}
        )
      runbook: docs/redis-sidekiq/README.md
    labels:
      aggregation: component_shard
      alert_class: traffic_cessation
      alert_type: cause
      feature_category: not_owned
      pager: pagerduty
      rules_domain: general
      severity: s2
      sli_type: ops
      slo_alert: "no"
      user_impacting: "yes"
    expr: |
      gitlab_component_shard_ops:rate_5m{component="secondary_servers",env="gprd",monitor="global",type="redis-sidekiq"} offset 1h
      unless
      gitlab_component_shard_ops:rate_5m{component="secondary_servers",env="gprd",monitor="global",type="redis-sidekiq"}
